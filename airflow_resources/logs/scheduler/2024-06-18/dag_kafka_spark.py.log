[2024-06-18T10:02:05.413+0000] {processor.py:157} INFO - Started process (PID=168) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:02:05.417+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:02:05.420+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:02:05.419+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:02:07.692+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:02:07.709+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:02:07,706 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 1, 37, 700370, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T10:02:07.710+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:02:07,710 - Sync 1 DAGs
[2024-06-18T10:02:08.096+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:02:08,093 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 1, 38, 89957, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:02:08.100+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:02:08,098 - Sync 1 DAGs
[2024-06-18T10:02:08.574+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:02:08,573 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 1, 38, 571865, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:02:08.575+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:02:08,575 - Sync 1 DAGs
[2024-06-18T10:02:08.578+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:02:38.956+0000] {processor.py:157} INFO - Started process (PID=431) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:02:38.957+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:02:38.958+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:02:38.958+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:02:39.328+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:02:39.400+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:02:39,400 - Sync 1 DAGs
[2024-06-18T10:02:39.419+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:02:39,419 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T09:59:26.025867+00:00, run_after=2024-06-18T10:04:26.025867+00:00
[2024-06-18T10:02:39.431+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.479 seconds
[2024-06-18T10:02:59.748+0000] {processor.py:157} INFO - Started process (PID=655) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:02:59.749+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:02:59.750+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:02:59.750+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:03:00.071+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:03:00.080+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:03:00,080 - Sync 1 DAGs
[2024-06-18T10:03:00.092+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:03:00,092 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T09:59:26.025867+00:00, run_after=2024-06-18T10:04:26.025867+00:00
[2024-06-18T10:03:00.102+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.357 seconds
[2024-06-18T10:03:01.011+0000] {processor.py:157} INFO - Started process (PID=661) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:03:01.012+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:03:01.013+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:03:01.013+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:03:01.280+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:03:01.287+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:03:01,285 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 2, 31, 280892, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T10:03:01.288+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:03:01,287 - Sync 1 DAGs
[2024-06-18T10:03:01.566+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:03:01,565 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 2, 31, 561923, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:03:01.568+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:03:01,568 - Sync 1 DAGs
[2024-06-18T10:03:01.633+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:03:01,630 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 2, 31, 627775, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:03:01.635+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:03:01,635 - Sync 1 DAGs
[2024-06-18T10:03:01.639+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:03:32.198+0000] {processor.py:157} INFO - Started process (PID=905) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:03:32.199+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:03:32.200+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:03:32.200+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:03:32.428+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:03:32.469+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:03:32,469 - Sync 1 DAGs
[2024-06-18T10:03:32.478+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:03:32,478 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T09:59:26.025867+00:00, run_after=2024-06-18T10:04:26.025867+00:00
[2024-06-18T10:03:32.489+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.293 seconds
[2024-06-18T10:04:03.102+0000] {processor.py:157} INFO - Started process (PID=1149) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:04:03.137+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:04:03.140+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:04:03.139+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:04:03.449+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:04:03.490+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:04:03,490 - Sync 1 DAGs
[2024-06-18T10:04:03.500+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:04:03,499 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T09:59:26.025867+00:00, run_after=2024-06-18T10:04:26.025867+00:00
[2024-06-18T10:04:03.507+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.411 seconds
[2024-06-18T10:04:33.710+0000] {processor.py:157} INFO - Started process (PID=1393) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:04:33.711+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:04:33.712+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:04:33.712+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:04:34.037+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:04:34.046+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:04:34,043 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 4, 4, 37901, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T10:04:34.047+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:04:34,047 - Sync 1 DAGs
[2024-06-18T10:04:34.475+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:04:34,473 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 4, 4, 472031, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:04:34.476+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:04:34,476 - Sync 1 DAGs
[2024-06-18T10:04:34.944+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:04:34,943 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 4, 4, 942270, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:04:34.945+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:04:34,945 - Sync 1 DAGs
[2024-06-18T10:04:34.949+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:05:05.084+0000] {processor.py:157} INFO - Started process (PID=1637) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:05:05.086+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:05:05.088+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:05:05.088+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:05:05.461+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:05:05.472+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:05:05,469 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 4, 35, 461996, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T10:05:05.474+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:05:05,473 - Sync 1 DAGs
[2024-06-18T10:05:05.914+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:05:05,914 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 4, 35, 913641, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:05:05.915+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:05:05,915 - Sync 1 DAGs
[2024-06-18T10:05:06.271+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:05:06,271 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 4, 36, 270171, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:05:06.272+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:05:06,272 - Sync 1 DAGs
[2024-06-18T10:05:06.274+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:05:37.248+0000] {processor.py:157} INFO - Started process (PID=1881) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:05:37.250+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:05:37.251+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:05:37.251+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:05:37.583+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:05:37.592+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:05:37,591 - Sync 1 DAGs
[2024-06-18T10:05:37.611+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:05:37,611 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T10:00:37.610923+00:00, run_after=2024-06-18T10:05:37.610923+00:00
[2024-06-18T10:05:37.621+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.378 seconds
[2024-06-18T10:06:08.119+0000] {processor.py:157} INFO - Started process (PID=2125) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:06:08.120+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:06:08.121+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:06:08.121+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:06:08.703+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:06:08.716+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:06:08,713 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 5, 38, 704431, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T10:06:08.717+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:06:08,717 - Sync 1 DAGs
[2024-06-18T10:06:08.727+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:06:08,726 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 5, 38, 726443, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:06:08.727+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:06:08,727 - Sync 1 DAGs
[2024-06-18T10:06:09.133+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:06:09,132 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 5, 39, 129966, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:06:09.136+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:06:09,135 - Sync 1 DAGs
[2024-06-18T10:06:09.141+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:06:39.946+0000] {processor.py:157} INFO - Started process (PID=2369) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:06:39.948+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:06:39.950+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:06:39.950+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:06:40.466+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:06:40.482+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:06:40,479 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 6, 10, 467415, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T10:06:40.483+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:06:40,483 - Sync 1 DAGs
[2024-06-18T10:06:40.730+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:06:40,729 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 6, 10, 728696, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:06:40.732+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:06:40,732 - Sync 1 DAGs
[2024-06-18T10:06:41.569+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:06:41,568 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 6, 11, 567639, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:06:41.570+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:06:41,570 - Sync 1 DAGs
[2024-06-18T10:06:41.575+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:07:12.023+0000] {processor.py:157} INFO - Started process (PID=2613) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:07:12.026+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:07:12.028+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:07:12.027+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:07:12.625+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:07:12.641+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:07:12,636 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 6, 42, 626420, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T10:07:12.643+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:07:12,642 - Sync 1 DAGs
[2024-06-18T10:07:13.001+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:07:12,999 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 6, 42, 995325, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:07:13.004+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:07:13,004 - Sync 1 DAGs
[2024-06-18T10:07:13.630+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:07:13,628 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 6, 43, 625800, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:07:13.633+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:07:13,633 - Sync 1 DAGs
[2024-06-18T10:07:13.638+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:07:43.834+0000] {processor.py:157} INFO - Started process (PID=2862) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:07:43.837+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:07:43.838+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:07:43.837+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:07:44.425+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:07:44.498+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:07:44,497 - Sync 1 DAGs
[2024-06-18T10:07:44.513+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:07:44,513 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T10:02:44.513494+00:00, run_after=2024-06-18T10:07:44.513494+00:00
[2024-06-18T10:07:44.526+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.695 seconds
[2024-06-18T10:08:15.285+0000] {processor.py:157} INFO - Started process (PID=3106) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:08:15.287+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:08:15.289+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:08:15.289+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:08:15.844+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:08:15.911+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:08:15,911 - Sync 1 DAGs
[2024-06-18T10:08:15.936+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:08:15,936 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T10:03:15.936195+00:00, run_after=2024-06-18T10:08:15.936195+00:00
[2024-06-18T10:08:15.949+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.669 seconds
[2024-06-18T10:08:46.240+0000] {processor.py:157} INFO - Started process (PID=3350) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:08:46.241+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:08:46.241+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:08:46.241+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:08:46.760+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:08:46.814+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:08:46,814 - Sync 1 DAGs
[2024-06-18T10:08:46.824+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:08:46,824 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T10:03:46.824009+00:00, run_after=2024-06-18T10:08:46.824009+00:00
[2024-06-18T10:08:46.833+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.596 seconds
[2024-06-18T10:09:16.964+0000] {processor.py:157} INFO - Started process (PID=3594) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:09:16.967+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:09:16.969+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:09:16.968+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:09:17.435+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:09:17.490+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:09:17,490 - Sync 1 DAGs
[2024-06-18T10:09:17.500+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:09:17,500 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T10:04:17.500120+00:00, run_after=2024-06-18T10:09:17.500120+00:00
[2024-06-18T10:09:17.508+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.552 seconds
[2024-06-18T10:09:47.823+0000] {processor.py:157} INFO - Started process (PID=3838) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:09:47.825+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:09:47.826+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:09:47.826+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:09:48.401+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:09:48.451+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:09:48,450 - Sync 1 DAGs
[2024-06-18T10:09:48.460+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:09:48,460 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T10:04:48.460749+00:00, run_after=2024-06-18T10:09:48.460749+00:00
[2024-06-18T10:09:48.470+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.652 seconds
[2024-06-18T10:10:19.015+0000] {processor.py:157} INFO - Started process (PID=4081) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:10:19.016+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:10:19.018+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:10:19.017+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:10:19.473+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:10:19.482+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:10:19,480 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 9, 49, 474664, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T10:10:19.483+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:10:19,483 - Sync 1 DAGs
[2024-06-18T10:10:19.526+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:10:19,525 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 9, 49, 525072, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:10:19.526+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:10:19,526 - Sync 1 DAGs
[2024-06-18T10:10:20.076+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:10:20,073 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 9, 50, 70430, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:10:20.079+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:10:20,079 - Sync 1 DAGs
[2024-06-18T10:10:20.085+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:10:50.649+0000] {processor.py:157} INFO - Started process (PID=4332) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:10:50.651+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:10:50.652+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:10:50.652+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:10:51.012+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:10:51.019+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:10:51,017 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 10, 21, 13581, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T10:10:51.020+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:10:51,019 - Sync 1 DAGs
[2024-06-18T10:10:51.192+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:10:51,191 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 10, 21, 190572, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:10:51.194+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:10:51,194 - Sync 1 DAGs
[2024-06-18T10:10:52.107+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:10:52,106 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 10, 22, 105470, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:10:52.108+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:10:52,107 - Sync 1 DAGs
[2024-06-18T10:10:52.110+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:11:22.211+0000] {processor.py:157} INFO - Started process (PID=4591) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:11:22.212+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:11:22.212+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:11:22.212+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:11:22.441+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:11:22.447+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:11:22,447 - Sync 1 DAGs
[2024-06-18T10:11:22.456+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:11:22,456 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T10:06:22.456767+00:00, run_after=2024-06-18T10:11:22.456767+00:00
[2024-06-18T10:11:22.464+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.255 seconds
[2024-06-18T10:11:53.277+0000] {processor.py:157} INFO - Started process (PID=4831) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:11:53.278+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:11:53.279+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:11:53.279+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:11:53.631+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:11:53.638+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:11:53,638 - Sync 1 DAGs
[2024-06-18T10:11:53.649+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:11:53,649 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T10:06:53.649558+00:00, run_after=2024-06-18T10:11:53.649558+00:00
[2024-06-18T10:11:53.657+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.385 seconds
[2024-06-18T10:12:23.724+0000] {processor.py:157} INFO - Started process (PID=5075) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:12:23.725+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:12:23.726+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:12:23.726+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:12:24.125+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:12:24.133+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:12:24,131 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 11, 54, 126546, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T10:12:24.134+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:12:24,133 - Sync 1 DAGs
[2024-06-18T10:12:24.336+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:12:24,335 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 11, 54, 332458, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:12:24.338+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:12:24,338 - Sync 1 DAGs
[2024-06-18T10:12:24.822+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:12:24,822 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 11, 54, 821539, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:12:24.825+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:12:24,825 - Sync 1 DAGs
[2024-06-18T10:12:24.826+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:12:55.212+0000] {processor.py:157} INFO - Started process (PID=5319) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:12:55.214+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:12:55.214+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:12:55.214+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:12:55.735+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:12:55.745+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:12:55,742 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 12, 25, 736392, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T10:12:55.746+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:12:55,746 - Sync 1 DAGs
[2024-06-18T10:12:55.847+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:12:55,846 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 12, 25, 846323, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:12:55.847+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:12:55,847 - Sync 1 DAGs
[2024-06-18T10:12:56.414+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:12:56,413 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 12, 26, 411106, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:12:56.416+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:12:56,416 - Sync 1 DAGs
[2024-06-18T10:12:56.421+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:13:26.690+0000] {processor.py:157} INFO - Started process (PID=5568) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:13:26.692+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:13:26.693+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:13:26.693+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:13:27.249+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:13:27.261+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:13:27,258 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 12, 57, 250010, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T10:13:27.262+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:13:27,262 - Sync 1 DAGs
[2024-06-18T10:13:27.751+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:13:27,750 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 12, 57, 750311, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:13:27.751+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:13:27,751 - Sync 1 DAGs
[2024-06-18T10:13:28.680+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:13:28,679 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 12, 58, 677459, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:13:28.682+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:13:28,682 - Sync 1 DAGs
[2024-06-18T10:13:28.688+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:13:59.455+0000] {processor.py:157} INFO - Started process (PID=5823) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:13:59.457+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:13:59.458+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:13:59.458+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:13:59.984+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:13:59.994+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:13:59,991 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 13, 29, 984911, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T10:13:59.994+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:13:59,994 - Sync 1 DAGs
[2024-06-18T10:14:00.201+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:14:00,200 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 13, 30, 197652, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:14:00.203+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:14:00,203 - Sync 1 DAGs
[2024-06-18T10:14:00.779+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:14:00,779 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 13, 30, 778022, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:14:00.780+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:14:00,780 - Sync 1 DAGs
[2024-06-18T10:14:00.782+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:14:31.778+0000] {processor.py:157} INFO - Started process (PID=6075) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:14:31.780+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:14:31.781+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:14:31.780+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:14:32.619+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:14:32.626+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:14:32,626 - Sync 1 DAGs
[2024-06-18T10:14:32.639+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:14:32,639 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T10:09:32.639371+00:00, run_after=2024-06-18T10:14:32.639371+00:00
[2024-06-18T10:14:32.650+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.876 seconds
[2024-06-18T10:15:03.164+0000] {processor.py:157} INFO - Started process (PID=6316) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:15:03.166+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:15:03.168+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:15:03.167+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:15:03.568+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:15:03.611+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:15:03,611 - Sync 1 DAGs
[2024-06-18T10:15:03.621+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:15:03,621 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T10:10:03.620988+00:00, run_after=2024-06-18T10:15:03.620988+00:00
[2024-06-18T10:15:03.629+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.472 seconds
[2024-06-18T10:15:34.462+0000] {processor.py:157} INFO - Started process (PID=6573) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:15:34.464+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:15:34.465+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:15:34.465+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:15:34.809+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:15:34.820+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:15:34,816 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 15, 4, 809949, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T10:15:34.820+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:15:34,820 - Sync 1 DAGs
[2024-06-18T10:15:34.878+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:15:34,878 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 15, 4, 877756, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:15:34.879+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:15:34,879 - Sync 1 DAGs
[2024-06-18T10:15:35.466+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:15:35,464 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 15, 5, 461417, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:15:35.469+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:15:35,469 - Sync 1 DAGs
[2024-06-18T10:15:35.477+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:16:05.675+0000] {processor.py:157} INFO - Started process (PID=6814) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:16:05.678+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:16:05.680+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:16:05.679+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:16:06.373+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:16:06.440+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:16:06,439 - Sync 1 DAGs
[2024-06-18T10:16:06.451+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:16:06,451 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T10:11:06.451513+00:00, run_after=2024-06-18T10:16:06.451513+00:00
[2024-06-18T10:16:06.460+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.721 seconds
[2024-06-18T10:16:37.053+0000] {processor.py:157} INFO - Started process (PID=7058) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:16:37.055+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:16:37.056+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:16:37.056+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:16:37.403+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:16:37.411+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:16:37,409 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 16, 7, 404348, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T10:16:37.412+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:16:37,412 - Sync 1 DAGs
[2024-06-18T10:16:37.464+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:16:37,464 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 16, 7, 463611, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:16:37.465+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:16:37,465 - Sync 1 DAGs
[2024-06-18T10:16:37.491+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:16:37,491 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 16, 7, 491097, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:16:37.492+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:16:37,492 - Sync 1 DAGs
[2024-06-18T10:16:37.493+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:17:08.165+0000] {processor.py:157} INFO - Started process (PID=7302) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:17:08.170+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:17:08.171+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:17:08.170+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:17:08.783+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:17:08.815+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:17:08,815 - Sync 1 DAGs
[2024-06-18T10:17:08.845+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:17:08,845 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T10:12:08.844952+00:00, run_after=2024-06-18T10:17:08.844952+00:00
[2024-06-18T10:17:08.883+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.725 seconds
[2024-06-18T10:17:39.735+0000] {processor.py:157} INFO - Started process (PID=7549) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:17:39.736+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:17:39.736+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:17:39.736+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:17:40.272+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:17:40.331+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:17:40,331 - Sync 1 DAGs
[2024-06-18T10:17:40.344+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:17:40,343 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T10:12:40.343741+00:00, run_after=2024-06-18T10:17:40.343741+00:00
[2024-06-18T10:17:40.355+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.622 seconds
[2024-06-18T10:18:10.562+0000] {processor.py:157} INFO - Started process (PID=7792) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:18:10.564+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:18:10.565+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:18:10.565+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:18:11.159+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:18:11.209+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:18:11,209 - Sync 1 DAGs
[2024-06-18T10:18:11.221+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:18:11,221 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T10:13:11.221455+00:00, run_after=2024-06-18T10:18:11.221455+00:00
[2024-06-18T10:18:11.230+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.674 seconds
[2024-06-18T10:18:41.729+0000] {processor.py:157} INFO - Started process (PID=8033) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:18:41.730+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:18:41.731+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:18:41.731+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:18:42.058+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:18:42.104+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:18:42,104 - Sync 1 DAGs
[2024-06-18T10:18:42.113+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:18:42,113 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T10:13:42.113767+00:00, run_after=2024-06-18T10:18:42.113767+00:00
[2024-06-18T10:18:42.123+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.397 seconds
[2024-06-18T10:19:12.207+0000] {processor.py:157} INFO - Started process (PID=8277) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:19:12.210+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:19:12.211+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:19:12.211+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:19:12.748+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:19:12.762+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:19:12,759 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 18, 42, 749977, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T10:19:12.763+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:19:12,763 - Sync 1 DAGs
[2024-06-18T10:19:13.262+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:19:13,262 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 18, 43, 261041, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:19:13.264+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:19:13,264 - Sync 1 DAGs
[2024-06-18T10:19:13.456+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:19:13,456 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 18, 43, 455770, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:19:13.457+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:19:13,457 - Sync 1 DAGs
[2024-06-18T10:19:13.459+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:19:43.884+0000] {processor.py:157} INFO - Started process (PID=8520) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:19:43.886+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:19:43.888+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:19:43.887+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:19:44.482+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:19:44.493+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:19:44,490 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 19, 14, 483163, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T10:19:44.494+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:19:44,493 - Sync 1 DAGs
[2024-06-18T10:19:44.950+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:19:44,950 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 19, 14, 949638, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:19:44.951+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:19:44,951 - Sync 1 DAGs
[2024-06-18T10:19:45.078+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:19:45,078 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 19, 15, 77527, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:19:45.080+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:19:45,079 - Sync 1 DAGs
[2024-06-18T10:19:45.081+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:20:15.636+0000] {processor.py:157} INFO - Started process (PID=8767) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:20:15.638+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:20:15.638+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:20:15.638+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:20:15.942+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:20:15.998+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:20:15,997 - Sync 1 DAGs
[2024-06-18T10:20:16.008+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:20:16,007 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T10:15:16.007748+00:00, run_after=2024-06-18T10:20:16.007748+00:00
[2024-06-18T10:20:16.016+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.383 seconds
[2024-06-18T10:20:46.115+0000] {processor.py:157} INFO - Started process (PID=9013) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:20:46.116+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:20:46.116+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:20:46.116+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:20:46.810+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:20:46.828+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:20:46,824 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 20, 16, 813291, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T10:20:46.829+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:20:46,829 - Sync 1 DAGs
[2024-06-18T10:20:47.175+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:20:47,174 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 20, 17, 174147, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:20:47.176+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:20:47,175 - Sync 1 DAGs
[2024-06-18T10:20:48.170+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:20:48,167 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 20, 18, 160770, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:20:48.176+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:20:48,176 - Sync 1 DAGs
[2024-06-18T10:20:48.222+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:21:18.596+0000] {processor.py:157} INFO - Started process (PID=9257) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:21:18.598+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:21:18.599+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:21:18.599+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:21:19.046+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:21:19.056+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:21:19,053 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 20, 49, 47866, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T10:21:19.058+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:21:19,057 - Sync 1 DAGs
[2024-06-18T10:21:19.097+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:21:19,096 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 20, 49, 96429, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:21:19.097+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:21:19,097 - Sync 1 DAGs
[2024-06-18T10:21:19.846+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:21:19,845 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 20, 49, 842952, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:21:19.847+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:21:19,847 - Sync 1 DAGs
[2024-06-18T10:21:19.849+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:21:50.506+0000] {processor.py:157} INFO - Started process (PID=9509) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:21:50.508+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:21:50.508+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:21:50.508+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:21:50.948+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:21:50.959+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:21:50,956 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 21, 20, 949250, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T10:21:50.959+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:21:50,959 - Sync 1 DAGs
[2024-06-18T10:21:51.212+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:21:51,210 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 21, 21, 208384, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:21:51.215+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:21:51,215 - Sync 1 DAGs
[2024-06-18T10:21:51.629+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:21:51,629 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 21, 21, 628935, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:21:51.630+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:21:51,630 - Sync 1 DAGs
[2024-06-18T10:21:51.631+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:22:21.793+0000] {processor.py:157} INFO - Started process (PID=9753) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:22:21.794+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:22:21.796+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:22:21.795+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:22:22.565+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:22:22.577+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:22:22,576 - Sync 1 DAGs
[2024-06-18T10:22:22.608+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:22:22,608 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T10:17:22.608384+00:00, run_after=2024-06-18T10:22:22.608384+00:00
[2024-06-18T10:22:22.625+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.835 seconds
[2024-06-18T10:22:53.065+0000] {processor.py:157} INFO - Started process (PID=9994) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:22:53.066+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:22:53.067+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:22:53.067+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:22:53.564+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:22:53.582+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:22:53,579 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 22, 23, 566321, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T10:22:53.583+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:22:53,583 - Sync 1 DAGs
[2024-06-18T10:22:53.983+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:22:53,982 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 22, 23, 982101, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:22:53.984+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:22:53,984 - Sync 1 DAGs
[2024-06-18T10:22:54.475+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:22:54,473 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 22, 24, 471471, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:22:54.476+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:22:54,476 - Sync 1 DAGs
[2024-06-18T10:22:54.500+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:23:24.618+0000] {processor.py:157} INFO - Started process (PID=10248) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:23:24.621+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:23:24.624+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:23:24.623+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:23:25.106+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:23:25.115+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:23:25,112 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 22, 55, 107104, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T10:23:25.116+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:23:25,116 - Sync 1 DAGs
[2024-06-18T10:23:25.315+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:23:25,314 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 22, 55, 314116, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:23:25.316+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:23:25,316 - Sync 1 DAGs
[2024-06-18T10:23:25.931+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:23:25,931 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 22, 55, 929848, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:23:25.932+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:23:25,932 - Sync 1 DAGs
[2024-06-18T10:23:25.936+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:23:56.103+0000] {processor.py:157} INFO - Started process (PID=10495) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:23:56.104+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:23:56.105+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:23:56.105+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:23:56.385+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:23:56.391+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:23:56,391 - Sync 1 DAGs
[2024-06-18T10:23:56.402+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:23:56,402 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T10:18:56.402347+00:00, run_after=2024-06-18T10:23:56.402347+00:00
[2024-06-18T10:23:56.411+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.312 seconds
[2024-06-18T10:24:26.535+0000] {processor.py:157} INFO - Started process (PID=10739) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:24:26.536+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:24:26.536+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:24:26.536+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:24:26.984+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:24:26.991+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:24:26,991 - Sync 1 DAGs
[2024-06-18T10:24:27.000+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:24:27,000 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T10:19:27.000704+00:00, run_after=2024-06-18T10:24:27.000704+00:00
[2024-06-18T10:24:27.009+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.476 seconds
[2024-06-18T10:24:57.705+0000] {processor.py:157} INFO - Started process (PID=10980) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:24:57.706+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:24:57.707+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:24:57.706+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:24:58.110+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:24:58.156+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:24:58,155 - Sync 1 DAGs
[2024-06-18T10:24:58.165+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:24:58,165 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T10:19:58.164976+00:00, run_after=2024-06-18T10:24:58.164976+00:00
[2024-06-18T10:24:58.173+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.471 seconds
[2024-06-18T10:25:28.288+0000] {processor.py:157} INFO - Started process (PID=11224) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:25:28.289+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:25:28.290+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:25:28.290+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:25:28.560+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:25:28.608+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:25:28,607 - Sync 1 DAGs
[2024-06-18T10:25:28.618+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:25:28,618 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T10:20:28.618177+00:00, run_after=2024-06-18T10:25:28.618177+00:00
[2024-06-18T10:25:28.627+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.343 seconds
[2024-06-18T10:25:59.101+0000] {processor.py:157} INFO - Started process (PID=11467) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:25:59.102+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:25:59.103+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:25:59.103+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:25:59.423+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:25:59.430+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:25:59,429 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 25, 29, 424118, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T10:25:59.431+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:25:59,431 - Sync 1 DAGs
[2024-06-18T10:25:59.839+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:25:59,837 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 25, 29, 834127, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:25:59.842+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:25:59,842 - Sync 1 DAGs
[2024-06-18T10:26:00.740+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:26:00,739 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 25, 30, 739174, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:26:00.741+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:26:00,741 - Sync 1 DAGs
[2024-06-18T10:26:00.742+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:26:30.778+0000] {processor.py:157} INFO - Started process (PID=11712) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:26:30.781+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:26:30.782+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:26:30.782+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:26:31.172+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:26:31.179+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:26:31,179 - Sync 1 DAGs
[2024-06-18T10:26:31.190+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:26:31,190 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T10:21:31.190718+00:00, run_after=2024-06-18T10:26:31.190718+00:00
[2024-06-18T10:26:31.198+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.422 seconds
[2024-06-18T10:27:01.364+0000] {processor.py:157} INFO - Started process (PID=11966) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:27:01.365+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:27:01.366+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:27:01.365+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:27:01.665+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:27:01.682+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:27:01,679 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 26, 31, 668240, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T10:27:01.683+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:27:01,683 - Sync 1 DAGs
[2024-06-18T10:27:01.937+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:27:01,937 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 26, 31, 936477, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:27:01.938+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:27:01,938 - Sync 1 DAGs
[2024-06-18T10:27:02.744+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:27:02,743 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 26, 32, 742617, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:27:02.746+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:27:02,746 - Sync 1 DAGs
[2024-06-18T10:27:02.749+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:27:32.907+0000] {processor.py:157} INFO - Started process (PID=12217) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:27:32.908+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:27:32.908+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:27:32.908+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:27:33.125+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:27:33.137+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:27:33,134 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 27, 3, 127754, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T10:27:33.138+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:27:33,138 - Sync 1 DAGs
[2024-06-18T10:27:33.550+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:27:33,550 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 27, 3, 549279, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:27:33.551+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:27:33,551 - Sync 1 DAGs
[2024-06-18T10:27:33.934+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:27:33,932 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 27, 3, 929574, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:27:33.937+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:27:33,936 - Sync 1 DAGs
[2024-06-18T10:27:33.942+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T10:59:24.362+0000] {processor.py:157} INFO - Started process (PID=12263) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:59:24.363+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:59:24.364+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:59:24.364+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:59:24.873+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:59:25.031+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:59:25,030 - Sync 1 DAGs
[2024-06-18T10:59:25.074+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:59:25,073 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T10:54:25.069436+00:00, run_after=2024-06-18T10:59:25.069436+00:00
[2024-06-18T10:59:25.266+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.907 seconds
[2024-06-18T10:59:55.590+0000] {processor.py:157} INFO - Started process (PID=12507) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:59:55.593+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T10:59:55.595+0000] {logging_mixin.py:154} INFO - [2024-06-18T10:59:55.593+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:59:56.109+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T10:59:56.181+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:59:56,181 - Sync 1 DAGs
[2024-06-18T10:59:56.200+0000] {logging_mixin.py:154} WARNING - 2024-06-18 10:59:56,200 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T10:54:56.199944+00:00, run_after=2024-06-18T10:59:56.199944+00:00
[2024-06-18T10:59:56.214+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.632 seconds
[2024-06-18T11:00:26.407+0000] {processor.py:157} INFO - Started process (PID=12751) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:00:26.408+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:00:26.409+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:00:26.409+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:00:26.854+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:00:26.938+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:00:26,931 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 59, 56, 867572, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T11:00:26.945+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:00:26,939 - Sync 1 DAGs
[2024-06-18T11:00:27.202+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:00:27,201 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 59, 57, 200047, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T11:00:27.203+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:00:27,203 - Sync 1 DAGs
[2024-06-18T11:00:27.357+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:00:27,356 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 10, 59, 57, 355475, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T11:00:27.359+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:00:27,359 - Sync 1 DAGs
[2024-06-18T11:00:27.362+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T11:00:58.196+0000] {processor.py:157} INFO - Started process (PID=13000) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:00:58.197+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:00:58.199+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:00:58.198+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:00:58.619+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:00:58.732+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:00:58,731 - Sync 1 DAGs
[2024-06-18T11:00:58.750+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:00:58,750 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T10:55:58.750585+00:00, run_after=2024-06-18T11:00:58.750585+00:00
[2024-06-18T11:00:58.765+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.572 seconds
[2024-06-18T11:01:29.138+0000] {processor.py:157} INFO - Started process (PID=13244) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:01:29.139+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:01:29.140+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:01:29.140+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:01:29.389+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:01:29.442+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:01:29,442 - Sync 1 DAGs
[2024-06-18T11:01:29.452+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:01:29,452 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T10:56:29.452702+00:00, run_after=2024-06-18T11:01:29.452702+00:00
[2024-06-18T11:01:29.466+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.332 seconds
[2024-06-18T11:01:59.743+0000] {processor.py:157} INFO - Started process (PID=13488) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:01:59.745+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:01:59.745+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:01:59.745+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:01:59.953+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:01:59.962+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:01:59,959 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 11, 1, 29, 954157, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T11:01:59.963+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:01:59,963 - Sync 1 DAGs
[2024-06-18T11:02:00.194+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:02:00,192 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 11, 1, 30, 189254, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T11:02:00.196+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:02:00,196 - Sync 1 DAGs
[2024-06-18T11:02:00.797+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:02:00,797 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 11, 1, 30, 796712, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T11:02:00.798+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:02:00,798 - Sync 1 DAGs
[2024-06-18T11:02:00.799+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T11:02:30.822+0000] {processor.py:157} INFO - Started process (PID=13737) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:02:30.823+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:02:30.824+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:02:30.823+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:02:31.046+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:02:31.058+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:02:31,057 - Sync 1 DAGs
[2024-06-18T11:02:31.085+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:02:31,084 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T10:57:31.084779+00:00, run_after=2024-06-18T11:02:31.084779+00:00
[2024-06-18T11:02:31.098+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.278 seconds
[2024-06-18T11:03:01.858+0000] {processor.py:157} INFO - Started process (PID=13981) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:03:01.859+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:03:01.860+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:03:01.859+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:03:02.185+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:03:02.201+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:03:02,198 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 11, 2, 32, 187619, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T11:03:02.202+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:03:02,201 - Sync 1 DAGs
[2024-06-18T11:03:02.385+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:03:02,384 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 11, 2, 32, 383636, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T11:03:02.386+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:03:02,386 - Sync 1 DAGs
[2024-06-18T11:03:03.017+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:03:03,017 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 11, 2, 33, 16101, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T11:03:03.018+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:03:03,018 - Sync 1 DAGs
[2024-06-18T11:03:03.019+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T11:03:33.448+0000] {processor.py:157} INFO - Started process (PID=14225) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:03:33.449+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:03:33.450+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:03:33.450+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:03:33.680+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:03:33.689+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:03:33,687 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 11, 3, 3, 681859, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T11:03:33.690+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:03:33,690 - Sync 1 DAGs
[2024-06-18T11:03:33.799+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:03:33,798 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 11, 3, 3, 796266, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T11:03:33.801+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:03:33,800 - Sync 1 DAGs
[2024-06-18T11:03:33.867+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:03:33,867 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 11, 3, 3, 866755, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T11:03:33.868+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:03:33,868 - Sync 1 DAGs
[2024-06-18T11:03:33.869+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T11:04:04.828+0000] {processor.py:157} INFO - Started process (PID=14474) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:04:04.829+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:04:04.830+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:04:04.830+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:04:05.421+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:04:05.435+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:04:05,433 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 11, 3, 35, 423757, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T11:04:05.436+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:04:05,436 - Sync 1 DAGs
[2024-06-18T11:04:05.566+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:04:05,565 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 11, 3, 35, 564677, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T11:04:05.567+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:04:05,567 - Sync 1 DAGs
[2024-06-18T11:04:06.143+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:04:06,142 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 11, 3, 36, 141770, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T11:04:06.144+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:04:06,144 - Sync 1 DAGs
[2024-06-18T11:04:06.145+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T11:04:36.956+0000] {processor.py:157} INFO - Started process (PID=14741) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:04:36.957+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:04:36.957+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:04:36.957+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:04:37.148+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:04:37.155+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:04:37,154 - Sync 1 DAGs
[2024-06-18T11:04:37.166+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:04:37,166 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T10:59:37.165897+00:00, run_after=2024-06-18T11:04:37.165897+00:00
[2024-06-18T11:04:37.173+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.219 seconds
[2024-06-18T11:05:07.433+0000] {processor.py:157} INFO - Started process (PID=14972) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:05:07.434+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:05:07.434+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:05:07.434+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:05:07.675+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:05:07.717+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:05:07,695 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 11, 4, 37, 676511, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T11:05:07.719+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:05:07,718 - Sync 1 DAGs
[2024-06-18T11:05:08.096+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:05:08,095 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 11, 4, 38, 93467, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T11:05:08.097+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:05:08,097 - Sync 1 DAGs
[2024-06-18T11:05:08.374+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:05:08,372 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 11, 4, 38, 371123, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T11:05:08.377+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:05:08,376 - Sync 1 DAGs
[2024-06-18T11:05:08.380+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T11:05:38.951+0000] {processor.py:157} INFO - Started process (PID=15224) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:05:38.952+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:05:38.953+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:05:38.953+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:05:39.235+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:05:39.375+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:05:39,375 - Sync 1 DAGs
[2024-06-18T11:05:39.418+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:05:39,418 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:00:39.417941+00:00, run_after=2024-06-18T11:05:39.417941+00:00
[2024-06-18T11:05:39.437+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.491 seconds
[2024-06-18T11:06:10.049+0000] {processor.py:157} INFO - Started process (PID=15463) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:06:10.064+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:06:10.066+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:06:10.066+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:06:10.707+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:06:10.805+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:06:10,801 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 11, 5, 40, 723344, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T11:06:10.807+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:06:10,806 - Sync 1 DAGs
[2024-06-18T11:06:11.259+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:06:11,258 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 11, 5, 41, 256768, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T11:06:11.262+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:06:11,262 - Sync 1 DAGs
[2024-06-18T11:06:11.936+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:06:11,935 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 11, 5, 41, 933165, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T11:06:11.940+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:06:11,940 - Sync 1 DAGs
[2024-06-18T11:06:11.941+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T11:06:42.443+0000] {processor.py:157} INFO - Started process (PID=15719) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:06:42.447+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:06:42.448+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:06:42.447+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:06:42.943+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:06:42.965+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:06:42,956 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 11, 6, 12, 947463, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T11:06:42.969+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:06:42,966 - Sync 1 DAGs
[2024-06-18T11:06:43.481+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:06:43,480 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 11, 6, 13, 479089, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T11:06:43.482+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:06:43,482 - Sync 1 DAGs
[2024-06-18T11:06:43.546+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:06:43,546 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 11, 6, 13, 545427, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T11:06:43.547+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:06:43,547 - Sync 1 DAGs
[2024-06-18T11:06:43.550+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T11:07:13.893+0000] {processor.py:157} INFO - Started process (PID=15968) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:07:13.899+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:07:13.905+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:07:13.905+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:07:15.166+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:07:15.625+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:07:15,625 - Sync 1 DAGs
[2024-06-18T11:07:15.696+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:07:15,696 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:02:15.695123+00:00, run_after=2024-06-18T11:07:15.695123+00:00
[2024-06-18T11:07:15.730+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 1.844 seconds
[2024-06-18T11:07:45.990+0000] {processor.py:157} INFO - Started process (PID=16220) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:07:45.991+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:07:45.993+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:07:45.992+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:07:46.567+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:07:46.754+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:07:46,754 - Sync 1 DAGs
[2024-06-18T11:07:46.799+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:07:46,798 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:02:46.798603+00:00, run_after=2024-06-18T11:07:46.798603+00:00
[2024-06-18T11:07:46.826+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.843 seconds
[2024-06-18T11:08:17.163+0000] {processor.py:157} INFO - Started process (PID=16461) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:08:17.164+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:08:17.165+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:08:17.165+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:08:17.456+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:08:17.536+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:08:17,536 - Sync 1 DAGs
[2024-06-18T11:08:17.551+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:08:17,551 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:03:17.551295+00:00, run_after=2024-06-18T11:08:17.551295+00:00
[2024-06-18T11:08:17.566+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.407 seconds
[2024-06-18T11:08:47.962+0000] {processor.py:157} INFO - Started process (PID=16703) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:08:47.964+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:08:47.965+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:08:47.965+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:08:48.163+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:08:48.173+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:08:48,170 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 11, 8, 18, 165029, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T11:08:48.173+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:08:48,173 - Sync 1 DAGs
[2024-06-18T11:08:48.470+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:08:48,469 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 11, 8, 18, 468547, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T11:08:48.471+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:08:48,471 - Sync 1 DAGs
[2024-06-18T11:08:48.702+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:08:48,701 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 11, 8, 18, 699537, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T11:08:48.704+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:08:48,704 - Sync 1 DAGs
[2024-06-18T11:08:48.709+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T11:09:19.160+0000] {processor.py:157} INFO - Started process (PID=16949) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:09:19.161+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:09:19.163+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:09:19.162+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:09:19.336+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:09:19.383+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:09:19,382 - Sync 1 DAGs
[2024-06-18T11:09:19.392+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:09:19,392 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:04:19.392653+00:00, run_after=2024-06-18T11:09:19.392653+00:00
[2024-06-18T11:09:19.401+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.247 seconds
[2024-06-18T11:09:49.713+0000] {processor.py:157} INFO - Started process (PID=17193) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:09:49.715+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:09:49.716+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:09:49.716+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:09:49.963+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:09:49.972+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:09:49,970 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "serialized_dag" does not exist
LINE 2: FROM serialized_dag 
             ^

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 11, 9, 19, 964362, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-18T11:09:49.973+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:09:49,973 - Sync 1 DAGs
[2024-06-18T11:09:50.023+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:09:50,023 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 11, 9, 20, 22759, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T11:09:50.024+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:09:50,024 - Sync 1 DAGs
[2024-06-18T11:09:50.787+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:09:50,787 - Failed to write serialized DAG: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 633, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 147, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT %(param_1)s AS anon_1 
FROM serialized_dag 
WHERE serialized_dag.dag_id = %(dag_id_1)s AND serialized_dag.last_updated > %(last_updated_1)s]
[parameters: {'param_1': True, 'dag_id_1': 'kafka_spark_dag', 'last_updated_1': datetime.datetime(2024, 6, 18, 11, 9, 20, 786525, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T11:09:50.788+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:09:50,788 - Sync 1 DAGs
[2024-06-18T11:09:50.789+0000] {processor.py:182} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 178, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 159, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 857, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 893, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 654, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 668, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 2953, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block

[SQL: SELECT dag.dag_id, dag.root_dag_id, dag.is_paused, dag.is_subdag, dag.is_active, dag.last_parsed_time, dag.last_pickled, dag.last_expired, dag.scheduler_lock, dag.pickle_id, dag.fileloc, dag.processor_subdir, dag.owners, dag.description, dag.default_view, dag.schedule_interval, dag.timetable_description, dag.max_active_tasks, dag.max_active_runs, dag.has_task_concurrency_limits, dag.has_import_errors, dag.next_dagrun, dag.next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end, dag.next_dagrun_create_after, dag_tag_1.name, dag_tag_1.dag_id AS dag_id_1, dag_schedule_dataset_reference_1.dataset_id, dag_schedule_dataset_reference_1.dag_id AS dag_id_2, dag_schedule_dataset_reference_1.created_at, dag_schedule_dataset_reference_1.updated_at, task_outlet_dataset_reference_1.dataset_id AS dataset_id_1, task_outlet_dataset_reference_1.dag_id AS dag_id_3, task_outlet_dataset_reference_1.task_id, task_outlet_dataset_reference_1.created_at AS created_at_1, task_outlet_dataset_reference_1.updated_at AS updated_at_1 
FROM dag LEFT OUTER JOIN dag_tag AS dag_tag_1 ON dag.dag_id = dag_tag_1.dag_id LEFT OUTER JOIN dag_schedule_dataset_reference AS dag_schedule_dataset_reference_1 ON dag.dag_id = dag_schedule_dataset_reference_1.dag_id LEFT OUTER JOIN task_outlet_dataset_reference AS task_outlet_dataset_reference_1 ON dag.dag_id = task_outlet_dataset_reference_1.dag_id 
WHERE dag.dag_id IN (%(dag_id_4_1)s) FOR UPDATE OF dag]
[parameters: {'dag_id_4_1': 'kafka_spark_dag'}]
(Background on this error at: https://sqlalche.me/e/14/2j85)
[2024-06-18T11:10:21.031+0000] {processor.py:157} INFO - Started process (PID=17442) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:10:21.032+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:10:21.033+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:10:21.033+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:10:21.226+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:10:21.233+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:10:21,233 - Sync 1 DAGs
[2024-06-18T11:10:21.246+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:10:21,246 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:05:21.245930+00:00, run_after=2024-06-18T11:10:21.245930+00:00
[2024-06-18T11:10:21.256+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.229 seconds
[2024-06-18T11:12:13.904+0000] {processor.py:157} INFO - Started process (PID=168) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:12:13.905+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:12:13.905+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:12:13.905+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:12:14.376+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:12:14.437+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:12:14,437 - Created Permission View: %s
[2024-06-18T11:12:14.442+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:12:14,442 - Created Permission View: %s
[2024-06-18T11:12:14.445+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:12:14,445 - Created Permission View: %s
[2024-06-18T11:12:14.446+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:12:14,446 - Sync 1 DAGs
[2024-06-18T11:12:14.452+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:12:14,451 - Creating ORM DAG for kafka_spark_dag
[2024-06-18T11:12:14.458+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:12:14,458 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:07:14.458082+00:00, run_after=2024-06-18T11:12:14.458082+00:00
[2024-06-18T11:12:14.466+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.565 seconds
[2024-06-18T11:12:44.855+0000] {processor.py:157} INFO - Started process (PID=412) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:12:44.857+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:12:44.860+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:12:44.859+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:12:45.418+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:12:45.468+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:12:45,467 - Sync 1 DAGs
[2024-06-18T11:12:45.477+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:12:45,477 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:07:45.477766+00:00, run_after=2024-06-18T11:12:45.477766+00:00
[2024-06-18T11:12:45.497+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.651 seconds
[2024-06-18T11:13:15.600+0000] {processor.py:157} INFO - Started process (PID=688) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:13:15.601+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:13:15.601+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:13:15.601+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:13:16.072+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:13:16.129+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:13:16,129 - Sync 1 DAGs
[2024-06-18T11:13:16.140+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:13:16,139 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:08:16.139842+00:00, run_after=2024-06-18T11:13:16.139842+00:00
[2024-06-18T11:13:16.151+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.553 seconds
[2024-06-18T11:13:46.941+0000] {processor.py:157} INFO - Started process (PID=953) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:13:46.943+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:13:46.944+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:13:46.944+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:13:47.372+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:13:47.421+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:13:47,420 - Sync 1 DAGs
[2024-06-18T11:13:47.431+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:13:47,430 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:08:47.430768+00:00, run_after=2024-06-18T11:13:47.430768+00:00
[2024-06-18T11:13:47.441+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.506 seconds
[2024-06-18T11:14:17.670+0000] {processor.py:157} INFO - Started process (PID=1199) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:14:17.671+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:14:17.672+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:14:17.672+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:14:18.629+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:14:18.704+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:14:18,703 - Sync 1 DAGs
[2024-06-18T11:14:18.720+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:14:18,719 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:13:47.430768+00:00, run_after=2024-06-18T11:18:47.430768+00:00
[2024-06-18T11:14:18.909+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 1.241 seconds
[2024-06-18T11:14:49.125+0000] {processor.py:157} INFO - Started process (PID=1482) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:14:49.126+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:14:49.126+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:14:49.126+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:14:49.644+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:14:49.717+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:14:49,717 - Sync 1 DAGs
[2024-06-18T11:14:49.730+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:14:49,730 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:13:47.430768+00:00, run_after=2024-06-18T11:18:47.430768+00:00
[2024-06-18T11:14:49.742+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.620 seconds
[2024-06-18T11:15:20.589+0000] {processor.py:157} INFO - Started process (PID=1731) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:15:20.591+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:15:20.592+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:15:20.592+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:15:21.213+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:15:21.267+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:15:21,267 - Sync 1 DAGs
[2024-06-18T11:15:21.277+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:15:21,277 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:13:47.430768+00:00, run_after=2024-06-18T11:18:47.430768+00:00
[2024-06-18T11:15:21.285+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.701 seconds
[2024-06-18T11:22:32.874+0000] {processor.py:157} INFO - Started process (PID=168) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:22:32.878+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:22:32.880+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:22:32.880+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:22:33.961+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:22:34.088+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:22:34,088 - Sync 1 DAGs
[2024-06-18T11:22:34.104+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:22:34,104 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:17:34.104021+00:00, run_after=2024-06-18T11:22:34.104021+00:00
[2024-06-18T11:22:34.118+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 1.247 seconds
[2024-06-18T11:23:06.016+0000] {processor.py:157} INFO - Started process (PID=442) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:23:06.021+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:23:06.023+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:23:06.022+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:23:08.194+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:23:08.370+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:23:08,369 - Sync 1 DAGs
[2024-06-18T11:23:08.393+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:23:08,393 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:22:34.104021+00:00, run_after=2024-06-18T11:27:34.104021+00:00
[2024-06-18T11:23:08.421+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 2.418 seconds
[2024-06-18T11:23:39.404+0000] {processor.py:157} INFO - Started process (PID=726) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:23:39.409+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:23:39.413+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:23:39.412+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:23:40.229+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:23:40.325+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:23:40,325 - Sync 1 DAGs
[2024-06-18T11:23:40.340+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:23:40,339 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:22:34.104021+00:00, run_after=2024-06-18T11:27:34.104021+00:00
[2024-06-18T11:23:40.353+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.954 seconds
[2024-06-18T11:24:05.870+0000] {processor.py:157} INFO - Started process (PID=915) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:24:05.873+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:24:05.876+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:24:05.875+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:24:07.122+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:24:07.129+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:24:07,129 - Sync 1 DAGs
[2024-06-18T11:24:07.140+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:24:07,140 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:22:34.104021+00:00, run_after=2024-06-18T11:27:34.104021+00:00
[2024-06-18T11:24:07.152+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 1.293 seconds
[2024-06-18T11:24:37.398+0000] {processor.py:157} INFO - Started process (PID=1167) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:24:37.399+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:24:37.401+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:24:37.400+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:24:37.701+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:24:37.744+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:24:37,744 - Sync 1 DAGs
[2024-06-18T11:24:37.752+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:24:37,752 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:22:34.104021+00:00, run_after=2024-06-18T11:27:34.104021+00:00
[2024-06-18T11:24:37.760+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.367 seconds
[2024-06-18T11:25:08.729+0000] {processor.py:157} INFO - Started process (PID=1431) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:25:08.730+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:25:08.732+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:25:08.731+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:25:09.022+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:25:09.069+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:25:09,069 - Sync 1 DAGs
[2024-06-18T11:25:09.078+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:25:09,078 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:22:34.104021+00:00, run_after=2024-06-18T11:27:34.104021+00:00
[2024-06-18T11:25:09.085+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.360 seconds
[2024-06-18T11:25:39.864+0000] {processor.py:157} INFO - Started process (PID=1675) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:25:39.865+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:25:39.866+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:25:39.866+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:25:40.163+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:25:40.215+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:25:40,215 - Sync 1 DAGs
[2024-06-18T11:25:40.226+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:25:40,226 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:22:34.104021+00:00, run_after=2024-06-18T11:27:34.104021+00:00
[2024-06-18T11:25:40.235+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.376 seconds
[2024-06-18T11:26:10.288+0000] {processor.py:157} INFO - Started process (PID=1942) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:26:10.290+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:26:10.290+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:26:10.290+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:26:10.630+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:26:10.672+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:26:10,672 - Sync 1 DAGs
[2024-06-18T11:26:10.680+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:26:10,680 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:22:34.104021+00:00, run_after=2024-06-18T11:27:34.104021+00:00
[2024-06-18T11:26:10.689+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.403 seconds
[2024-06-18T11:26:41.629+0000] {processor.py:157} INFO - Started process (PID=2206) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:26:41.631+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:26:41.631+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:26:41.631+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:26:42.316+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:26:42.386+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:26:42,385 - Sync 1 DAGs
[2024-06-18T11:26:42.398+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:26:42,398 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:22:34.104021+00:00, run_after=2024-06-18T11:27:34.104021+00:00
[2024-06-18T11:26:42.408+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.782 seconds
[2024-06-18T11:27:13.417+0000] {processor.py:157} INFO - Started process (PID=2450) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:27:13.420+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:27:13.421+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:27:13.421+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:27:13.812+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:27:13.874+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:27:13,874 - Sync 1 DAGs
[2024-06-18T11:27:13.887+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:27:13,887 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:22:34.104021+00:00, run_after=2024-06-18T11:27:34.104021+00:00
[2024-06-18T11:27:13.897+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.486 seconds
[2024-06-18T11:27:43.951+0000] {processor.py:157} INFO - Started process (PID=2732) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:27:43.952+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:27:43.953+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:27:43.952+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:27:44.385+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:27:44.444+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:27:44,444 - Sync 1 DAGs
[2024-06-18T11:27:44.454+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:27:44,454 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:27:34.104021+00:00, run_after=2024-06-18T11:32:34.104021+00:00
[2024-06-18T11:27:44.462+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.513 seconds
[2024-06-18T11:28:14.578+0000] {processor.py:157} INFO - Started process (PID=3004) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:28:14.580+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:28:14.585+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:28:14.585+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:28:15.219+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:28:15.270+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:28:15,269 - Sync 1 DAGs
[2024-06-18T11:28:15.280+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:28:15,280 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:27:34.104021+00:00, run_after=2024-06-18T11:32:34.104021+00:00
[2024-06-18T11:28:15.296+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.723 seconds
[2024-06-18T11:28:46.208+0000] {processor.py:157} INFO - Started process (PID=3263) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:28:46.210+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:28:46.211+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:28:46.211+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:28:46.695+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:28:46.758+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:28:46,758 - Sync 1 DAGs
[2024-06-18T11:28:46.768+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:28:46,768 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:27:34.104021+00:00, run_after=2024-06-18T11:32:34.104021+00:00
[2024-06-18T11:28:46.777+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.574 seconds
[2024-06-18T11:29:16.853+0000] {processor.py:157} INFO - Started process (PID=3507) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:29:16.855+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:29:16.856+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:29:16.855+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:29:17.515+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:29:17.562+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:29:17,561 - Sync 1 DAGs
[2024-06-18T11:29:17.574+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:29:17,574 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:27:34.104021+00:00, run_after=2024-06-18T11:32:34.104021+00:00
[2024-06-18T11:29:17.584+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.735 seconds
[2024-06-18T11:29:47.653+0000] {processor.py:157} INFO - Started process (PID=3782) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:29:47.655+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:29:47.657+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:29:47.657+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:29:48.271+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:29:48.323+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:29:48,323 - Sync 1 DAGs
[2024-06-18T11:29:48.333+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:29:48,333 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:27:34.104021+00:00, run_after=2024-06-18T11:32:34.104021+00:00
[2024-06-18T11:29:48.346+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.698 seconds
[2024-06-18T11:30:18.499+0000] {processor.py:157} INFO - Started process (PID=4061) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:30:18.500+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:30:18.502+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:30:18.501+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:30:19.116+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:30:19.176+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:30:19,175 - Sync 1 DAGs
[2024-06-18T11:30:19.187+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:30:19,187 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:27:34.104021+00:00, run_after=2024-06-18T11:32:34.104021+00:00
[2024-06-18T11:30:19.200+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.705 seconds
[2024-06-18T11:30:49.328+0000] {processor.py:157} INFO - Started process (PID=4342) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:30:49.329+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:30:49.329+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:30:49.329+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:30:49.704+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:30:49.755+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:30:49,755 - Sync 1 DAGs
[2024-06-18T11:30:49.767+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:30:49,767 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:27:34.104021+00:00, run_after=2024-06-18T11:32:34.104021+00:00
[2024-06-18T11:30:49.779+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.454 seconds
[2024-06-18T11:31:19.826+0000] {processor.py:157} INFO - Started process (PID=4594) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:31:19.827+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:31:19.827+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:31:19.827+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:31:20.185+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:31:20.294+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:31:20,294 - Sync 1 DAGs
[2024-06-18T11:31:20.306+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:31:20,305 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:27:34.104021+00:00, run_after=2024-06-18T11:32:34.104021+00:00
[2024-06-18T11:31:20.329+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.505 seconds
[2024-06-18T11:31:51.310+0000] {processor.py:157} INFO - Started process (PID=4841) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:31:51.312+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:31:51.314+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:31:51.313+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:31:51.683+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:31:51.733+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:31:51,732 - Sync 1 DAGs
[2024-06-18T11:31:51.742+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:31:51,742 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:27:34.104021+00:00, run_after=2024-06-18T11:32:34.104021+00:00
[2024-06-18T11:31:51.750+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.445 seconds
[2024-06-18T11:32:22.704+0000] {processor.py:157} INFO - Started process (PID=5085) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:32:22.706+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:32:22.707+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:32:22.707+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:32:23.078+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:32:23.127+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:32:23,127 - Sync 1 DAGs
[2024-06-18T11:32:23.136+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:32:23,136 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:27:34.104021+00:00, run_after=2024-06-18T11:32:34.104021+00:00
[2024-06-18T11:32:23.148+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.448 seconds
[2024-06-18T11:32:53.268+0000] {processor.py:157} INFO - Started process (PID=5337) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:32:53.270+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:32:53.272+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:32:53.271+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:32:53.596+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:32:53.655+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:32:53,654 - Sync 1 DAGs
[2024-06-18T11:32:53.664+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:32:53,664 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:32:34.104021+00:00, run_after=2024-06-18T11:37:34.104021+00:00
[2024-06-18T11:32:53.673+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.412 seconds
[2024-06-18T11:33:24.539+0000] {processor.py:157} INFO - Started process (PID=5581) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:33:24.540+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:33:24.541+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:33:24.541+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:33:24.894+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:33:24.939+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:33:24,938 - Sync 1 DAGs
[2024-06-18T11:33:24.947+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:33:24,947 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:32:34.104021+00:00, run_after=2024-06-18T11:37:34.104021+00:00
[2024-06-18T11:33:24.956+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.419 seconds
[2024-06-18T11:33:55.343+0000] {processor.py:157} INFO - Started process (PID=5825) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:33:55.346+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:33:55.347+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:33:55.347+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:33:55.759+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:33:55.806+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:33:55,805 - Sync 1 DAGs
[2024-06-18T11:33:55.815+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:33:55,815 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:32:34.104021+00:00, run_after=2024-06-18T11:37:34.104021+00:00
[2024-06-18T11:33:55.824+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.489 seconds
[2024-06-18T11:34:26.424+0000] {processor.py:157} INFO - Started process (PID=6069) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:34:26.426+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:34:26.427+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:34:26.427+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:34:26.860+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:34:26.912+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:34:26,912 - Sync 1 DAGs
[2024-06-18T11:34:26.921+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:34:26,921 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:32:34.104021+00:00, run_after=2024-06-18T11:37:34.104021+00:00
[2024-06-18T11:34:26.929+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.509 seconds
[2024-06-18T11:34:57.801+0000] {processor.py:157} INFO - Started process (PID=6320) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:34:57.803+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:34:57.804+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:34:57.804+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:34:58.223+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:34:58.271+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:34:58,270 - Sync 1 DAGs
[2024-06-18T11:34:58.280+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:34:58,280 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:32:34.104021+00:00, run_after=2024-06-18T11:37:34.104021+00:00
[2024-06-18T11:34:58.288+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.492 seconds
[2024-06-18T11:35:29.105+0000] {processor.py:157} INFO - Started process (PID=6564) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:35:29.106+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:35:29.108+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:35:29.107+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:35:29.732+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:35:29.789+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:35:29,789 - Sync 1 DAGs
[2024-06-18T11:35:29.804+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:35:29,804 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:32:34.104021+00:00, run_after=2024-06-18T11:37:34.104021+00:00
[2024-06-18T11:35:29.825+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.726 seconds
[2024-06-18T11:36:00.248+0000] {processor.py:157} INFO - Started process (PID=6808) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:36:00.250+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:36:00.252+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:36:00.251+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:36:00.632+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:36:00.680+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:36:00,680 - Sync 1 DAGs
[2024-06-18T11:36:00.688+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:36:00,688 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:32:34.104021+00:00, run_after=2024-06-18T11:37:34.104021+00:00
[2024-06-18T11:36:00.697+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.458 seconds
[2024-06-18T11:36:31.471+0000] {processor.py:157} INFO - Started process (PID=7052) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:36:31.474+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:36:31.475+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:36:31.475+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:36:32.001+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:36:32.050+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:36:32,050 - Sync 1 DAGs
[2024-06-18T11:36:32.060+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:36:32,060 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:32:34.104021+00:00, run_after=2024-06-18T11:37:34.104021+00:00
[2024-06-18T11:36:32.069+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.607 seconds
[2024-06-18T11:37:02.186+0000] {processor.py:157} INFO - Started process (PID=7296) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:37:02.189+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:37:02.190+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:37:02.190+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:37:02.629+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:37:02.681+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:37:02,680 - Sync 1 DAGs
[2024-06-18T11:37:02.691+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:37:02,691 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:32:34.104021+00:00, run_after=2024-06-18T11:37:34.104021+00:00
[2024-06-18T11:37:02.701+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.522 seconds
[2024-06-18T11:37:33.692+0000] {processor.py:157} INFO - Started process (PID=7540) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:37:33.694+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:37:33.695+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:37:33.695+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:37:34.019+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:37:34.058+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:37:34,058 - Sync 1 DAGs
[2024-06-18T11:37:34.067+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:37:34,067 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:32:34.104021+00:00, run_after=2024-06-18T11:37:34.104021+00:00
[2024-06-18T11:37:34.075+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.388 seconds
[2024-06-18T11:38:04.570+0000] {processor.py:157} INFO - Started process (PID=7792) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:38:04.572+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:38:04.573+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:38:04.573+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:38:05.113+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:38:05.169+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:38:05,169 - Sync 1 DAGs
[2024-06-18T11:38:05.181+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:38:05,181 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:37:34.104021+00:00, run_after=2024-06-18T11:42:34.104021+00:00
[2024-06-18T11:38:05.190+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.627 seconds
[2024-06-18T11:38:35.314+0000] {processor.py:157} INFO - Started process (PID=8035) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:38:35.316+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:38:35.317+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:38:35.317+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:38:35.654+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:38:35.702+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:38:35,702 - Sync 1 DAGs
[2024-06-18T11:38:35.713+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:38:35,712 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:37:34.104021+00:00, run_after=2024-06-18T11:42:34.104021+00:00
[2024-06-18T11:38:35.722+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.415 seconds
[2024-06-18T11:39:06.234+0000] {processor.py:157} INFO - Started process (PID=8285) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:39:06.236+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:39:06.236+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:39:06.236+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:39:06.465+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:39:06.518+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:39:06,518 - Sync 1 DAGs
[2024-06-18T11:39:06.528+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:39:06,528 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:37:34.104021+00:00, run_after=2024-06-18T11:42:34.104021+00:00
[2024-06-18T11:39:06.537+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.305 seconds
[2024-06-18T11:39:36.807+0000] {processor.py:157} INFO - Started process (PID=8529) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:39:36.808+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:39:36.809+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:39:36.808+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:39:37.039+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:39:37.084+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:39:37,083 - Sync 1 DAGs
[2024-06-18T11:39:37.092+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:39:37,092 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:37:34.104021+00:00, run_after=2024-06-18T11:42:34.104021+00:00
[2024-06-18T11:39:37.100+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.295 seconds
[2024-06-18T11:40:07.340+0000] {processor.py:157} INFO - Started process (PID=8773) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:40:07.341+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:40:07.342+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:40:07.341+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:40:07.632+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:40:07.684+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:40:07,684 - Sync 1 DAGs
[2024-06-18T11:40:07.696+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:40:07,696 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:37:34.104021+00:00, run_after=2024-06-18T11:42:34.104021+00:00
[2024-06-18T11:40:07.705+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.367 seconds
[2024-06-18T11:40:38.214+0000] {processor.py:157} INFO - Started process (PID=9017) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:40:38.215+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:40:38.216+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:40:38.216+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:40:38.448+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:40:38.500+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:40:38,500 - Sync 1 DAGs
[2024-06-18T11:40:38.510+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:40:38,510 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:37:34.104021+00:00, run_after=2024-06-18T11:42:34.104021+00:00
[2024-06-18T11:40:38.520+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.308 seconds
[2024-06-18T11:41:08.780+0000] {processor.py:157} INFO - Started process (PID=9263) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:41:08.783+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:41:08.785+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:41:08.784+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:41:09.612+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:41:09.704+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:41:09,703 - Sync 1 DAGs
[2024-06-18T11:41:09.728+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:41:09,727 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:37:34.104021+00:00, run_after=2024-06-18T11:42:34.104021+00:00
[2024-06-18T11:41:09.747+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.979 seconds
[2024-06-18T11:41:40.360+0000] {processor.py:157} INFO - Started process (PID=9507) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:41:40.363+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:41:40.365+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:41:40.364+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:41:41.346+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:41:41.515+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:41:41,515 - Sync 1 DAGs
[2024-06-18T11:41:41.528+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:41:41,528 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:37:34.104021+00:00, run_after=2024-06-18T11:42:34.104021+00:00
[2024-06-18T11:41:41.539+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 1.188 seconds
[2024-06-18T11:42:11.763+0000] {processor.py:157} INFO - Started process (PID=9751) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:42:11.766+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:42:11.767+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:42:11.767+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:42:12.297+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:42:12.342+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:42:12,342 - Sync 1 DAGs
[2024-06-18T11:42:12.359+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:42:12,359 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:37:34.104021+00:00, run_after=2024-06-18T11:42:34.104021+00:00
[2024-06-18T11:42:12.373+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.619 seconds
[2024-06-18T11:42:42.454+0000] {processor.py:157} INFO - Started process (PID=10002) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:42:42.458+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:42:42.463+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:42:42.462+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:42:43.016+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:42:43.107+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:42:43,106 - Sync 1 DAGs
[2024-06-18T11:42:43.118+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:42:43,118 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:42:34.104021+00:00, run_after=2024-06-18T11:47:34.104021+00:00
[2024-06-18T11:42:43.130+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.703 seconds
[2024-06-18T11:43:13.535+0000] {processor.py:157} INFO - Started process (PID=10250) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:43:13.537+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:43:13.538+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:43:13.538+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:43:14.198+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:43:14.246+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:43:14,246 - Sync 1 DAGs
[2024-06-18T11:43:14.255+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:43:14,255 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:42:34.104021+00:00, run_after=2024-06-18T11:47:34.104021+00:00
[2024-06-18T11:43:14.263+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.758 seconds
[2024-06-18T11:43:44.601+0000] {processor.py:157} INFO - Started process (PID=10491) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:43:44.605+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:43:44.607+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:43:44.606+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:43:45.161+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:43:45.223+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:43:45,223 - Sync 1 DAGs
[2024-06-18T11:43:45.234+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:43:45,234 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:42:34.104021+00:00, run_after=2024-06-18T11:47:34.104021+00:00
[2024-06-18T11:43:45.244+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.656 seconds
[2024-06-18T11:44:15.704+0000] {processor.py:157} INFO - Started process (PID=10741) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:44:15.705+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:44:15.705+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:44:15.705+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:44:15.933+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:44:15.991+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:44:15,990 - Sync 1 DAGs
[2024-06-18T11:44:16.002+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:44:16,002 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:42:34.104021+00:00, run_after=2024-06-18T11:47:34.104021+00:00
[2024-06-18T11:44:16.011+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.309 seconds
[2024-06-18T11:44:46.954+0000] {processor.py:157} INFO - Started process (PID=10979) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:44:46.961+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:44:46.967+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:44:46.965+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:44:48.040+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:44:48.096+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:44:48,096 - Sync 1 DAGs
[2024-06-18T11:44:48.106+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:44:48,106 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:42:34.104021+00:00, run_after=2024-06-18T11:47:34.104021+00:00
[2024-06-18T11:44:48.115+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 1.199 seconds
[2024-06-18T11:45:18.258+0000] {processor.py:157} INFO - Started process (PID=11226) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:45:18.259+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:45:18.261+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:45:18.260+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:45:18.784+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:45:18.835+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:45:18,834 - Sync 1 DAGs
[2024-06-18T11:45:18.844+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:45:18,844 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:42:34.104021+00:00, run_after=2024-06-18T11:47:34.104021+00:00
[2024-06-18T11:45:18.852+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.602 seconds
[2024-06-18T11:45:48.972+0000] {processor.py:157} INFO - Started process (PID=11470) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:45:48.974+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:45:48.975+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:45:48.975+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:45:49.467+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:45:49.515+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:45:49,514 - Sync 1 DAGs
[2024-06-18T11:45:49.526+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:45:49,526 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:42:34.104021+00:00, run_after=2024-06-18T11:47:34.104021+00:00
[2024-06-18T11:45:49.535+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.569 seconds
[2024-06-18T11:46:20.233+0000] {processor.py:157} INFO - Started process (PID=11711) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:46:20.236+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:46:20.238+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:46:20.237+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:46:20.769+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:46:20.829+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:46:20,829 - Sync 1 DAGs
[2024-06-18T11:46:20.839+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:46:20,839 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:42:34.104021+00:00, run_after=2024-06-18T11:47:34.104021+00:00
[2024-06-18T11:46:20.847+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.638 seconds
[2024-06-18T11:46:51.092+0000] {processor.py:157} INFO - Started process (PID=11955) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:46:51.095+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:46:51.097+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:46:51.096+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:46:51.638+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:46:51.687+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:46:51,686 - Sync 1 DAGs
[2024-06-18T11:46:51.696+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:46:51,696 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:42:34.104021+00:00, run_after=2024-06-18T11:47:34.104021+00:00
[2024-06-18T11:46:51.704+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.623 seconds
[2024-06-18T11:47:22.665+0000] {processor.py:157} INFO - Started process (PID=12199) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:47:22.667+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:47:22.669+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:47:22.668+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:47:23.140+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:47:23.189+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:47:23,189 - Sync 1 DAGs
[2024-06-18T11:47:23.198+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:47:23,198 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:42:34.104021+00:00, run_after=2024-06-18T11:47:34.104021+00:00
[2024-06-18T11:47:23.208+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.550 seconds
[2024-06-18T11:47:53.773+0000] {processor.py:157} INFO - Started process (PID=12451) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:47:53.775+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:47:53.777+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:47:53.776+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:47:54.391+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:47:54.442+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:47:54,442 - Sync 1 DAGs
[2024-06-18T11:47:54.451+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:47:54,451 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:47:34.104021+00:00, run_after=2024-06-18T11:52:34.104021+00:00
[2024-06-18T11:47:54.459+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.695 seconds
[2024-06-18T11:48:24.636+0000] {processor.py:157} INFO - Started process (PID=12695) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:48:24.638+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:48:24.640+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:48:24.639+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:48:24.993+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:48:25.072+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:48:25,072 - Sync 1 DAGs
[2024-06-18T11:48:25.084+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:48:25,084 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:47:34.104021+00:00, run_after=2024-06-18T11:52:34.104021+00:00
[2024-06-18T11:48:25.093+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.472 seconds
[2024-06-18T11:48:55.543+0000] {processor.py:157} INFO - Started process (PID=12939) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:48:55.549+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:48:55.556+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:48:55.551+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:48:55.880+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:48:55.992+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:48:55,991 - Sync 1 DAGs
[2024-06-18T11:48:56.005+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:48:56,005 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:47:34.104021+00:00, run_after=2024-06-18T11:52:34.104021+00:00
[2024-06-18T11:48:56.016+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.501 seconds
[2024-06-18T11:49:26.983+0000] {processor.py:157} INFO - Started process (PID=13183) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:49:26.986+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:49:26.988+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:49:26.987+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:49:27.381+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:49:27.457+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:49:27,457 - Sync 1 DAGs
[2024-06-18T11:49:27.470+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:49:27,470 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:47:34.104021+00:00, run_after=2024-06-18T11:52:34.104021+00:00
[2024-06-18T11:49:27.479+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.513 seconds
[2024-06-18T11:49:57.933+0000] {processor.py:157} INFO - Started process (PID=13427) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:49:57.937+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:49:57.940+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:49:57.939+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:49:58.224+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:49:58.281+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:49:58,280 - Sync 1 DAGs
[2024-06-18T11:49:58.292+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:49:58,292 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:47:34.104021+00:00, run_after=2024-06-18T11:52:34.104021+00:00
[2024-06-18T11:49:58.301+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.381 seconds
[2024-06-18T11:50:29.141+0000] {processor.py:157} INFO - Started process (PID=13671) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:50:29.145+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:50:29.147+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:50:29.146+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:50:29.436+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:50:29.505+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:50:29,504 - Sync 1 DAGs
[2024-06-18T11:50:29.515+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:50:29,515 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:47:34.104021+00:00, run_after=2024-06-18T11:52:34.104021+00:00
[2024-06-18T11:50:29.524+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.402 seconds
[2024-06-18T11:50:59.613+0000] {processor.py:157} INFO - Started process (PID=13914) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:50:59.617+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:50:59.620+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:50:59.619+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:50:59.947+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:51:00.007+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:51:00,007 - Sync 1 DAGs
[2024-06-18T11:51:00.033+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:51:00,033 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:47:34.104021+00:00, run_after=2024-06-18T11:52:34.104021+00:00
[2024-06-18T11:51:00.044+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.454 seconds
[2024-06-18T11:51:30.693+0000] {processor.py:157} INFO - Started process (PID=14162) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:51:30.696+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:51:30.701+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:51:30.701+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:51:30.737+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:51:30.733+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/dag_kafka_spark.py", line 7, in <module>
    from src.kafka_client.kafka_stream_data import stream
  File "/opt/airflow/dags/src/kafka_client/kafka_stream_data.py", line 1, in <module>
    from src.constants import (
ImportError: cannot import name 'yaml_data' from 'src.constants' (/opt/airflow/dags/src/constants.py)
[2024-06-18T11:51:30.738+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:51:30.780+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.094 seconds
[2024-06-18T11:52:01.315+0000] {processor.py:157} INFO - Started process (PID=14412) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:52:01.318+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:52:01.320+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:52:01.319+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:52:01.347+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:52:01.344+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/dag_kafka_spark.py", line 7, in <module>
    from src.kafka_client.kafka_stream_data import stream
  File "/opt/airflow/dags/src/kafka_client/kafka_stream_data.py", line 1, in <module>
    from src.constants import (
ImportError: cannot import name 'yaml_data' from 'src.constants' (/opt/airflow/dags/src/constants.py)
[2024-06-18T11:52:01.347+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:52:01.377+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.074 seconds
[2024-06-18T11:52:31.520+0000] {processor.py:157} INFO - Started process (PID=14655) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:52:31.523+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:52:31.525+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:52:31.524+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:52:31.542+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:52:31.539+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/dag_kafka_spark.py", line 7, in <module>
    from src.kafka_client.kafka_stream_data import stream
  File "/opt/airflow/dags/src/kafka_client/kafka_stream_data.py", line 1, in <module>
    from src.constants import (
ImportError: cannot import name 'yaml_data' from 'src.constants' (/opt/airflow/dags/src/constants.py)
[2024-06-18T11:52:31.543+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:52:31.569+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.059 seconds
[2024-06-18T11:53:02.231+0000] {processor.py:157} INFO - Started process (PID=14898) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:53:02.233+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:53:02.235+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:53:02.234+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:53:02.252+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:53:02.249+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/dag_kafka_spark.py", line 7, in <module>
    from src.kafka_client.kafka_stream_data import stream
  File "/opt/airflow/dags/src/kafka_client/kafka_stream_data.py", line 1, in <module>
    from src.constants import (
ImportError: cannot import name 'yaml_data' from 'src.constants' (/opt/airflow/dags/src/constants.py)
[2024-06-18T11:53:02.252+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:53:02.273+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.050 seconds
[2024-06-18T11:53:32.413+0000] {processor.py:157} INFO - Started process (PID=15141) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:53:32.414+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:53:32.416+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:53:32.415+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:53:32.435+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:53:32.433+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/dag_kafka_spark.py", line 7, in <module>
    from src.kafka_client.kafka_stream_data import stream
  File "/opt/airflow/dags/src/kafka_client/kafka_stream_data.py", line 1, in <module>
    from src.constants import (
ImportError: cannot import name 'yaml_data' from 'src.constants' (/opt/airflow/dags/src/constants.py)
[2024-06-18T11:53:32.436+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:53:32.461+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.055 seconds
[2024-06-18T11:53:34.477+0000] {processor.py:157} INFO - Started process (PID=15146) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:53:34.479+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:53:34.480+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:53:34.480+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:53:34.513+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:53:34.501+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/dag_kafka_spark.py", line 7, in <module>
    from src.kafka_client.kafka_stream_data import stream
  File "/opt/airflow/dags/src/kafka_client/kafka_stream_data.py", line 1, in <module>
    from src.constants import (
ImportError: cannot import name 'yaml_data' from 'src.constants' (/opt/airflow/dags/src/constants.py)
[2024-06-18T11:53:34.514+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:53:34.545+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.074 seconds
[2024-06-18T11:53:35.593+0000] {processor.py:157} INFO - Started process (PID=15157) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:53:35.595+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:53:35.596+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:53:35.596+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:53:35.613+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:53:35.611+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/dag_kafka_spark.py", line 7, in <module>
    from src.kafka_client.kafka_stream_data import stream
  File "/opt/airflow/dags/src/kafka_client/kafka_stream_data.py", line 1, in <module>
    from src.constants import (
ImportError: cannot import name 'yaml_data' from 'src.constants' (/opt/airflow/dags/src/constants.py)
[2024-06-18T11:53:35.613+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:53:35.633+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.046 seconds
[2024-06-18T11:54:05.955+0000] {processor.py:157} INFO - Started process (PID=15400) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:54:05.956+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:54:05.956+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:54:05.956+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:54:05.968+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:54:05.964+0000] {dagbag.py:346} ERROR - Failed to import: /opt/airflow/dags/dag_kafka_spark.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/dag_kafka_spark.py", line 7, in <module>
    from src.kafka_client.kafka_stream_data import stream
  File "/opt/airflow/dags/src/kafka_client/kafka_stream_data.py", line 1, in <module>
    from src.constants import (
ImportError: cannot import name 'yaml_data' from 'src.constants' (/opt/airflow/dags/src/constants.py)
[2024-06-18T11:54:05.968+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:54:05.992+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.041 seconds
[2024-06-18T11:54:36.293+0000] {processor.py:157} INFO - Started process (PID=15643) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:54:36.294+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:54:36.295+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:54:36.295+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:54:36.466+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:54:36.531+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:54:36,529 - Sync 1 DAGs
[2024-06-18T11:54:36.546+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:54:36,546 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:49:36.546386+00:00, run_after=2024-06-18T11:54:36.546386+00:00
[2024-06-18T11:54:36.564+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.273 seconds
[2024-06-18T11:55:07.343+0000] {processor.py:157} INFO - Started process (PID=15881) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:55:07.345+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:55:07.347+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:55:07.346+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:55:07.636+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:55:07.702+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:55:07,702 - Sync 1 DAGs
[2024-06-18T11:55:07.714+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:55:07,714 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:50:07.714228+00:00, run_after=2024-06-18T11:55:07.714228+00:00
[2024-06-18T11:55:07.724+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.390 seconds
[2024-06-18T11:55:37.806+0000] {processor.py:157} INFO - Started process (PID=16125) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:55:37.808+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:55:37.809+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:55:37.809+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:55:38.090+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:55:38.144+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:55:38,144 - Sync 1 DAGs
[2024-06-18T11:55:38.155+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:55:38,154 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:50:38.154799+00:00, run_after=2024-06-18T11:55:38.154799+00:00
[2024-06-18T11:55:38.164+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.365 seconds
[2024-06-18T11:56:08.394+0000] {processor.py:157} INFO - Started process (PID=16369) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:56:08.396+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:56:08.397+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:56:08.397+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:56:08.693+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:56:08.759+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:56:08,758 - Sync 1 DAGs
[2024-06-18T11:56:08.770+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:56:08,770 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:51:08.770106+00:00, run_after=2024-06-18T11:56:08.770106+00:00
[2024-06-18T11:56:08.781+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.393 seconds
[2024-06-18T11:56:39.739+0000] {processor.py:157} INFO - Started process (PID=16613) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:56:39.740+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:56:39.742+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:56:39.741+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:56:40.021+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:56:40.099+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:56:40,099 - Sync 1 DAGs
[2024-06-18T11:56:40.110+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:56:40,110 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:51:40.109953+00:00, run_after=2024-06-18T11:56:40.109953+00:00
[2024-06-18T11:56:40.120+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.388 seconds
[2024-06-18T11:57:10.176+0000] {processor.py:157} INFO - Started process (PID=16860) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:57:10.177+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:57:10.178+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:57:10.178+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:57:10.427+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:57:10.490+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:57:10,490 - Sync 1 DAGs
[2024-06-18T11:57:10.500+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:57:10,500 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:52:10.500571+00:00, run_after=2024-06-18T11:57:10.500571+00:00
[2024-06-18T11:57:10.509+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.337 seconds
[2024-06-18T11:57:41.376+0000] {processor.py:157} INFO - Started process (PID=17101) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:57:41.377+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:57:41.378+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:57:41.378+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:57:41.636+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:57:41.690+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:57:41,689 - Sync 1 DAGs
[2024-06-18T11:57:41.700+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:57:41,699 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:52:41.699803+00:00, run_after=2024-06-18T11:57:41.699803+00:00
[2024-06-18T11:57:41.708+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.339 seconds
[2024-06-18T11:58:12.660+0000] {processor.py:157} INFO - Started process (PID=17345) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:58:12.662+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:58:12.664+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:58:12.663+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:58:12.961+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:58:13.024+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:58:13,023 - Sync 1 DAGs
[2024-06-18T11:58:13.052+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:58:13,051 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:53:13.051617+00:00, run_after=2024-06-18T11:58:13.051617+00:00
[2024-06-18T11:58:13.082+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.440 seconds
[2024-06-18T11:58:43.933+0000] {processor.py:157} INFO - Started process (PID=17589) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:58:43.935+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:58:43.937+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:58:43.937+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:58:44.388+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:58:44.469+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:58:44,469 - Sync 1 DAGs
[2024-06-18T11:58:44.483+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:58:44,483 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:53:44.483158+00:00, run_after=2024-06-18T11:58:44.483158+00:00
[2024-06-18T11:58:44.495+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.569 seconds
[2024-06-18T11:59:14.839+0000] {processor.py:157} INFO - Started process (PID=17841) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:59:14.845+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:59:14.854+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:59:14.848+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:59:16.186+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:59:16.303+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:59:16,303 - Sync 1 DAGs
[2024-06-18T11:59:16.314+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:59:16,314 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:58:44.483158+00:00, run_after=2024-06-18T12:03:44.483158+00:00
[2024-06-18T11:59:16.326+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 1.506 seconds
[2024-06-18T11:59:46.636+0000] {processor.py:157} INFO - Started process (PID=18085) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:59:46.637+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T11:59:46.638+0000] {logging_mixin.py:154} INFO - [2024-06-18T11:59:46.638+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:59:46.978+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T11:59:47.316+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:59:47,315 - Sync 1 DAGs
[2024-06-18T11:59:47.441+0000] {logging_mixin.py:154} WARNING - 2024-06-18 11:59:47,441 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:58:44.483158+00:00, run_after=2024-06-18T12:03:44.483158+00:00
[2024-06-18T11:59:47.480+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.848 seconds
[2024-06-18T12:00:17.539+0000] {processor.py:157} INFO - Started process (PID=18352) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:00:17.539+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:00:17.540+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:00:17.540+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:00:17.746+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:00:17.808+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:00:17,808 - Sync 1 DAGs
[2024-06-18T12:00:17.819+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:00:17,819 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:58:44.483158+00:00, run_after=2024-06-18T12:03:44.483158+00:00
[2024-06-18T12:00:17.828+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.291 seconds
[2024-06-18T12:00:47.955+0000] {processor.py:157} INFO - Started process (PID=18581) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:00:47.957+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:00:47.958+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:00:47.957+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:00:48.275+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:00:48.619+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:00:48,619 - Sync 1 DAGs
[2024-06-18T12:00:48.666+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:00:48,666 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:58:44.483158+00:00, run_after=2024-06-18T12:03:44.483158+00:00
[2024-06-18T12:00:48.759+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.810 seconds
[2024-06-18T12:01:19.444+0000] {processor.py:157} INFO - Started process (PID=18840) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:01:19.445+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:01:19.450+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:01:19.449+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:01:19.715+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:01:19.777+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:01:19,776 - Sync 1 DAGs
[2024-06-18T12:01:19.786+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:01:19,786 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:58:44.483158+00:00, run_after=2024-06-18T12:03:44.483158+00:00
[2024-06-18T12:01:19.795+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.356 seconds
[2024-06-18T12:01:49.897+0000] {processor.py:157} INFO - Started process (PID=19104) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:01:49.897+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:01:49.898+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:01:49.898+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:01:50.143+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:01:50.222+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:01:50,222 - Sync 1 DAGs
[2024-06-18T12:01:50.236+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:01:50,236 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:58:44.483158+00:00, run_after=2024-06-18T12:03:44.483158+00:00
[2024-06-18T12:01:50.250+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.355 seconds
[2024-06-18T12:02:20.458+0000] {processor.py:157} INFO - Started process (PID=19328) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:02:20.459+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:02:20.460+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:02:20.460+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:02:20.711+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:02:20.770+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:02:20,770 - Sync 1 DAGs
[2024-06-18T12:02:20.782+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:02:20,782 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:58:44.483158+00:00, run_after=2024-06-18T12:03:44.483158+00:00
[2024-06-18T12:02:20.794+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.341 seconds
[2024-06-18T12:02:50.919+0000] {processor.py:157} INFO - Started process (PID=19571) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:02:50.921+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:02:50.923+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:02:50.922+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:02:51.177+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:02:51.236+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:02:51,236 - Sync 1 DAGs
[2024-06-18T12:02:51.247+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:02:51,247 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T11:58:44.483158+00:00, run_after=2024-06-18T12:03:44.483158+00:00
[2024-06-18T12:02:51.256+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.345 seconds
[2024-06-18T12:06:12.803+0000] {processor.py:157} INFO - Started process (PID=19800) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:06:12.805+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:06:12.808+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:06:12.807+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:06:13.201+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:06:13.279+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:06:13,278 - Sync 1 DAGs
[2024-06-18T12:06:13.293+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:06:13,293 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:01:13.293298+00:00, run_after=2024-06-18T12:06:13.293298+00:00
[2024-06-18T12:06:13.309+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.521 seconds
[2024-06-18T12:06:43.689+0000] {processor.py:157} INFO - Started process (PID=20071) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:06:43.689+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:06:43.690+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:06:43.690+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:06:43.995+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:06:44.087+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:06:44,086 - Sync 1 DAGs
[2024-06-18T12:06:44.099+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:06:44,099 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:06:13.293298+00:00, run_after=2024-06-18T12:11:13.293298+00:00
[2024-06-18T12:06:44.114+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.428 seconds
[2024-06-18T12:07:14.967+0000] {processor.py:157} INFO - Started process (PID=20332) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:07:14.968+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:07:14.969+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:07:14.969+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:07:15.220+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:07:15.292+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:07:15,291 - Sync 1 DAGs
[2024-06-18T12:07:15.302+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:07:15,302 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:06:13.293298+00:00, run_after=2024-06-18T12:11:13.293298+00:00
[2024-06-18T12:07:15.312+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.349 seconds
[2024-06-18T12:07:45.439+0000] {processor.py:157} INFO - Started process (PID=20580) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:07:45.440+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:07:45.441+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:07:45.441+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:07:45.672+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:07:45.745+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:07:45,745 - Sync 1 DAGs
[2024-06-18T12:07:45.756+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:07:45,756 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:06:13.293298+00:00, run_after=2024-06-18T12:11:13.293298+00:00
[2024-06-18T12:07:45.765+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.329 seconds
[2024-06-18T12:08:16.712+0000] {processor.py:157} INFO - Started process (PID=20844) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:08:16.713+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:08:16.715+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:08:16.715+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:08:16.977+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:08:17.034+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:08:17,033 - Sync 1 DAGs
[2024-06-18T12:08:17.045+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:08:17,045 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:06:13.293298+00:00, run_after=2024-06-18T12:11:13.293298+00:00
[2024-06-18T12:08:17.057+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.350 seconds
[2024-06-18T12:08:47.140+0000] {processor.py:157} INFO - Started process (PID=21089) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:08:47.141+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:08:47.143+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:08:47.142+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:08:48.110+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:08:48.235+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:08:48,234 - Sync 1 DAGs
[2024-06-18T12:08:48.248+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:08:48,247 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:06:13.293298+00:00, run_after=2024-06-18T12:11:13.293298+00:00
[2024-06-18T12:08:48.259+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 1.124 seconds
[2024-06-18T12:09:18.699+0000] {processor.py:157} INFO - Started process (PID=21343) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:09:18.701+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:09:18.702+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:09:18.702+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:09:18.955+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:09:19.021+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:09:19,021 - Sync 1 DAGs
[2024-06-18T12:09:19.036+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:09:19,036 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:06:13.293298+00:00, run_after=2024-06-18T12:11:13.293298+00:00
[2024-06-18T12:09:19.046+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.352 seconds
[2024-06-18T12:09:49.179+0000] {processor.py:157} INFO - Started process (PID=21607) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:09:49.180+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:09:49.180+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:09:49.180+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:09:49.407+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:09:49.525+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:09:49,525 - Sync 1 DAGs
[2024-06-18T12:09:49.536+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:09:49,536 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:06:13.293298+00:00, run_after=2024-06-18T12:11:13.293298+00:00
[2024-06-18T12:09:49.545+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.368 seconds
[2024-06-18T12:10:19.589+0000] {processor.py:157} INFO - Started process (PID=21851) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:10:19.590+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:10:19.590+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:10:19.590+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:10:19.802+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:10:19.879+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:10:19,879 - Sync 1 DAGs
[2024-06-18T12:10:19.890+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:10:19,890 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:06:13.293298+00:00, run_after=2024-06-18T12:11:13.293298+00:00
[2024-06-18T12:10:19.899+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.311 seconds
[2024-06-18T12:10:50.394+0000] {processor.py:157} INFO - Started process (PID=22075) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:10:50.395+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:10:50.395+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:10:50.395+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:10:50.672+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:10:50.756+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:10:50,756 - Sync 1 DAGs
[2024-06-18T12:10:50.771+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:10:50,771 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:06:13.293298+00:00, run_after=2024-06-18T12:11:13.293298+00:00
[2024-06-18T12:10:50.784+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.393 seconds
[2024-06-18T12:11:20.813+0000] {processor.py:157} INFO - Started process (PID=22343) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:11:20.814+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:11:20.814+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:11:20.814+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:11:21.038+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:11:21.109+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:11:21,109 - Sync 1 DAGs
[2024-06-18T12:11:21.121+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:11:21,120 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:11:13.293298+00:00, run_after=2024-06-18T12:16:13.293298+00:00
[2024-06-18T12:11:21.131+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.320 seconds
[2024-06-18T12:11:51.175+0000] {processor.py:157} INFO - Started process (PID=22607) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:11:51.176+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:11:51.177+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:11:51.177+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:11:51.433+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:11:51.506+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:11:51,506 - Sync 1 DAGs
[2024-06-18T12:11:51.525+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:11:51,524 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:11:13.293298+00:00, run_after=2024-06-18T12:16:13.293298+00:00
[2024-06-18T12:11:51.569+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.396 seconds
[2024-06-18T12:12:21.609+0000] {processor.py:157} INFO - Started process (PID=22872) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:12:21.610+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:12:21.611+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:12:21.611+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:12:21.812+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:12:21.894+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:12:21,893 - Sync 1 DAGs
[2024-06-18T12:12:21.906+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:12:21,906 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:11:13.293298+00:00, run_after=2024-06-18T12:16:13.293298+00:00
[2024-06-18T12:12:21.919+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.312 seconds
[2024-06-18T12:12:52.023+0000] {processor.py:157} INFO - Started process (PID=23117) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:12:52.024+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:12:52.024+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:12:52.024+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:12:52.257+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:12:52.319+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:12:52,319 - Sync 1 DAGs
[2024-06-18T12:12:52.332+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:12:52,332 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:11:13.293298+00:00, run_after=2024-06-18T12:16:13.293298+00:00
[2024-06-18T12:12:52.343+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.321 seconds
[2024-06-18T12:13:22.503+0000] {processor.py:157} INFO - Started process (PID=23303) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:13:22.505+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:13:22.506+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:13:22.506+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:13:22.746+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:13:22.807+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:13:22,807 - Sync 1 DAGs
[2024-06-18T12:13:22.819+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:13:22,819 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:11:13.293298+00:00, run_after=2024-06-18T12:16:13.293298+00:00
[2024-06-18T12:13:23.333+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.836 seconds
[2024-06-18T12:13:54.181+0000] {processor.py:157} INFO - Started process (PID=23592) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:13:54.184+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:13:54.185+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:13:54.185+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:13:54.476+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:13:54.534+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:13:54,533 - Sync 1 DAGs
[2024-06-18T12:13:54.545+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:13:54,545 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:11:13.293298+00:00, run_after=2024-06-18T12:16:13.293298+00:00
[2024-06-18T12:13:54.765+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.588 seconds
[2024-06-18T12:14:24.969+0000] {processor.py:157} INFO - Started process (PID=23836) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:14:24.971+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:14:24.972+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:14:24.972+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:14:25.255+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:14:25.330+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:14:25,329 - Sync 1 DAGs
[2024-06-18T12:14:25.343+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:14:25,342 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:11:13.293298+00:00, run_after=2024-06-18T12:16:13.293298+00:00
[2024-06-18T12:14:25.356+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.391 seconds
[2024-06-18T12:14:56.350+0000] {processor.py:157} INFO - Started process (PID=24099) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:14:56.352+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:14:56.353+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:14:56.353+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:14:56.632+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:14:56.690+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:14:56,690 - Sync 1 DAGs
[2024-06-18T12:14:56.701+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:14:56,701 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:11:13.293298+00:00, run_after=2024-06-18T12:16:13.293298+00:00
[2024-06-18T12:14:56.711+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.367 seconds
[2024-06-18T12:15:27.490+0000] {processor.py:157} INFO - Started process (PID=24343) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:15:27.492+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:15:27.493+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:15:27.493+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:15:27.724+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:15:27.966+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:15:27,966 - Sync 1 DAGs
[2024-06-18T12:15:27.978+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:15:27,977 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:11:13.293298+00:00, run_after=2024-06-18T12:16:13.293298+00:00
[2024-06-18T12:15:27.993+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.508 seconds
[2024-06-18T12:15:58.948+0000] {processor.py:157} INFO - Started process (PID=24612) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:15:58.950+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:15:58.951+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:15:58.951+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:15:59.185+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:15:59.235+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:15:59,235 - Sync 1 DAGs
[2024-06-18T12:15:59.463+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:15:59,463 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:11:13.293298+00:00, run_after=2024-06-18T12:16:13.293298+00:00
[2024-06-18T12:15:59.473+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.528 seconds
[2024-06-18T12:16:30.288+0000] {processor.py:157} INFO - Started process (PID=24885) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:16:30.291+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:16:30.292+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:16:30.292+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:16:30.550+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:16:30.607+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:16:30,607 - Sync 1 DAGs
[2024-06-18T12:16:30.806+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:16:30,806 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:16:13.293298+00:00, run_after=2024-06-18T12:21:13.293298+00:00
[2024-06-18T12:16:30.815+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.531 seconds
[2024-06-18T12:17:01.720+0000] {processor.py:157} INFO - Started process (PID=25169) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:17:01.722+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:17:01.723+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:17:01.723+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:17:01.952+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:17:02.002+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:17:02,002 - Sync 1 DAGs
[2024-06-18T12:17:02.011+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:17:02,011 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:16:13.293298+00:00, run_after=2024-06-18T12:21:13.293298+00:00
[2024-06-18T12:17:02.222+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.507 seconds
[2024-06-18T12:17:32.249+0000] {processor.py:157} INFO - Started process (PID=25423) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:17:32.250+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:17:32.251+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:17:32.251+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:17:32.399+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:17:32.645+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:17:32,645 - Sync 1 DAGs
[2024-06-18T12:17:32.654+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:17:32,654 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:16:13.293298+00:00, run_after=2024-06-18T12:21:13.293298+00:00
[2024-06-18T12:17:32.662+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.414 seconds
[2024-06-18T12:18:03.230+0000] {processor.py:157} INFO - Started process (PID=25661) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:18:03.232+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:18:03.233+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:18:03.233+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:18:03.489+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:18:03.711+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:18:03,711 - Sync 1 DAGs
[2024-06-18T12:18:03.719+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:18:03,719 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:16:13.293298+00:00, run_after=2024-06-18T12:21:13.293298+00:00
[2024-06-18T12:18:03.727+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.502 seconds
[2024-06-18T12:18:33.792+0000] {processor.py:157} INFO - Started process (PID=25911) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:18:33.793+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:18:33.794+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:18:33.794+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:18:33.943+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:18:34.164+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:18:34,163 - Sync 1 DAGs
[2024-06-18T12:18:34.178+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:18:34,178 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:16:13.293298+00:00, run_after=2024-06-18T12:21:13.293298+00:00
[2024-06-18T12:18:34.188+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.399 seconds
[2024-06-18T12:19:04.890+0000] {processor.py:157} INFO - Started process (PID=26154) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:19:04.892+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:19:04.893+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:19:04.893+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:19:05.088+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:19:05.308+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:19:05,307 - Sync 1 DAGs
[2024-06-18T12:19:05.317+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:19:05,317 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:16:13.293298+00:00, run_after=2024-06-18T12:21:13.293298+00:00
[2024-06-18T12:19:05.325+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.439 seconds
[2024-06-18T12:19:36.164+0000] {processor.py:157} INFO - Started process (PID=26398) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:19:36.166+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:19:36.167+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:19:36.167+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:19:36.335+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:19:36.623+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:19:36,623 - Sync 1 DAGs
[2024-06-18T12:19:36.634+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:19:36,634 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:16:13.293298+00:00, run_after=2024-06-18T12:21:13.293298+00:00
[2024-06-18T12:19:36.643+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.485 seconds
[2024-06-18T12:20:06.808+0000] {processor.py:157} INFO - Started process (PID=26642) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:20:06.810+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:20:06.811+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:20:06.811+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:20:06.989+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:20:07.261+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:20:07,261 - Sync 1 DAGs
[2024-06-18T12:20:07.270+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:20:07,270 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:16:13.293298+00:00, run_after=2024-06-18T12:21:13.293298+00:00
[2024-06-18T12:20:07.280+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.476 seconds
[2024-06-18T12:20:37.860+0000] {processor.py:157} INFO - Started process (PID=26887) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:20:37.862+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:20:37.863+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:20:37.863+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:20:38.029+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:20:38.272+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:20:38,272 - Sync 1 DAGs
[2024-06-18T12:20:38.281+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:20:38,281 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:16:13.293298+00:00, run_after=2024-06-18T12:21:13.293298+00:00
[2024-06-18T12:20:38.291+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.436 seconds
[2024-06-18T12:21:08.347+0000] {processor.py:157} INFO - Started process (PID=27131) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:21:08.349+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:21:08.350+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:21:08.350+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:21:08.514+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:21:08.787+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:21:08,787 - Sync 1 DAGs
[2024-06-18T12:21:08.796+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:21:08,796 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:16:13.293298+00:00, run_after=2024-06-18T12:21:13.293298+00:00
[2024-06-18T12:21:08.804+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.462 seconds
[2024-06-18T12:21:38.906+0000] {processor.py:157} INFO - Started process (PID=27379) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:21:38.909+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:21:38.910+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:21:38.910+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:21:39.070+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:21:39.316+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:21:39,315 - Sync 1 DAGs
[2024-06-18T12:21:39.325+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:21:39,325 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:21:13.293298+00:00, run_after=2024-06-18T12:26:13.293298+00:00
[2024-06-18T12:21:39.334+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.433 seconds
[2024-06-18T12:22:10.049+0000] {processor.py:157} INFO - Started process (PID=27623) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:22:10.052+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:22:10.054+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:22:10.053+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:22:10.273+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:22:10.544+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:22:10,544 - Sync 1 DAGs
[2024-06-18T12:22:10.555+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:22:10,555 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:21:13.293298+00:00, run_after=2024-06-18T12:26:13.293298+00:00
[2024-06-18T12:22:10.566+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.525 seconds
[2024-06-18T12:22:40.805+0000] {processor.py:157} INFO - Started process (PID=27871) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:22:40.807+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:22:40.808+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:22:40.807+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:22:40.999+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:22:41.276+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:22:41,275 - Sync 1 DAGs
[2024-06-18T12:22:41.285+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:22:41,285 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:21:13.293298+00:00, run_after=2024-06-18T12:26:13.293298+00:00
[2024-06-18T12:22:41.294+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.496 seconds
[2024-06-18T12:23:11.456+0000] {processor.py:157} INFO - Started process (PID=28115) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:23:11.459+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:23:11.460+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:23:11.460+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:23:11.899+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:23:11.954+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:23:11,954 - Sync 1 DAGs
[2024-06-18T12:23:11.964+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:23:11,964 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:21:13.293298+00:00, run_after=2024-06-18T12:26:13.293298+00:00
[2024-06-18T12:23:11.973+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.535 seconds
[2024-06-18T12:23:42.589+0000] {processor.py:157} INFO - Started process (PID=28359) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:23:42.591+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:23:42.592+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:23:42.592+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:23:42.798+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:23:43.079+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:23:43,079 - Sync 1 DAGs
[2024-06-18T12:23:43.088+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:23:43,088 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:21:13.293298+00:00, run_after=2024-06-18T12:26:13.293298+00:00
[2024-06-18T12:23:43.096+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.512 seconds
[2024-06-18T12:24:13.304+0000] {processor.py:157} INFO - Started process (PID=28609) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:24:13.306+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:24:13.307+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:24:13.307+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:24:13.567+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:24:13.912+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:24:13,912 - Sync 1 DAGs
[2024-06-18T12:24:13.924+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:24:13,923 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:21:13.293298+00:00, run_after=2024-06-18T12:26:13.293298+00:00
[2024-06-18T12:24:13.933+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.636 seconds
[2024-06-18T12:24:16.338+0000] {processor.py:157} INFO - Started process (PID=28615) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:24:16.338+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:24:16.339+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:24:16.339+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:24:16.514+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:24:16.523+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:24:16,522 - Sync 1 DAGs
[2024-06-18T12:24:16.627+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:24:16,627 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:21:13.293298+00:00, run_after=2024-06-18T12:26:13.293298+00:00
[2024-06-18T12:24:16.636+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.301 seconds
[2024-06-18T12:31:02.141+0000] {processor.py:157} INFO - Started process (PID=167) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:31:02.142+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:31:02.143+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:31:02.142+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:31:02.887+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:31:02.995+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:31:02,994 - Sync 1 DAGs
[2024-06-18T12:31:03.009+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:31:03,009 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:26:03.009106+00:00, run_after=2024-06-18T12:31:03.009106+00:00
[2024-06-18T12:31:03.028+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.891 seconds
[2024-06-18T12:31:33.140+0000] {processor.py:157} INFO - Started process (PID=444) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:31:33.141+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:31:33.142+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:31:33.141+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:31:33.598+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:31:33.664+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:31:33,663 - Sync 1 DAGs
[2024-06-18T12:31:33.678+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:31:33,678 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:26:33.678059+00:00, run_after=2024-06-18T12:31:33.678059+00:00
[2024-06-18T12:31:33.702+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.565 seconds
[2024-06-18T12:32:04.791+0000] {processor.py:157} INFO - Started process (PID=710) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:32:04.798+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:32:04.801+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:32:04.800+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:32:05.261+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:32:05.341+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:32:05,341 - Sync 1 DAGs
[2024-06-18T12:32:05.357+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:32:05,356 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:27:05.356757+00:00, run_after=2024-06-18T12:32:05.356757+00:00
[2024-06-18T12:32:05.370+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.585 seconds
[2024-06-18T12:32:35.669+0000] {processor.py:157} INFO - Started process (PID=944) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:32:35.671+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:32:35.672+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:32:35.672+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:32:36.184+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:32:36.194+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:32:36,193 - Sync 1 DAGs
[2024-06-18T12:32:36.212+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:32:36,212 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:27:36.212173+00:00, run_after=2024-06-18T12:32:36.212173+00:00
[2024-06-18T12:32:36.227+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.562 seconds
[2024-06-18T12:33:06.455+0000] {processor.py:157} INFO - Started process (PID=1188) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:33:06.457+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:33:06.459+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:33:06.459+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:33:07.093+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:33:07.166+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:33:07,166 - Sync 1 DAGs
[2024-06-18T12:33:07.179+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:33:07,179 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:28:07.178906+00:00, run_after=2024-06-18T12:33:07.178906+00:00
[2024-06-18T12:33:07.190+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.740 seconds
[2024-06-18T12:33:37.237+0000] {processor.py:157} INFO - Started process (PID=1450) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:33:37.241+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:33:37.242+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:33:37.242+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:33:39.642+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:33:39.661+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:33:39,660 - Sync 1 DAGs
[2024-06-18T12:33:39.695+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:33:39,695 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:28:39.695643+00:00, run_after=2024-06-18T12:33:39.695643+00:00
[2024-06-18T12:33:39.716+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 2.481 seconds
[2024-06-18T12:34:09.921+0000] {processor.py:157} INFO - Started process (PID=1711) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:34:09.930+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:34:09.950+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:34:09.940+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:34:12.749+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:34:12.763+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:34:12,762 - Sync 1 DAGs
[2024-06-18T12:34:12.778+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:34:12,778 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:29:12.778685+00:00, run_after=2024-06-18T12:34:12.778685+00:00
[2024-06-18T12:34:12.805+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 2.897 seconds
[2024-06-18T12:34:43.257+0000] {processor.py:157} INFO - Started process (PID=1995) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:34:43.258+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:34:43.258+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:34:43.258+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:34:43.787+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:34:43.800+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:34:43,799 - Sync 1 DAGs
[2024-06-18T12:34:43.817+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:34:43,817 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:29:43.817523+00:00, run_after=2024-06-18T12:34:43.817523+00:00
[2024-06-18T12:34:43.842+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.588 seconds
[2024-06-18T12:35:14.108+0000] {processor.py:157} INFO - Started process (PID=2239) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:35:14.110+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:35:14.110+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:35:14.110+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:35:14.424+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:35:14.469+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:35:14,469 - Sync 1 DAGs
[2024-06-18T12:35:14.478+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:35:14,478 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:30:14.478530+00:00, run_after=2024-06-18T12:35:14.478530+00:00
[2024-06-18T12:35:14.486+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.381 seconds
[2024-06-18T12:35:44.591+0000] {processor.py:157} INFO - Started process (PID=2506) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:35:44.592+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:35:44.592+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:35:44.592+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:35:44.932+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:35:44.984+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:35:44,984 - Sync 1 DAGs
[2024-06-18T12:35:44.995+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:35:44,995 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:30:44.995151+00:00, run_after=2024-06-18T12:35:44.995151+00:00
[2024-06-18T12:35:45.005+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.416 seconds
[2024-06-18T12:36:15.385+0000] {processor.py:157} INFO - Started process (PID=2727) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:36:15.389+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:36:15.393+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:36:15.392+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:36:16.224+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:36:16.284+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:36:16,284 - Sync 1 DAGs
[2024-06-18T12:36:16.297+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:36:16,297 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:31:16.296965+00:00, run_after=2024-06-18T12:36:16.296965+00:00
[2024-06-18T12:36:16.313+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.931 seconds
[2024-06-18T12:36:46.428+0000] {processor.py:157} INFO - Started process (PID=2971) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:36:46.429+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:36:46.430+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:36:46.430+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:36:46.842+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:36:46.908+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:36:46,908 - Sync 1 DAGs
[2024-06-18T12:36:46.918+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:36:46,918 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:31:46.918332+00:00, run_after=2024-06-18T12:36:46.918332+00:00
[2024-06-18T12:36:46.926+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.503 seconds
[2024-06-18T12:37:17.054+0000] {processor.py:157} INFO - Started process (PID=3214) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:37:17.056+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:37:17.057+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:37:17.057+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:37:17.503+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:37:17.558+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:37:17,558 - Sync 1 DAGs
[2024-06-18T12:37:17.570+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:37:17,570 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:32:17.570138+00:00, run_after=2024-06-18T12:37:17.570138+00:00
[2024-06-18T12:37:17.582+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.533 seconds
[2024-06-18T12:37:48.561+0000] {processor.py:157} INFO - Started process (PID=3471) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:37:48.570+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:37:48.580+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:37:48.580+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:37:49.067+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:37:49.114+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:37:49,114 - Sync 1 DAGs
[2024-06-18T12:37:49.123+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:37:49,123 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:32:49.123568+00:00, run_after=2024-06-18T12:37:49.123568+00:00
[2024-06-18T12:37:49.132+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.578 seconds
[2024-06-18T12:40:07.735+0000] {processor.py:157} INFO - Started process (PID=168) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:40:07.736+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:40:07.736+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:40:07.736+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:40:08.125+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:40:08.246+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:40:08,245 - Sync 1 DAGs
[2024-06-18T12:40:08.262+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:40:08,262 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:35:08.262209+00:00, run_after=2024-06-18T12:40:08.262209+00:00
[2024-06-18T12:40:08.276+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.544 seconds
[2024-06-18T12:40:38.308+0000] {processor.py:157} INFO - Started process (PID=448) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:40:38.309+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:40:38.310+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:40:38.310+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:40:38.794+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:40:38.886+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:40:38,885 - Sync 1 DAGs
[2024-06-18T12:40:38.899+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:40:38,899 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:40:08.262209+00:00, run_after=2024-06-18T12:45:08.262209+00:00
[2024-06-18T12:40:38.909+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.603 seconds
[2024-06-18T12:41:09.017+0000] {processor.py:157} INFO - Started process (PID=710) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:41:09.018+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:41:09.019+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:41:09.019+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:41:09.468+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:41:09.515+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:41:09,515 - Sync 1 DAGs
[2024-06-18T12:41:09.525+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:41:09,524 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:40:08.262209+00:00, run_after=2024-06-18T12:45:08.262209+00:00
[2024-06-18T12:41:09.534+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.518 seconds
[2024-06-18T12:41:40.279+0000] {processor.py:157} INFO - Started process (PID=974) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:41:40.280+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:41:40.281+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:41:40.281+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:41:40.700+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:41:40.772+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:41:40,772 - Sync 1 DAGs
[2024-06-18T12:41:40.784+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:41:40,784 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:40:08.262209+00:00, run_after=2024-06-18T12:45:08.262209+00:00
[2024-06-18T12:41:40.794+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.519 seconds
[2024-06-18T12:42:10.986+0000] {processor.py:157} INFO - Started process (PID=1243) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:42:10.987+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:42:10.987+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:42:10.987+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:42:11.590+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:42:11.680+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:42:11,679 - Sync 1 DAGs
[2024-06-18T12:42:11.694+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:42:11,694 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:40:08.262209+00:00, run_after=2024-06-18T12:45:08.262209+00:00
[2024-06-18T12:42:11.719+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.736 seconds
[2024-06-18T12:42:42.470+0000] {processor.py:157} INFO - Started process (PID=1507) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:42:42.471+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:42:42.472+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:42:42.471+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:42:42.912+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:42:42.976+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:42:42,976 - Sync 1 DAGs
[2024-06-18T12:42:42.990+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:42:42,990 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:40:08.262209+00:00, run_after=2024-06-18T12:45:08.262209+00:00
[2024-06-18T12:42:43.001+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.534 seconds
[2024-06-18T12:43:13.079+0000] {processor.py:157} INFO - Started process (PID=1776) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:43:13.080+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:43:13.081+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:43:13.080+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:43:13.457+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:43:13.509+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:43:13,509 - Sync 1 DAGs
[2024-06-18T12:43:13.519+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:43:13,519 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:40:08.262209+00:00, run_after=2024-06-18T12:45:08.262209+00:00
[2024-06-18T12:43:13.528+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.450 seconds
[2024-06-18T12:43:43.629+0000] {processor.py:157} INFO - Started process (PID=2045) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:43:43.630+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:43:43.630+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:43:43.630+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:43:43.987+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:43:44.030+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:43:44,029 - Sync 1 DAGs
[2024-06-18T12:43:44.039+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:43:44,039 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:40:08.262209+00:00, run_after=2024-06-18T12:45:08.262209+00:00
[2024-06-18T12:43:44.049+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.422 seconds
[2024-06-18T12:44:14.064+0000] {processor.py:157} INFO - Started process (PID=2317) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:44:14.064+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:44:14.065+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:44:14.065+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:44:14.499+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:44:14.545+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:44:14,545 - Sync 1 DAGs
[2024-06-18T12:44:14.554+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:44:14,554 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:40:08.262209+00:00, run_after=2024-06-18T12:45:08.262209+00:00
[2024-06-18T12:44:14.562+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.500 seconds
[2024-06-18T12:44:44.671+0000] {processor.py:157} INFO - Started process (PID=2586) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:44:44.672+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:44:44.672+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:44:44.672+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:44:45.035+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:44:45.089+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:44:45,088 - Sync 1 DAGs
[2024-06-18T12:44:45.099+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:44:45,099 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:40:08.262209+00:00, run_after=2024-06-18T12:45:08.262209+00:00
[2024-06-18T12:44:45.108+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.439 seconds
[2024-06-18T12:45:15.273+0000] {processor.py:157} INFO - Started process (PID=2759) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:45:15.273+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:45:15.274+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:45:15.274+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:45:15.842+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:45:15.939+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:45:15,938 - Sync 1 DAGs
[2024-06-18T12:45:15.970+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:45:15,970 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:45:08.262209+00:00, run_after=2024-06-18T12:50:08.262209+00:00
[2024-06-18T12:45:15.988+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.718 seconds
[2024-06-18T12:45:46.440+0000] {processor.py:157} INFO - Started process (PID=3005) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:45:46.441+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:45:46.442+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:45:46.442+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:45:47.014+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:45:47.072+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:45:47,072 - Sync 1 DAGs
[2024-06-18T12:45:47.082+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:45:47,082 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:45:08.262209+00:00, run_after=2024-06-18T12:50:08.262209+00:00
[2024-06-18T12:45:47.092+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.655 seconds
[2024-06-18T12:46:17.859+0000] {processor.py:157} INFO - Started process (PID=3222) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:46:17.860+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:46:17.861+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:46:17.861+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:46:18.443+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:46:18.496+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:46:18,495 - Sync 1 DAGs
[2024-06-18T12:46:18.507+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:46:18,506 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:45:08.262209+00:00, run_after=2024-06-18T12:50:08.262209+00:00
[2024-06-18T12:46:18.518+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.661 seconds
[2024-06-18T12:46:48.624+0000] {processor.py:157} INFO - Started process (PID=3476) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:46:48.626+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:46:48.626+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:46:48.626+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:46:49.294+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:46:49.348+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:46:49,348 - Sync 1 DAGs
[2024-06-18T12:46:49.359+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:46:49,359 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:45:08.262209+00:00, run_after=2024-06-18T12:50:08.262209+00:00
[2024-06-18T12:46:49.374+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.752 seconds
[2024-06-18T12:47:20.205+0000] {processor.py:157} INFO - Started process (PID=3742) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:47:20.208+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:47:20.209+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:47:20.209+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:47:20.663+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:47:20.712+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:47:20,712 - Sync 1 DAGs
[2024-06-18T12:47:20.723+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:47:20,723 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:45:08.262209+00:00, run_after=2024-06-18T12:50:08.262209+00:00
[2024-06-18T12:47:20.738+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.538 seconds
[2024-06-18T12:47:50.948+0000] {processor.py:157} INFO - Started process (PID=4010) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:47:50.952+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:47:50.955+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:47:50.955+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:47:51.456+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:47:51.572+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:47:51,572 - Sync 1 DAGs
[2024-06-18T12:47:51.593+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:47:51,593 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:45:08.262209+00:00, run_after=2024-06-18T12:50:08.262209+00:00
[2024-06-18T12:47:51.640+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.696 seconds
[2024-06-18T12:48:21.913+0000] {processor.py:157} INFO - Started process (PID=4264) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:48:21.917+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:48:21.919+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:48:21.918+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:48:22.758+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:48:22.939+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:48:22,937 - Sync 1 DAGs
[2024-06-18T12:48:22.977+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:48:22,977 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:45:08.262209+00:00, run_after=2024-06-18T12:50:08.262209+00:00
[2024-06-18T12:48:22.998+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 1.092 seconds
[2024-06-18T12:48:53.716+0000] {processor.py:157} INFO - Started process (PID=4538) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:48:53.719+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:48:53.720+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:48:53.720+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:48:54.137+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:48:54.197+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:48:54,196 - Sync 1 DAGs
[2024-06-18T12:48:54.209+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:48:54,209 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:45:08.262209+00:00, run_after=2024-06-18T12:50:08.262209+00:00
[2024-06-18T12:48:54.222+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.511 seconds
[2024-06-18T12:49:24.265+0000] {processor.py:157} INFO - Started process (PID=4807) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:49:24.266+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:49:24.267+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:49:24.267+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:49:24.678+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:49:24.733+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:49:24,732 - Sync 1 DAGs
[2024-06-18T12:49:24.743+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:49:24,742 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:45:08.262209+00:00, run_after=2024-06-18T12:50:08.262209+00:00
[2024-06-18T12:49:24.752+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.489 seconds
[2024-06-18T12:49:54.939+0000] {processor.py:157} INFO - Started process (PID=5074) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:49:54.940+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:49:54.940+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:49:54.940+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:49:55.477+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:49:55.535+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:49:55,535 - Sync 1 DAGs
[2024-06-18T12:49:55.546+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:49:55,546 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:45:08.262209+00:00, run_after=2024-06-18T12:50:08.262209+00:00
[2024-06-18T12:49:55.555+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.619 seconds
[2024-06-18T12:50:26.581+0000] {processor.py:157} INFO - Started process (PID=5320) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:50:26.582+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T12:50:26.583+0000] {logging_mixin.py:154} INFO - [2024-06-18T12:50:26.583+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:50:27.365+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T12:50:27.480+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:50:27,480 - Sync 1 DAGs
[2024-06-18T12:50:27.495+0000] {logging_mixin.py:154} WARNING - 2024-06-18 12:50:27,495 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T12:45:27.495410+00:00, run_after=2024-06-18T12:50:27.495410+00:00
[2024-06-18T12:50:27.507+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.927 seconds
[2024-06-18T14:00:57.727+0000] {processor.py:157} INFO - Started process (PID=168) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:00:57.730+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:00:57.737+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:00:57.736+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:00:58.735+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:00:58.922+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:00:58,921 - Sync 1 DAGs
[2024-06-18T14:00:58.943+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:00:58,943 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T13:55:58.942918+00:00, run_after=2024-06-18T14:00:58.942918+00:00
[2024-06-18T14:00:58.961+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 1.238 seconds
[2024-06-18T14:01:08.620+0000] {processor.py:157} INFO - Started process (PID=168) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:01:08.624+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:01:08.626+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:01:08.625+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:01:09.055+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:01:09.072+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:01:09,072 - Sync 1 DAGs
[2024-06-18T14:01:09.085+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:01:09,085 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T13:56:09.084943+00:00, run_after=2024-06-18T14:01:09.084943+00:00
[2024-06-18T14:01:09.097+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.487 seconds
[2024-06-18T14:01:39.225+0000] {processor.py:157} INFO - Started process (PID=412) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:01:39.232+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:01:39.235+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:01:39.234+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:01:39.683+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:01:39.738+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:01:39,737 - Sync 1 DAGs
[2024-06-18T14:01:39.749+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:01:39,749 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T13:56:39.749652+00:00, run_after=2024-06-18T14:01:39.749652+00:00
[2024-06-18T14:01:39.759+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.547 seconds
[2024-06-18T14:02:10.690+0000] {processor.py:157} INFO - Started process (PID=681) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:02:10.693+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:02:10.695+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:02:10.694+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:02:11.054+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:02:11.099+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:02:11,099 - Sync 1 DAGs
[2024-06-18T14:02:11.109+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:02:11,109 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T13:57:11.108960+00:00, run_after=2024-06-18T14:02:11.108960+00:00
[2024-06-18T14:02:11.118+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.432 seconds
[2024-06-18T14:02:41.406+0000] {processor.py:157} INFO - Started process (PID=958) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:02:41.407+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:02:41.407+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:02:41.407+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:02:41.747+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:02:41.753+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:02:41,753 - Sync 1 DAGs
[2024-06-18T14:02:41.763+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:02:41,763 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T13:57:41.763286+00:00, run_after=2024-06-18T14:02:41.763286+00:00
[2024-06-18T14:02:41.771+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.367 seconds
[2024-06-18T14:03:11.856+0000] {processor.py:157} INFO - Started process (PID=1212) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:03:11.857+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:03:11.857+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:03:11.857+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:03:12.189+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:03:12.240+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:03:12,240 - Sync 1 DAGs
[2024-06-18T14:03:12.249+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:03:12,249 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T13:58:12.249317+00:00, run_after=2024-06-18T14:03:12.249317+00:00
[2024-06-18T14:03:12.257+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.403 seconds
[2024-06-18T14:03:42.518+0000] {processor.py:157} INFO - Started process (PID=1413) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:03:42.520+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:03:42.522+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:03:42.521+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:03:42.868+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:03:42.873+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:03:42,873 - Sync 1 DAGs
[2024-06-18T14:03:42.883+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:03:42,883 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T13:58:42.883137+00:00, run_after=2024-06-18T14:03:42.883137+00:00
[2024-06-18T14:03:42.891+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.378 seconds
[2024-06-18T14:04:13.563+0000] {processor.py:157} INFO - Started process (PID=1657) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:04:13.567+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:04:13.567+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:04:13.567+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:04:13.927+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:04:13.933+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:04:13,933 - Sync 1 DAGs
[2024-06-18T14:04:13.943+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:04:13,943 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T13:59:13.943330+00:00, run_after=2024-06-18T14:04:13.943330+00:00
[2024-06-18T14:04:13.951+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.391 seconds
[2024-06-18T14:04:44.435+0000] {processor.py:157} INFO - Started process (PID=1901) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:04:44.436+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:04:44.437+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:04:44.437+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:04:44.826+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:04:44.873+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:04:44,873 - Sync 1 DAGs
[2024-06-18T14:04:44.883+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:04:44,883 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T13:59:44.883255+00:00, run_after=2024-06-18T14:04:44.883255+00:00
[2024-06-18T14:04:44.892+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.460 seconds
[2024-06-18T14:05:14.935+0000] {processor.py:157} INFO - Started process (PID=2168) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:05:14.936+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:05:14.937+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:05:14.937+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:05:15.467+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:05:15.521+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:05:15,521 - Sync 1 DAGs
[2024-06-18T14:05:15.532+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:05:15,532 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T14:00:15.532321+00:00, run_after=2024-06-18T14:05:15.532321+00:00
[2024-06-18T14:05:15.541+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.609 seconds
[2024-06-18T14:05:45.589+0000] {processor.py:157} INFO - Started process (PID=2389) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:05:45.590+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:05:45.592+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:05:45.591+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:05:46.019+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:05:46.064+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:05:46,064 - Sync 1 DAGs
[2024-06-18T14:05:46.073+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:05:46,073 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T14:00:46.073658+00:00, run_after=2024-06-18T14:05:46.073658+00:00
[2024-06-18T14:05:46.082+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.497 seconds
[2024-06-18T14:06:16.178+0000] {processor.py:157} INFO - Started process (PID=2658) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:06:16.179+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:06:16.179+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:06:16.179+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:06:16.549+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:06:16.594+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:06:16,594 - Sync 1 DAGs
[2024-06-18T14:06:16.605+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:06:16,605 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T14:01:16.605388+00:00, run_after=2024-06-18T14:06:16.605388+00:00
[2024-06-18T14:06:16.614+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.437 seconds
[2024-06-18T14:06:46.731+0000] {processor.py:157} INFO - Started process (PID=2920) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:06:46.732+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:06:46.732+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:06:46.732+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:06:47.128+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:06:47.133+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:06:47,133 - Sync 1 DAGs
[2024-06-18T14:06:47.143+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:06:47,143 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T14:01:47.143342+00:00, run_after=2024-06-18T14:06:47.143342+00:00
[2024-06-18T14:06:47.155+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.425 seconds
[2024-06-18T14:07:17.274+0000] {processor.py:157} INFO - Started process (PID=3184) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:07:17.275+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:07:17.276+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:07:17.275+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:07:17.698+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:07:17.705+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:07:17,704 - Sync 1 DAGs
[2024-06-18T14:07:17.717+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:07:17,717 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T14:02:17.717414+00:00, run_after=2024-06-18T14:07:17.717414+00:00
[2024-06-18T14:07:17.726+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.454 seconds
[2024-06-18T14:07:48.391+0000] {processor.py:157} INFO - Started process (PID=3430) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:07:48.393+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:07:48.394+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:07:48.394+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:07:48.882+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:07:48.888+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:07:48,887 - Sync 1 DAGs
[2024-06-18T14:07:48.904+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:07:48,904 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T14:02:48.903973+00:00, run_after=2024-06-18T14:07:48.903973+00:00
[2024-06-18T14:07:48.913+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.528 seconds
[2024-06-18T14:08:19.013+0000] {processor.py:157} INFO - Started process (PID=3691) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:08:19.014+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:08:19.015+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:08:19.014+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:08:19.457+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:08:19.509+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:08:19,509 - Sync 1 DAGs
[2024-06-18T14:08:19.524+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:08:19,524 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T14:03:19.524168+00:00, run_after=2024-06-18T14:08:19.524168+00:00
[2024-06-18T14:08:19.538+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.528 seconds
[2024-06-18T14:08:49.819+0000] {processor.py:157} INFO - Started process (PID=3965) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:08:49.819+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:08:49.820+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:08:49.820+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:08:50.201+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:08:50.255+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:08:50,255 - Sync 1 DAGs
[2024-06-18T14:08:50.271+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:08:50,271 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T14:03:50.271338+00:00, run_after=2024-06-18T14:08:50.271338+00:00
[2024-06-18T14:08:50.282+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.466 seconds
[2024-06-18T14:09:20.920+0000] {processor.py:157} INFO - Started process (PID=4231) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:09:20.922+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:09:20.922+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:09:20.922+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:09:21.267+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:09:21.317+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:09:21,316 - Sync 1 DAGs
[2024-06-18T14:09:21.328+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:09:21,327 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T14:04:21.327836+00:00, run_after=2024-06-18T14:09:21.327836+00:00
[2024-06-18T14:09:21.338+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.419 seconds
[2024-06-18T14:09:52.160+0000] {processor.py:157} INFO - Started process (PID=4475) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:09:52.161+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:09:52.162+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:09:52.162+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:09:52.670+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:09:52.735+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:09:52,734 - Sync 1 DAGs
[2024-06-18T14:09:52.749+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:09:52,748 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T14:04:52.748892+00:00, run_after=2024-06-18T14:09:52.748892+00:00
[2024-06-18T14:09:52.761+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.604 seconds
[2024-06-18T14:10:23.470+0000] {processor.py:157} INFO - Started process (PID=4734) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:10:23.472+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:10:23.472+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:10:23.472+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:10:23.915+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:10:23.968+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:10:23,968 - Sync 1 DAGs
[2024-06-18T14:10:23.978+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:10:23,978 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T14:05:23.978517+00:00, run_after=2024-06-18T14:10:23.978517+00:00
[2024-06-18T14:10:23.990+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.523 seconds
[2024-06-18T14:10:54.721+0000] {processor.py:157} INFO - Started process (PID=4987) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:10:54.723+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:10:54.724+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:10:54.724+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:10:55.221+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:10:55.267+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:10:55,267 - Sync 1 DAGs
[2024-06-18T14:10:55.275+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:10:55,275 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T14:05:55.275617+00:00, run_after=2024-06-18T14:10:55.275617+00:00
[2024-06-18T14:10:55.283+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.568 seconds
[2024-06-18T14:11:25.310+0000] {processor.py:157} INFO - Started process (PID=5271) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:11:25.311+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:11:25.312+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:11:25.312+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:11:25.629+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:11:25.636+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:11:25,636 - Sync 1 DAGs
[2024-06-18T14:11:25.646+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:11:25,645 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T14:06:25.645879+00:00, run_after=2024-06-18T14:11:25.645879+00:00
[2024-06-18T14:11:25.656+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.347 seconds
[2024-06-18T14:11:55.675+0000] {processor.py:157} INFO - Started process (PID=5513) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:11:55.675+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:11:55.677+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:11:55.677+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:11:56.005+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:11:56.011+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:11:56,011 - Sync 1 DAGs
[2024-06-18T14:11:56.021+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:11:56,021 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T14:06:56.021251+00:00, run_after=2024-06-18T14:11:56.021251+00:00
[2024-06-18T14:11:56.029+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.357 seconds
[2024-06-18T14:12:26.885+0000] {processor.py:157} INFO - Started process (PID=5758) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:12:26.886+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:12:26.888+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:12:26.887+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:12:27.265+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:12:27.310+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:12:27,309 - Sync 1 DAGs
[2024-06-18T14:12:27.320+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:12:27,320 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T14:07:27.320515+00:00, run_after=2024-06-18T14:12:27.320515+00:00
[2024-06-18T14:12:27.330+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.448 seconds
[2024-06-18T14:12:57.785+0000] {processor.py:157} INFO - Started process (PID=6002) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:12:57.787+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:12:57.788+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:12:57.787+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:12:58.113+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:12:58.120+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:12:58,120 - Sync 1 DAGs
[2024-06-18T14:12:58.130+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:12:58,130 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T14:07:58.130699+00:00, run_after=2024-06-18T14:12:58.130699+00:00
[2024-06-18T14:12:58.140+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.359 seconds
[2024-06-18T14:13:28.274+0000] {processor.py:157} INFO - Started process (PID=6246) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:13:28.276+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:13:28.278+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:13:28.277+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:13:28.613+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:13:28.624+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:13:28,624 - Sync 1 DAGs
[2024-06-18T14:13:28.638+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:13:28,638 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T14:08:28.638211+00:00, run_after=2024-06-18T14:13:28.638211+00:00
[2024-06-18T14:13:28.648+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.378 seconds
[2024-06-18T14:13:58.822+0000] {processor.py:157} INFO - Started process (PID=6490) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:13:58.824+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:13:58.825+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:13:58.824+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:13:59.376+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:13:59.439+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:13:59,439 - Sync 1 DAGs
[2024-06-18T14:13:59.451+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:13:59,451 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T14:08:59.451118+00:00, run_after=2024-06-18T14:13:59.451118+00:00
[2024-06-18T14:13:59.463+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.643 seconds
[2024-06-18T14:14:29.562+0000] {processor.py:157} INFO - Started process (PID=6734) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:14:29.566+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:14:29.568+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:14:29.567+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:14:29.809+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:14:29.858+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:14:29,857 - Sync 1 DAGs
[2024-06-18T14:14:29.866+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:14:29,866 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T14:09:29.866365+00:00, run_after=2024-06-18T14:14:29.866365+00:00
[2024-06-18T14:14:29.874+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.317 seconds
[2024-06-18T14:15:00.851+0000] {processor.py:157} INFO - Started process (PID=6978) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:15:00.852+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:15:00.853+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:15:00.853+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:15:01.199+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:15:01.206+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:15:01,205 - Sync 1 DAGs
[2024-06-18T14:15:01.216+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:15:01,216 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T14:10:01.216800+00:00, run_after=2024-06-18T14:15:01.216800+00:00
[2024-06-18T14:15:01.226+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.379 seconds
[2024-06-18T14:15:31.871+0000] {processor.py:157} INFO - Started process (PID=7222) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:15:31.873+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:15:31.874+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:15:31.874+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:15:32.108+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:15:32.146+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:15:32,145 - Sync 1 DAGs
[2024-06-18T14:15:32.154+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:15:32,154 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T14:10:32.154618+00:00, run_after=2024-06-18T14:15:32.154618+00:00
[2024-06-18T14:15:32.162+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.294 seconds
[2024-06-18T14:16:02.411+0000] {processor.py:157} INFO - Started process (PID=7466) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:16:02.413+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:16:02.414+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:16:02.414+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:16:02.707+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:16:02.750+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:16:02,750 - Sync 1 DAGs
[2024-06-18T14:16:02.759+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:16:02,759 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T14:11:02.759427+00:00, run_after=2024-06-18T14:16:02.759427+00:00
[2024-06-18T14:16:02.768+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.361 seconds
[2024-06-18T14:16:33.218+0000] {processor.py:157} INFO - Started process (PID=7709) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:16:33.220+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:16:33.221+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:16:33.221+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:16:33.523+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:16:33.563+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:16:33,563 - Sync 1 DAGs
[2024-06-18T14:16:33.572+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:16:33,572 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T14:11:33.572117+00:00, run_after=2024-06-18T14:16:33.572117+00:00
[2024-06-18T14:16:33.579+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.364 seconds
[2024-06-18T14:17:03.739+0000] {processor.py:157} INFO - Started process (PID=7953) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:17:03.740+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:17:03.742+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:17:03.741+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:17:04.173+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:17:04.217+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:17:04,216 - Sync 1 DAGs
[2024-06-18T14:17:04.225+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:17:04,225 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T14:12:04.225319+00:00, run_after=2024-06-18T14:17:04.225319+00:00
[2024-06-18T14:17:04.233+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.498 seconds
[2024-06-18T14:17:34.697+0000] {processor.py:157} INFO - Started process (PID=8197) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:17:34.700+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:17:34.701+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:17:34.700+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:17:35.107+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:17:35.161+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:17:35,160 - Sync 1 DAGs
[2024-06-18T14:17:35.169+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:17:35,169 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T14:12:35.169227+00:00, run_after=2024-06-18T14:17:35.169227+00:00
[2024-06-18T14:17:35.177+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.483 seconds
[2024-06-18T14:18:05.202+0000] {processor.py:157} INFO - Started process (PID=8440) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:18:05.203+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:18:05.203+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:18:05.203+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:18:05.530+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:18:05.585+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:18:05,585 - Sync 1 DAGs
[2024-06-18T14:18:05.596+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:18:05,596 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T14:13:05.596203+00:00, run_after=2024-06-18T14:18:05.596203+00:00
[2024-06-18T14:18:05.606+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.407 seconds
[2024-06-18T14:18:36.352+0000] {processor.py:157} INFO - Started process (PID=8683) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:18:36.353+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:18:36.354+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:18:36.354+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:18:36.839+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:18:36.895+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:18:36,894 - Sync 1 DAGs
[2024-06-18T14:18:36.906+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:18:36,905 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T14:13:36.905566+00:00, run_after=2024-06-18T14:18:36.905566+00:00
[2024-06-18T14:18:36.915+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.566 seconds
[2024-06-18T14:19:07.317+0000] {processor.py:157} INFO - Started process (PID=8927) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:19:07.318+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:19:07.319+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:19:07.318+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:19:07.646+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:19:07.689+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:19:07,688 - Sync 1 DAGs
[2024-06-18T14:19:07.697+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:19:07,697 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T14:14:07.697209+00:00, run_after=2024-06-18T14:19:07.697209+00:00
[2024-06-18T14:19:07.707+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.393 seconds
[2024-06-18T14:19:38.033+0000] {processor.py:157} INFO - Started process (PID=9171) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:19:38.035+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:19:38.036+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:19:38.035+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:19:38.380+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:19:38.483+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:19:38,483 - Sync 1 DAGs
[2024-06-18T14:19:38.533+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:19:38,533 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T14:14:38.533315+00:00, run_after=2024-06-18T14:19:38.533315+00:00
[2024-06-18T14:19:38.557+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.528 seconds
[2024-06-18T14:20:09.292+0000] {processor.py:157} INFO - Started process (PID=9421) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:20:09.313+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:20:09.315+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:20:09.315+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:20:09.985+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:20:10.088+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:20:10,086 - Sync 1 DAGs
[2024-06-18T14:20:10.111+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:20:10,111 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T14:15:10.111029+00:00, run_after=2024-06-18T14:20:10.111029+00:00
[2024-06-18T14:20:10.138+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.914 seconds
[2024-06-18T14:20:41.060+0000] {processor.py:157} INFO - Started process (PID=9665) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:20:41.061+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:20:41.062+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:20:41.062+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:20:41.394+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:20:41.399+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:20:41,399 - Sync 1 DAGs
[2024-06-18T14:20:41.409+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:20:41,409 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T14:15:41.409771+00:00, run_after=2024-06-18T14:20:41.409771+00:00
[2024-06-18T14:20:41.417+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.359 seconds
[2024-06-18T14:21:11.754+0000] {processor.py:157} INFO - Started process (PID=9909) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:21:11.755+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:21:11.756+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:21:11.756+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:21:12.101+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:21:12.173+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:21:12,172 - Sync 1 DAGs
[2024-06-18T14:21:12.185+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:21:12,185 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T14:16:12.185110+00:00, run_after=2024-06-18T14:21:12.185110+00:00
[2024-06-18T14:21:12.194+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.444 seconds
[2024-06-18T14:21:42.450+0000] {processor.py:157} INFO - Started process (PID=10153) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:21:42.450+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:21:42.451+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:21:42.451+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:21:42.920+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:21:42.927+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:21:42,927 - Sync 1 DAGs
[2024-06-18T14:21:42.938+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:21:42,938 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T14:16:42.938711+00:00, run_after=2024-06-18T14:21:42.938711+00:00
[2024-06-18T14:21:42.948+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.500 seconds
[2024-06-18T14:22:13.875+0000] {processor.py:157} INFO - Started process (PID=10397) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:22:13.877+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T14:22:13.880+0000] {logging_mixin.py:154} INFO - [2024-06-18T14:22:13.879+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:22:14.430+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T14:22:14.443+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:22:14,443 - Sync 1 DAGs
[2024-06-18T14:22:14.464+0000] {logging_mixin.py:154} WARNING - 2024-06-18 14:22:14,464 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T14:17:14.464007+00:00, run_after=2024-06-18T14:22:14.464007+00:00
[2024-06-18T14:22:14.481+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.615 seconds
[2024-06-18T15:06:41.304+0000] {processor.py:157} INFO - Started process (PID=10457) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:06:41.305+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:06:41.307+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:06:41.306+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:06:41.912+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:06:41.971+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:06:41,971 - Sync 1 DAGs
[2024-06-18T15:06:41.983+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:06:41,983 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:01:41.983504+00:00, run_after=2024-06-18T15:06:41.983504+00:00
[2024-06-18T15:06:41.998+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.701 seconds
[2024-06-18T15:06:49.444+0000] {processor.py:157} INFO - Started process (PID=168) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:06:49.444+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:06:49.445+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:06:49.445+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:06:49.781+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:06:49.794+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:06:49,793 - Sync 1 DAGs
[2024-06-18T15:06:49.805+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:06:49,805 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:01:49.805420+00:00, run_after=2024-06-18T15:06:49.805420+00:00
[2024-06-18T15:06:49.816+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.375 seconds
[2024-06-18T15:07:20.152+0000] {processor.py:157} INFO - Started process (PID=412) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:07:20.154+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:07:20.156+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:07:20.155+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:07:20.611+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:07:20.618+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:07:20,618 - Sync 1 DAGs
[2024-06-18T15:07:20.629+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:07:20,629 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:02:20.628907+00:00, run_after=2024-06-18T15:07:20.628907+00:00
[2024-06-18T15:07:20.637+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.489 seconds
[2024-06-18T15:07:50.659+0000] {processor.py:157} INFO - Started process (PID=679) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:07:50.660+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:07:50.660+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:07:50.660+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:07:50.989+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:07:50.995+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:07:50,995 - Sync 1 DAGs
[2024-06-18T15:07:51.006+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:07:51,006 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:02:51.006221+00:00, run_after=2024-06-18T15:07:51.006221+00:00
[2024-06-18T15:07:51.014+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.357 seconds
[2024-06-18T15:08:21.118+0000] {processor.py:157} INFO - Started process (PID=940) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:08:21.119+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:08:21.120+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:08:21.120+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:08:21.568+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:08:21.623+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:08:21,622 - Sync 1 DAGs
[2024-06-18T15:08:21.633+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:08:21,633 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:03:21.633567+00:00, run_after=2024-06-18T15:08:21.633567+00:00
[2024-06-18T15:08:21.643+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.527 seconds
[2024-06-18T15:08:52.391+0000] {processor.py:157} INFO - Started process (PID=1199) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:08:52.393+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:08:52.394+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:08:52.394+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:08:52.879+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:08:52.884+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:08:52,884 - Sync 1 DAGs
[2024-06-18T15:08:52.896+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:08:52,896 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:03:52.896176+00:00, run_after=2024-06-18T15:08:52.896176+00:00
[2024-06-18T15:08:52.904+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.516 seconds
[2024-06-18T15:09:22.988+0000] {processor.py:157} INFO - Started process (PID=1443) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:09:22.989+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:09:22.990+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:09:22.990+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:09:23.550+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:09:23.556+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:09:23,555 - Sync 1 DAGs
[2024-06-18T15:09:23.568+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:09:23,568 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:04:23.568281+00:00, run_after=2024-06-18T15:09:23.568281+00:00
[2024-06-18T15:09:23.579+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.595 seconds
[2024-06-18T15:09:53.870+0000] {processor.py:157} INFO - Started process (PID=1687) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:09:53.872+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:09:53.873+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:09:53.873+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:09:54.464+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:09:54.585+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:09:54,585 - Sync 1 DAGs
[2024-06-18T15:09:54.604+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:09:54,604 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:04:54.604197+00:00, run_after=2024-06-18T15:09:54.604197+00:00
[2024-06-18T15:09:54.617+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.752 seconds
[2024-06-18T15:10:25.525+0000] {processor.py:157} INFO - Started process (PID=1961) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:10:25.526+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:10:25.526+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:10:25.526+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:10:25.917+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:10:25.966+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:10:25,965 - Sync 1 DAGs
[2024-06-18T15:10:25.976+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:10:25,976 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:05:25.976153+00:00, run_after=2024-06-18T15:10:25.976153+00:00
[2024-06-18T15:10:25.985+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.465 seconds
[2024-06-18T15:10:56.047+0000] {processor.py:157} INFO - Started process (PID=2225) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:10:56.048+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:10:56.048+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:10:56.048+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:10:56.376+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:10:56.419+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:10:56,418 - Sync 1 DAGs
[2024-06-18T15:10:56.427+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:10:56,427 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:05:56.427779+00:00, run_after=2024-06-18T15:10:56.427779+00:00
[2024-06-18T15:10:56.436+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.390 seconds
[2024-06-18T15:11:26.572+0000] {processor.py:157} INFO - Started process (PID=2487) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:11:26.573+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:11:26.573+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:11:26.573+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:11:26.950+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:11:27.000+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:11:27,000 - Sync 1 DAGs
[2024-06-18T15:11:27.010+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:11:27,010 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:06:27.010271+00:00, run_after=2024-06-18T15:11:27.010271+00:00
[2024-06-18T15:11:27.018+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.448 seconds
[2024-06-18T15:11:57.098+0000] {processor.py:157} INFO - Started process (PID=2741) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:11:57.099+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:11:57.100+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:11:57.100+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:11:57.524+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:11:57.530+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:11:57,530 - Sync 1 DAGs
[2024-06-18T15:11:57.541+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:11:57,541 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:06:57.541342+00:00, run_after=2024-06-18T15:11:57.541342+00:00
[2024-06-18T15:11:57.549+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.453 seconds
[2024-06-18T15:12:27.656+0000] {processor.py:157} INFO - Started process (PID=2980) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:12:27.657+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:12:27.657+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:12:27.657+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:12:28.072+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:12:28.125+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:12:28,125 - Sync 1 DAGs
[2024-06-18T15:12:28.136+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:12:28,136 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:07:28.136047+00:00, run_after=2024-06-18T15:12:28.136047+00:00
[2024-06-18T15:12:28.145+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.492 seconds
[2024-06-18T15:12:58.816+0000] {processor.py:157} INFO - Started process (PID=3231) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:12:58.818+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:12:58.819+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:12:58.818+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:12:59.275+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:12:59.320+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:12:59,320 - Sync 1 DAGs
[2024-06-18T15:12:59.331+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:12:59,331 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:07:59.331469+00:00, run_after=2024-06-18T15:12:59.331469+00:00
[2024-06-18T15:12:59.342+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.531 seconds
[2024-06-18T15:13:29.407+0000] {processor.py:157} INFO - Started process (PID=3510) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:13:29.408+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:13:29.408+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:13:29.408+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:13:29.800+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:13:29.841+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:13:29,841 - Sync 1 DAGs
[2024-06-18T15:13:29.850+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:13:29,850 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:08:29.850481+00:00, run_after=2024-06-18T15:13:29.850481+00:00
[2024-06-18T15:13:29.858+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.454 seconds
[2024-06-18T15:14:00.826+0000] {processor.py:157} INFO - Started process (PID=3754) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:14:00.828+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:14:00.830+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:14:00.829+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:14:01.238+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:14:01.382+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:14:01,382 - Sync 1 DAGs
[2024-06-18T15:14:01.425+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:14:01,424 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:09:01.423852+00:00, run_after=2024-06-18T15:14:01.423852+00:00
[2024-06-18T15:14:01.459+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.638 seconds
[2024-06-18T15:14:31.501+0000] {processor.py:157} INFO - Started process (PID=4027) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:14:31.502+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:14:31.502+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:14:31.502+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:14:31.832+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:14:31.838+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:14:31,838 - Sync 1 DAGs
[2024-06-18T15:14:31.848+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:14:31,848 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:09:31.848242+00:00, run_after=2024-06-18T15:14:31.848242+00:00
[2024-06-18T15:14:31.856+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.356 seconds
[2024-06-18T15:15:02.704+0000] {processor.py:157} INFO - Started process (PID=4271) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:15:02.705+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:15:02.706+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:15:02.706+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:15:03.065+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:15:03.070+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:15:03,070 - Sync 1 DAGs
[2024-06-18T15:15:03.080+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:15:03,080 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:10:03.080746+00:00, run_after=2024-06-18T15:15:03.080746+00:00
[2024-06-18T15:15:03.089+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.390 seconds
[2024-06-18T15:15:33.110+0000] {processor.py:157} INFO - Started process (PID=4515) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:15:33.112+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:15:33.113+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:15:33.112+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:15:33.437+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:15:33.487+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:15:33,486 - Sync 1 DAGs
[2024-06-18T15:15:33.497+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:15:33,497 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:10:33.497097+00:00, run_after=2024-06-18T15:15:33.497097+00:00
[2024-06-18T15:15:33.507+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.399 seconds
[2024-06-18T15:16:04.447+0000] {processor.py:157} INFO - Started process (PID=4784) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:16:04.447+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:16:04.448+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:16:04.448+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:16:04.864+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:16:04.911+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:16:04,911 - Sync 1 DAGs
[2024-06-18T15:16:04.924+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:16:04,924 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:11:04.923730+00:00, run_after=2024-06-18T15:16:04.923730+00:00
[2024-06-18T15:16:04.936+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.493 seconds
[2024-06-18T15:16:35.825+0000] {processor.py:157} INFO - Started process (PID=5028) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:16:35.827+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:16:35.828+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:16:35.828+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:16:36.175+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:16:36.217+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:16:36,216 - Sync 1 DAGs
[2024-06-18T15:16:36.225+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:16:36,225 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:11:36.225806+00:00, run_after=2024-06-18T15:16:36.225806+00:00
[2024-06-18T15:16:36.233+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.412 seconds
[2024-06-18T15:17:06.772+0000] {processor.py:157} INFO - Started process (PID=5272) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:17:06.774+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:17:06.775+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:17:06.775+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:17:07.181+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:17:07.192+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:17:07,192 - Sync 1 DAGs
[2024-06-18T15:17:07.205+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:17:07,205 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:12:07.205266+00:00, run_after=2024-06-18T15:17:07.205266+00:00
[2024-06-18T15:17:07.214+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.447 seconds
[2024-06-18T15:17:37.832+0000] {processor.py:157} INFO - Started process (PID=5515) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:17:37.834+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:17:37.835+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:17:37.835+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:17:38.079+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:17:38.122+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:17:38,121 - Sync 1 DAGs
[2024-06-18T15:17:38.137+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:17:38,136 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:12:38.136496+00:00, run_after=2024-06-18T15:17:38.136496+00:00
[2024-06-18T15:17:38.148+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.319 seconds
[2024-06-18T15:18:08.777+0000] {processor.py:157} INFO - Started process (PID=5759) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:18:08.779+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:18:08.780+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:18:08.780+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:18:09.221+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:18:09.231+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:18:09,231 - Sync 1 DAGs
[2024-06-18T15:18:09.242+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:18:09,242 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:13:09.242006+00:00, run_after=2024-06-18T15:18:09.242006+00:00
[2024-06-18T15:18:09.249+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.478 seconds
[2024-06-18T15:18:40.003+0000] {processor.py:157} INFO - Started process (PID=6003) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:18:40.005+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:18:40.006+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:18:40.006+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:18:40.531+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:18:40.541+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:18:40,541 - Sync 1 DAGs
[2024-06-18T15:18:40.551+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:18:40,551 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:13:40.551370+00:00, run_after=2024-06-18T15:18:40.551370+00:00
[2024-06-18T15:18:40.559+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.561 seconds
[2024-06-18T15:19:10.624+0000] {processor.py:157} INFO - Started process (PID=6247) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:19:10.626+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:19:10.627+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:19:10.627+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:19:11.024+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:19:11.030+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:19:11,030 - Sync 1 DAGs
[2024-06-18T15:19:11.039+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:19:11,039 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:14:11.039141+00:00, run_after=2024-06-18T15:19:11.039141+00:00
[2024-06-18T15:19:11.047+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.427 seconds
[2024-06-18T15:19:41.407+0000] {processor.py:157} INFO - Started process (PID=6491) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:19:41.408+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:19:41.409+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:19:41.409+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:19:41.697+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:19:41.755+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:19:41,754 - Sync 1 DAGs
[2024-06-18T15:19:41.770+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:19:41,769 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:14:41.769658+00:00, run_after=2024-06-18T15:19:41.769658+00:00
[2024-06-18T15:19:41.782+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.377 seconds
[2024-06-18T15:20:11.988+0000] {processor.py:157} INFO - Started process (PID=6735) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:20:11.991+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:20:11.992+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:20:11.992+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:20:12.298+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:20:12.341+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:20:12,341 - Sync 1 DAGs
[2024-06-18T15:20:12.351+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:20:12,351 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:15:12.350757+00:00, run_after=2024-06-18T15:20:12.350757+00:00
[2024-06-18T15:20:12.360+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.376 seconds
[2024-06-18T15:20:42.460+0000] {processor.py:157} INFO - Started process (PID=6979) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:20:42.463+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:20:42.464+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:20:42.464+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:20:42.853+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:20:42.862+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:20:42,862 - Sync 1 DAGs
[2024-06-18T15:20:42.872+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:20:42,872 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:15:42.872505+00:00, run_after=2024-06-18T15:20:42.872505+00:00
[2024-06-18T15:20:42.880+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.430 seconds
[2024-06-18T15:21:13.725+0000] {processor.py:157} INFO - Started process (PID=7223) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:21:13.726+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:21:13.728+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:21:13.727+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:21:13.980+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:21:13.987+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:21:13,987 - Sync 1 DAGs
[2024-06-18T15:21:13.996+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:21:13,996 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:16:13.996489+00:00, run_after=2024-06-18T15:21:13.996489+00:00
[2024-06-18T15:21:14.003+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.284 seconds
[2024-06-18T15:21:44.255+0000] {processor.py:157} INFO - Started process (PID=7467) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:21:44.257+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:21:44.258+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:21:44.257+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:21:44.672+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:21:44.678+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:21:44,678 - Sync 1 DAGs
[2024-06-18T15:21:44.688+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:21:44,688 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:16:44.688268+00:00, run_after=2024-06-18T15:21:44.688268+00:00
[2024-06-18T15:21:44.696+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.445 seconds
[2024-06-18T15:22:15.398+0000] {processor.py:157} INFO - Started process (PID=7711) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:22:15.400+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:22:15.401+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:22:15.401+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:22:15.878+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:22:15.934+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:22:15,934 - Sync 1 DAGs
[2024-06-18T15:22:15.945+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:22:15,945 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:17:15.945408+00:00, run_after=2024-06-18T15:22:15.945408+00:00
[2024-06-18T15:22:15.958+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.564 seconds
[2024-06-18T15:22:46.896+0000] {processor.py:157} INFO - Started process (PID=7955) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:22:46.899+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:22:46.900+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:22:46.900+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:22:47.289+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:22:47.334+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:22:47,334 - Sync 1 DAGs
[2024-06-18T15:22:47.343+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:22:47,343 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:17:47.343467+00:00, run_after=2024-06-18T15:22:47.343467+00:00
[2024-06-18T15:22:47.352+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.459 seconds
[2024-06-18T15:23:18.232+0000] {processor.py:157} INFO - Started process (PID=8206) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:23:18.233+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:23:18.234+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:23:18.234+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:23:18.596+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:23:18.638+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:23:18,637 - Sync 1 DAGs
[2024-06-18T15:23:18.646+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:23:18,646 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:18:18.646713+00:00, run_after=2024-06-18T15:23:18.646713+00:00
[2024-06-18T15:23:18.654+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.426 seconds
[2024-06-18T15:23:49.559+0000] {processor.py:157} INFO - Started process (PID=8450) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:23:49.562+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:23:49.563+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:23:49.562+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:23:49.929+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:23:49.981+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:23:49,980 - Sync 1 DAGs
[2024-06-18T15:23:49.990+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:23:49,990 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:18:49.989996+00:00, run_after=2024-06-18T15:23:49.989996+00:00
[2024-06-18T15:23:49.997+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.443 seconds
[2024-06-18T15:24:20.041+0000] {processor.py:157} INFO - Started process (PID=8697) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:24:20.042+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:24:20.043+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:24:20.042+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:24:20.551+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:24:20.605+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:24:20,605 - Sync 1 DAGs
[2024-06-18T15:24:20.616+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:24:20,616 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:19:20.616064+00:00, run_after=2024-06-18T15:24:20.616064+00:00
[2024-06-18T15:24:20.625+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.587 seconds
[2024-06-18T15:24:50.685+0000] {processor.py:157} INFO - Started process (PID=8938) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:24:50.687+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:24:50.688+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:24:50.688+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:24:50.963+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:24:51.002+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:24:51,002 - Sync 1 DAGs
[2024-06-18T15:24:51.034+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:24:51,034 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:19:51.033763+00:00, run_after=2024-06-18T15:24:51.033763+00:00
[2024-06-18T15:24:51.047+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.368 seconds
[2024-06-18T15:25:21.991+0000] {processor.py:157} INFO - Started process (PID=9182) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:25:21.992+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:25:21.993+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:25:21.993+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:25:22.306+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:25:22.352+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:25:22,352 - Sync 1 DAGs
[2024-06-18T15:25:22.361+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:25:22,361 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:20:22.361292+00:00, run_after=2024-06-18T15:25:22.361292+00:00
[2024-06-18T15:25:22.369+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.392 seconds
[2024-06-18T15:25:52.920+0000] {processor.py:157} INFO - Started process (PID=9426) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:25:52.922+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:25:52.924+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:25:52.923+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:25:53.358+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:25:53.365+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:25:53,365 - Sync 1 DAGs
[2024-06-18T15:25:53.382+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:25:53,382 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:20:53.382776+00:00, run_after=2024-06-18T15:25:53.382776+00:00
[2024-06-18T15:25:53.391+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.475 seconds
[2024-06-18T15:26:23.821+0000] {processor.py:157} INFO - Started process (PID=9670) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:26:23.823+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:26:23.825+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:26:23.824+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:26:24.218+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:26:24.224+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:26:24,224 - Sync 1 DAGs
[2024-06-18T15:26:24.235+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:26:24,235 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:21:24.234979+00:00, run_after=2024-06-18T15:26:24.234979+00:00
[2024-06-18T15:26:24.242+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.427 seconds
[2024-06-18T15:26:54.598+0000] {processor.py:157} INFO - Started process (PID=9914) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:26:54.599+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:26:54.600+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:26:54.600+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:26:55.003+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:26:55.079+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:26:55,078 - Sync 1 DAGs
[2024-06-18T15:26:55.088+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:26:55,088 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:21:55.088089+00:00, run_after=2024-06-18T15:26:55.088089+00:00
[2024-06-18T15:26:55.099+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.504 seconds
[2024-06-18T15:27:26.041+0000] {processor.py:157} INFO - Started process (PID=10158) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:27:26.043+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:27:26.044+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:27:26.043+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:27:26.295+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:27:26.335+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:27:26,335 - Sync 1 DAGs
[2024-06-18T15:27:26.343+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:27:26,343 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:22:26.343343+00:00, run_after=2024-06-18T15:27:26.343343+00:00
[2024-06-18T15:27:26.350+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.314 seconds
[2024-06-18T15:27:56.768+0000] {processor.py:157} INFO - Started process (PID=10402) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:27:56.770+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:27:56.772+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:27:56.771+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:27:57.027+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:27:57.067+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:27:57,066 - Sync 1 DAGs
[2024-06-18T15:27:57.074+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:27:57,074 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:22:57.074425+00:00, run_after=2024-06-18T15:27:57.074425+00:00
[2024-06-18T15:27:57.084+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.323 seconds
[2024-06-18T15:28:27.413+0000] {processor.py:157} INFO - Started process (PID=10646) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:28:27.416+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:28:27.417+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:28:27.417+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:28:27.898+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:28:27.939+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:28:27,939 - Sync 1 DAGs
[2024-06-18T15:28:27.948+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:28:27,948 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:23:27.948231+00:00, run_after=2024-06-18T15:28:27.948231+00:00
[2024-06-18T15:28:27.956+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.548 seconds
[2024-06-18T15:28:58.814+0000] {processor.py:157} INFO - Started process (PID=10890) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:28:58.817+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:28:58.818+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:28:58.817+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:28:59.176+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:28:59.182+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:28:59,181 - Sync 1 DAGs
[2024-06-18T15:28:59.192+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:28:59,192 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:23:59.192285+00:00, run_after=2024-06-18T15:28:59.192285+00:00
[2024-06-18T15:28:59.199+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.390 seconds
[2024-06-18T15:29:29.965+0000] {processor.py:157} INFO - Started process (PID=11134) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:29:29.967+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:29:29.968+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:29:29.968+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:29:30.313+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:29:30.319+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:29:30,319 - Sync 1 DAGs
[2024-06-18T15:29:30.328+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:29:30,328 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:24:30.328474+00:00, run_after=2024-06-18T15:29:30.328474+00:00
[2024-06-18T15:29:30.335+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.375 seconds
[2024-06-18T15:30:00.842+0000] {processor.py:157} INFO - Started process (PID=11378) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:30:00.844+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:30:00.846+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:30:00.845+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:30:01.108+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:30:01.112+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:30:01,112 - Sync 1 DAGs
[2024-06-18T15:30:01.124+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:30:01,124 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:25:01.124274+00:00, run_after=2024-06-18T15:30:01.124274+00:00
[2024-06-18T15:30:01.133+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.298 seconds
[2024-06-18T15:30:31.995+0000] {processor.py:157} INFO - Started process (PID=11622) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:30:31.999+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:30:32.001+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:30:32.000+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:30:32.317+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:30:32.325+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:30:32,324 - Sync 1 DAGs
[2024-06-18T15:30:32.339+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:30:32,339 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:25:32.339478+00:00, run_after=2024-06-18T15:30:32.339478+00:00
[2024-06-18T15:30:32.349+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.369 seconds
[2024-06-18T15:31:03.258+0000] {processor.py:157} INFO - Started process (PID=11866) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:31:03.259+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:31:03.261+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:31:03.260+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:31:03.444+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:31:03.493+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:31:03,492 - Sync 1 DAGs
[2024-06-18T15:31:03.504+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:31:03,504 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:26:03.503936+00:00, run_after=2024-06-18T15:31:03.503936+00:00
[2024-06-18T15:31:03.514+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.262 seconds
[2024-06-18T15:31:33.728+0000] {processor.py:157} INFO - Started process (PID=12116) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:31:33.729+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:31:33.729+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:31:33.729+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:31:33.896+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:31:33.903+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:31:33,902 - Sync 1 DAGs
[2024-06-18T15:31:33.917+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:31:33,917 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:26:33.917318+00:00, run_after=2024-06-18T15:31:33.917318+00:00
[2024-06-18T15:31:33.928+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.201 seconds
[2024-06-18T15:32:04.648+0000] {processor.py:157} INFO - Started process (PID=12354) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:32:04.651+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:32:04.658+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:32:04.654+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:32:04.946+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:32:05.002+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:32:05,002 - Sync 1 DAGs
[2024-06-18T15:32:05.014+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:32:05,014 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:27:05.014323+00:00, run_after=2024-06-18T15:32:05.014323+00:00
[2024-06-18T15:32:05.028+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.394 seconds
[2024-06-18T15:32:35.975+0000] {processor.py:157} INFO - Started process (PID=12597) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:32:35.978+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:32:35.979+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:32:35.979+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:32:36.215+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:32:36.288+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:32:36,288 - Sync 1 DAGs
[2024-06-18T15:32:36.300+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:32:36,300 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:27:36.300231+00:00, run_after=2024-06-18T15:32:36.300231+00:00
[2024-06-18T15:32:36.310+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.341 seconds
[2024-06-18T15:33:07.008+0000] {processor.py:157} INFO - Started process (PID=12840) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:33:07.010+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:33:07.011+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:33:07.011+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:33:07.265+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:33:07.271+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:33:07,271 - Sync 1 DAGs
[2024-06-18T15:33:07.287+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:33:07,287 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:28:07.286925+00:00, run_after=2024-06-18T15:33:07.286925+00:00
[2024-06-18T15:33:07.307+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.320 seconds
[2024-06-18T15:33:37.498+0000] {processor.py:157} INFO - Started process (PID=13084) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:33:37.500+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:33:37.501+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:33:37.501+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:33:37.704+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:33:37.710+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:33:37,710 - Sync 1 DAGs
[2024-06-18T15:33:37.737+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:33:37,737 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:28:37.737071+00:00, run_after=2024-06-18T15:33:37.737071+00:00
[2024-06-18T15:33:37.748+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.257 seconds
[2024-06-18T15:34:08.068+0000] {processor.py:157} INFO - Started process (PID=13328) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:34:08.071+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:34:08.072+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:34:08.072+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:34:08.321+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:34:08.329+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:34:08,329 - Sync 1 DAGs
[2024-06-18T15:34:08.346+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:34:08,345 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:29:08.345472+00:00, run_after=2024-06-18T15:34:08.345472+00:00
[2024-06-18T15:34:08.356+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.297 seconds
[2024-06-18T15:34:39.405+0000] {processor.py:157} INFO - Started process (PID=13572) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:34:39.409+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:34:39.415+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:34:39.413+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:34:39.637+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:34:39.644+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:34:39,643 - Sync 1 DAGs
[2024-06-18T15:34:39.670+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:34:39,670 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:29:39.669518+00:00, run_after=2024-06-18T15:34:39.669518+00:00
[2024-06-18T15:34:39.679+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.285 seconds
[2024-06-18T15:35:09.881+0000] {processor.py:157} INFO - Started process (PID=13816) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:35:09.884+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:35:09.885+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:35:09.885+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:35:10.162+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:35:10.244+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:35:10,243 - Sync 1 DAGs
[2024-06-18T15:35:10.272+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:35:10,272 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:30:10.271879+00:00, run_after=2024-06-18T15:35:10.271879+00:00
[2024-06-18T15:35:10.294+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.445 seconds
[2024-06-18T15:35:40.546+0000] {processor.py:157} INFO - Started process (PID=14066) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:35:40.550+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:35:40.553+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:35:40.552+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:35:40.878+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:35:41.000+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:35:40,997 - Sync 1 DAGs
[2024-06-18T15:35:41.031+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:35:41,030 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:30:41.030428+00:00, run_after=2024-06-18T15:35:41.030428+00:00
[2024-06-18T15:35:41.058+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.515 seconds
[2024-06-18T15:36:11.442+0000] {processor.py:157} INFO - Started process (PID=14303) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:36:11.444+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:36:11.445+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:36:11.444+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:36:11.780+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:36:11.830+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:36:11,830 - Sync 1 DAGs
[2024-06-18T15:36:11.842+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:36:11,842 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:31:11.842008+00:00, run_after=2024-06-18T15:36:11.842008+00:00
[2024-06-18T15:36:11.851+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.413 seconds
[2024-06-18T15:36:42.746+0000] {processor.py:157} INFO - Started process (PID=14547) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:36:42.748+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:36:42.749+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:36:42.749+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:36:43.004+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:36:43.090+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:36:43,089 - Sync 1 DAGs
[2024-06-18T15:36:43.103+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:36:43,103 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:31:43.102868+00:00, run_after=2024-06-18T15:36:43.102868+00:00
[2024-06-18T15:36:43.114+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.372 seconds
[2024-06-18T15:37:13.275+0000] {processor.py:157} INFO - Started process (PID=14797) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:37:13.276+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:37:13.276+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:37:13.276+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:37:13.421+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:37:13.512+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:37:13,512 - Sync 1 DAGs
[2024-06-18T15:37:13.526+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:37:13,526 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:32:13.526665+00:00, run_after=2024-06-18T15:37:13.526665+00:00
[2024-06-18T15:37:13.555+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.282 seconds
[2024-06-18T15:37:44.018+0000] {processor.py:157} INFO - Started process (PID=15035) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:37:44.019+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:37:44.022+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:37:44.022+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:37:44.444+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:37:44.507+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:37:44,507 - Sync 1 DAGs
[2024-06-18T15:37:44.528+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:37:44,528 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:32:44.528035+00:00, run_after=2024-06-18T15:37:44.528035+00:00
[2024-06-18T15:37:44.537+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.523 seconds
[2024-06-18T15:38:15.019+0000] {processor.py:157} INFO - Started process (PID=15279) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:38:15.021+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:38:15.023+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:38:15.022+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:38:15.272+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:38:15.330+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:38:15,329 - Sync 1 DAGs
[2024-06-18T15:38:15.345+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:38:15,345 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:33:15.345577+00:00, run_after=2024-06-18T15:38:15.345577+00:00
[2024-06-18T15:38:15.359+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.346 seconds
[2024-06-18T15:38:45.952+0000] {processor.py:157} INFO - Started process (PID=15523) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:38:45.954+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:38:45.955+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:38:45.955+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:38:46.186+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:38:46.194+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:38:46,194 - Sync 1 DAGs
[2024-06-18T15:38:46.208+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:38:46,208 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:33:46.208322+00:00, run_after=2024-06-18T15:38:46.208322+00:00
[2024-06-18T15:38:46.218+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.269 seconds
[2024-06-18T15:39:17.257+0000] {processor.py:157} INFO - Started process (PID=15766) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:39:17.258+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:39:17.259+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:39:17.259+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:39:17.484+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:39:17.541+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:39:17,541 - Sync 1 DAGs
[2024-06-18T15:39:17.553+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:39:17,553 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:34:17.553092+00:00, run_after=2024-06-18T15:39:17.553092+00:00
[2024-06-18T15:39:17.563+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.312 seconds
[2024-06-18T15:39:48.260+0000] {processor.py:157} INFO - Started process (PID=16010) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:39:48.271+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:39:48.276+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:39:48.276+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:39:48.553+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:39:48.560+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:39:48,560 - Sync 1 DAGs
[2024-06-18T15:39:48.575+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:39:48,575 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:34:48.575704+00:00, run_after=2024-06-18T15:39:48.575704+00:00
[2024-06-18T15:39:48.585+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.349 seconds
[2024-06-18T15:40:18.828+0000] {processor.py:157} INFO - Started process (PID=16259) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:40:18.829+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:40:18.830+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:40:18.830+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:40:18.997+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:40:19.050+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:40:19,050 - Sync 1 DAGs
[2024-06-18T15:40:19.063+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:40:19,063 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:35:19.062939+00:00, run_after=2024-06-18T15:40:19.062939+00:00
[2024-06-18T15:40:19.074+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.248 seconds
[2024-06-18T15:40:49.798+0000] {processor.py:157} INFO - Started process (PID=16497) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:40:49.799+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:40:49.800+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:40:49.800+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:40:49.994+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:40:50.001+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:40:50,001 - Sync 1 DAGs
[2024-06-18T15:40:50.026+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:40:50,026 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:35:50.026652+00:00, run_after=2024-06-18T15:40:50.026652+00:00
[2024-06-18T15:40:50.036+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.242 seconds
[2024-06-18T15:41:20.308+0000] {processor.py:157} INFO - Started process (PID=16741) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:41:20.309+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:41:20.310+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:41:20.310+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:41:20.516+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:41:20.523+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:41:20,523 - Sync 1 DAGs
[2024-06-18T15:41:20.536+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:41:20,536 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:36:20.536600+00:00, run_after=2024-06-18T15:41:20.536600+00:00
[2024-06-18T15:41:20.545+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.240 seconds
[2024-06-18T15:41:51.360+0000] {processor.py:157} INFO - Started process (PID=16985) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:41:51.362+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:41:51.363+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:41:51.363+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:41:51.591+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:41:51.657+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:41:51,656 - Sync 1 DAGs
[2024-06-18T15:41:51.672+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:41:51,672 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:36:51.671954+00:00, run_after=2024-06-18T15:41:51.671954+00:00
[2024-06-18T15:41:51.681+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.326 seconds
[2024-06-18T15:42:22.453+0000] {processor.py:157} INFO - Started process (PID=17236) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:42:22.455+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:42:22.457+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:42:22.456+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:42:22.689+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:42:22.750+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:42:22,749 - Sync 1 DAGs
[2024-06-18T15:42:22.765+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:42:22,764 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:37:22.764851+00:00, run_after=2024-06-18T15:42:22.764851+00:00
[2024-06-18T15:42:22.778+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.333 seconds
[2024-06-18T15:42:52.834+0000] {processor.py:157} INFO - Started process (PID=17500) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:42:52.835+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:42:52.836+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:42:52.836+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:42:53.032+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:42:53.084+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:42:53,084 - Sync 1 DAGs
[2024-06-18T15:42:53.094+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:42:53,094 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:37:53.094497+00:00, run_after=2024-06-18T15:42:53.094497+00:00
[2024-06-18T15:42:53.105+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.276 seconds
[2024-06-18T15:43:23.233+0000] {processor.py:157} INFO - Started process (PID=17752) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:43:23.234+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:43:23.235+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:43:23.234+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:43:23.408+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:43:23.456+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:43:23,456 - Sync 1 DAGs
[2024-06-18T15:43:23.466+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:43:23,466 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:38:23.466239+00:00, run_after=2024-06-18T15:43:23.466239+00:00
[2024-06-18T15:43:23.475+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.243 seconds
[2024-06-18T15:43:53.829+0000] {processor.py:157} INFO - Started process (PID=17968) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:43:53.830+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:43:53.831+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:43:53.831+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:43:54.044+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:43:54.111+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:43:54,110 - Sync 1 DAGs
[2024-06-18T15:43:54.121+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:43:54,121 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:38:54.121370+00:00, run_after=2024-06-18T15:43:54.121370+00:00
[2024-06-18T15:43:54.131+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.304 seconds
[2024-06-18T15:44:24.856+0000] {processor.py:157} INFO - Started process (PID=18212) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:44:24.857+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:44:24.858+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:44:24.858+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:44:25.074+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:44:25.123+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:44:25,123 - Sync 1 DAGs
[2024-06-18T15:44:25.133+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:44:25,133 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:39:25.133721+00:00, run_after=2024-06-18T15:44:25.133721+00:00
[2024-06-18T15:44:25.142+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.289 seconds
[2024-06-18T15:44:55.819+0000] {processor.py:157} INFO - Started process (PID=18456) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:44:55.820+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:44:55.822+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:44:55.821+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:44:56.155+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:44:56.210+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:44:56,210 - Sync 1 DAGs
[2024-06-18T15:44:56.223+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:44:56,223 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:39:56.222910+00:00, run_after=2024-06-18T15:44:56.222910+00:00
[2024-06-18T15:44:56.233+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.418 seconds
[2024-06-18T15:45:26.837+0000] {processor.py:157} INFO - Started process (PID=18700) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:45:26.838+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:45:26.839+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:45:26.839+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:45:27.047+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:45:27.101+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:45:27,101 - Sync 1 DAGs
[2024-06-18T15:45:27.113+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:45:27,112 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:40:27.112783+00:00, run_after=2024-06-18T15:45:27.112783+00:00
[2024-06-18T15:45:27.126+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.293 seconds
[2024-06-18T15:45:57.825+0000] {processor.py:157} INFO - Started process (PID=18944) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:45:57.827+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:45:57.828+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:45:57.828+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:45:58.019+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:45:58.025+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:45:58,025 - Sync 1 DAGs
[2024-06-18T15:45:58.037+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:45:58,037 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:40:58.037237+00:00, run_after=2024-06-18T15:45:58.037237+00:00
[2024-06-18T15:45:58.045+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.225 seconds
[2024-06-18T15:46:28.315+0000] {processor.py:157} INFO - Started process (PID=19188) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:46:28.316+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:46:28.321+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:46:28.320+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:46:28.651+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:46:28.663+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:46:28,662 - Sync 1 DAGs
[2024-06-18T15:46:28.679+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:46:28,679 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:41:28.679580+00:00, run_after=2024-06-18T15:46:28.679580+00:00
[2024-06-18T15:46:28.692+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.380 seconds
[2024-06-18T15:46:58.747+0000] {processor.py:157} INFO - Started process (PID=19440) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:46:58.748+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:46:58.749+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:46:58.749+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:46:58.984+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:46:58.997+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:46:58,997 - Sync 1 DAGs
[2024-06-18T15:46:59.011+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:46:59,011 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:41:59.011129+00:00, run_after=2024-06-18T15:46:59.011129+00:00
[2024-06-18T15:46:59.022+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.282 seconds
[2024-06-18T15:47:29.106+0000] {processor.py:157} INFO - Started process (PID=19676) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:47:29.108+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:47:29.109+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:47:29.109+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:47:29.398+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:47:29.462+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:47:29,462 - Sync 1 DAGs
[2024-06-18T15:47:29.486+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:47:29,486 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:42:29.486519+00:00, run_after=2024-06-18T15:47:29.486519+00:00
[2024-06-18T15:47:29.502+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.404 seconds
[2024-06-18T15:47:59.617+0000] {processor.py:157} INFO - Started process (PID=19933) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:47:59.621+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:47:59.622+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:47:59.622+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:47:59.901+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:47:59.965+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:47:59,965 - Sync 1 DAGs
[2024-06-18T15:47:59.977+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:47:59,977 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:42:59.977228+00:00, run_after=2024-06-18T15:47:59.977228+00:00
[2024-06-18T15:47:59.988+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.373 seconds
[2024-06-18T15:48:30.251+0000] {processor.py:157} INFO - Started process (PID=20164) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:48:30.255+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:48:30.257+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:48:30.256+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:48:30.576+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:48:30.634+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:48:30,634 - Sync 1 DAGs
[2024-06-18T15:48:30.645+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:48:30,645 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:43:30.645479+00:00, run_after=2024-06-18T15:48:30.645479+00:00
[2024-06-18T15:48:30.654+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.406 seconds
[2024-06-18T15:49:00.673+0000] {processor.py:157} INFO - Started process (PID=20428) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:49:00.674+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:49:00.674+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:49:00.674+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:49:00.872+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:49:00.939+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:49:00,939 - Sync 1 DAGs
[2024-06-18T15:49:00.952+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:49:00,951 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:44:00.951878+00:00, run_after=2024-06-18T15:49:00.951878+00:00
[2024-06-18T15:49:00.973+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.302 seconds
[2024-06-18T15:49:31.090+0000] {processor.py:157} INFO - Started process (PID=20652) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:49:31.093+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:49:31.094+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:49:31.093+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:49:31.374+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:49:31.446+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:49:31,445 - Sync 1 DAGs
[2024-06-18T15:49:31.458+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:49:31,458 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:44:31.458521+00:00, run_after=2024-06-18T15:49:31.458521+00:00
[2024-06-18T15:49:31.470+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.386 seconds
[2024-06-18T15:50:02.145+0000] {processor.py:157} INFO - Started process (PID=20895) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:50:02.147+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:50:02.149+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:50:02.149+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:50:02.334+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:50:02.387+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:50:02,387 - Sync 1 DAGs
[2024-06-18T15:50:02.399+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:50:02,399 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:45:02.399267+00:00, run_after=2024-06-18T15:50:02.399267+00:00
[2024-06-18T15:50:02.410+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.271 seconds
[2024-06-18T15:50:33.099+0000] {processor.py:157} INFO - Started process (PID=21139) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:50:33.101+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:50:33.103+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:50:33.102+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:50:33.332+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:50:33.386+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:50:33,385 - Sync 1 DAGs
[2024-06-18T15:50:33.398+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:50:33,398 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:45:33.398326+00:00, run_after=2024-06-18T15:50:33.398326+00:00
[2024-06-18T15:50:33.408+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.317 seconds
[2024-06-18T15:51:03.439+0000] {processor.py:157} INFO - Started process (PID=21401) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:51:03.440+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:51:03.443+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:51:03.443+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:51:03.605+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:51:03.653+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:51:03,653 - Sync 1 DAGs
[2024-06-18T15:51:03.662+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:51:03,662 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:46:03.662323+00:00, run_after=2024-06-18T15:51:03.662323+00:00
[2024-06-18T15:51:03.670+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.233 seconds
[2024-06-18T15:51:34.667+0000] {processor.py:157} INFO - Started process (PID=21662) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:51:34.669+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:51:34.671+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:51:34.670+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:51:34.858+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:51:34.864+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:51:34,864 - Sync 1 DAGs
[2024-06-18T15:51:34.875+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:51:34,875 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:46:34.875294+00:00, run_after=2024-06-18T15:51:34.875294+00:00
[2024-06-18T15:51:34.883+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.222 seconds
[2024-06-18T15:52:05.414+0000] {processor.py:157} INFO - Started process (PID=21906) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:52:05.416+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:52:05.417+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:52:05.417+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:52:05.626+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:52:05.680+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:52:05,680 - Sync 1 DAGs
[2024-06-18T15:52:05.690+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:52:05,690 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:47:05.690351+00:00, run_after=2024-06-18T15:52:05.690351+00:00
[2024-06-18T15:52:05.700+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.290 seconds
[2024-06-18T15:52:36.287+0000] {processor.py:157} INFO - Started process (PID=22150) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:52:36.289+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:52:36.290+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:52:36.290+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:52:36.510+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:52:36.562+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:52:36,562 - Sync 1 DAGs
[2024-06-18T15:52:36.781+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:52:36,781 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:47:36.780997+00:00, run_after=2024-06-18T15:52:36.780997+00:00
[2024-06-18T15:52:36.791+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.511 seconds
[2024-06-18T15:53:06.860+0000] {processor.py:157} INFO - Started process (PID=22419) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:53:06.862+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:53:06.862+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:53:06.862+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:53:07.087+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:53:07.139+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:53:07,139 - Sync 1 DAGs
[2024-06-18T15:53:07.155+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:53:07,154 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:48:07.154709+00:00, run_after=2024-06-18T15:53:07.154709+00:00
[2024-06-18T15:53:07.319+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.461 seconds
[2024-06-18T15:53:37.615+0000] {processor.py:157} INFO - Started process (PID=22688) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:53:37.616+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:53:37.617+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:53:37.616+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:53:37.795+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:53:37.853+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:53:37,853 - Sync 1 DAGs
[2024-06-18T15:53:37.864+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:53:37,863 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:48:37.863838+00:00, run_after=2024-06-18T15:53:37.863838+00:00
[2024-06-18T15:53:37.874+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.261 seconds
[2024-06-18T15:54:08.364+0000] {processor.py:157} INFO - Started process (PID=22917) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:54:08.365+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:54:08.366+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:54:08.366+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:54:08.579+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:54:08.633+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:54:08,633 - Sync 1 DAGs
[2024-06-18T15:54:08.646+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:54:08,645 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:49:08.645880+00:00, run_after=2024-06-18T15:54:08.645880+00:00
[2024-06-18T15:54:08.656+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.294 seconds
[2024-06-18T15:54:39.348+0000] {processor.py:157} INFO - Started process (PID=23161) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:54:39.350+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:54:39.351+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:54:39.350+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:54:39.562+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:54:39.612+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:54:39,612 - Sync 1 DAGs
[2024-06-18T15:54:39.775+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:54:39,775 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:49:39.775759+00:00, run_after=2024-06-18T15:54:39.775759+00:00
[2024-06-18T15:54:39.785+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.440 seconds
[2024-06-18T15:55:09.914+0000] {processor.py:157} INFO - Started process (PID=23430) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:55:09.916+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:55:09.916+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:55:09.916+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:55:10.093+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:55:10.149+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:55:10,148 - Sync 1 DAGs
[2024-06-18T15:55:10.322+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:55:10,322 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:50:10.322151+00:00, run_after=2024-06-18T15:55:10.322151+00:00
[2024-06-18T15:55:10.331+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.419 seconds
[2024-06-18T15:55:41.170+0000] {processor.py:157} INFO - Started process (PID=23674) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:55:41.171+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:55:41.172+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:55:41.172+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:55:41.365+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:55:41.371+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:55:41,371 - Sync 1 DAGs
[2024-06-18T15:55:41.382+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:55:41,382 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:50:41.382104+00:00, run_after=2024-06-18T15:55:41.382104+00:00
[2024-06-18T15:55:41.390+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.223 seconds
[2024-06-18T15:56:12.068+0000] {processor.py:157} INFO - Started process (PID=23918) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:56:12.069+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:56:12.070+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:56:12.070+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:56:12.264+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:56:12.270+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:56:12,269 - Sync 1 DAGs
[2024-06-18T15:56:12.285+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:56:12,285 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:51:12.284982+00:00, run_after=2024-06-18T15:56:12.284982+00:00
[2024-06-18T15:56:12.296+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.230 seconds
[2024-06-18T15:56:42.979+0000] {processor.py:157} INFO - Started process (PID=24162) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:56:42.982+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:56:42.983+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:56:42.983+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:56:43.399+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:56:43.619+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:56:43,619 - Sync 1 DAGs
[2024-06-18T15:56:43.630+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:56:43,629 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:51:43.629839+00:00, run_after=2024-06-18T15:56:43.629839+00:00
[2024-06-18T15:56:43.642+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.667 seconds
[2024-06-18T15:57:14.632+0000] {processor.py:157} INFO - Started process (PID=24431) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:57:14.633+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:57:14.634+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:57:14.633+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:57:14.812+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:57:14.818+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:57:14,818 - Sync 1 DAGs
[2024-06-18T15:57:14.829+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:57:14,829 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:52:14.829424+00:00, run_after=2024-06-18T15:57:14.829424+00:00
[2024-06-18T15:57:14.837+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.209 seconds
[2024-06-18T15:57:44.902+0000] {processor.py:157} INFO - Started process (PID=24688) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:57:44.904+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:57:44.904+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:57:44.904+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:57:45.107+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:57:45.113+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:57:45,113 - Sync 1 DAGs
[2024-06-18T15:57:45.126+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:57:45,126 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:52:45.126175+00:00, run_after=2024-06-18T15:57:45.126175+00:00
[2024-06-18T15:57:45.135+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.235 seconds
[2024-06-18T15:58:15.881+0000] {processor.py:157} INFO - Started process (PID=24919) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:58:15.883+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:58:15.884+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:58:15.884+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:58:16.090+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:58:16.097+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:58:16,097 - Sync 1 DAGs
[2024-06-18T15:58:16.113+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:58:16,113 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:53:16.112782+00:00, run_after=2024-06-18T15:58:16.112782+00:00
[2024-06-18T15:58:16.128+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.251 seconds
[2024-06-18T15:58:46.851+0000] {processor.py:157} INFO - Started process (PID=25163) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:58:46.853+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:58:46.854+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:58:46.854+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:58:47.071+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:58:47.078+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:58:47,078 - Sync 1 DAGs
[2024-06-18T15:58:47.090+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:58:47,090 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:53:47.090065+00:00, run_after=2024-06-18T15:58:47.090065+00:00
[2024-06-18T15:58:47.100+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.254 seconds
[2024-06-18T15:59:17.522+0000] {processor.py:157} INFO - Started process (PID=25407) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:59:17.524+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:59:17.525+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:59:17.525+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:59:17.753+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:59:17.979+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:59:17,979 - Sync 1 DAGs
[2024-06-18T15:59:17.988+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:59:17,988 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:54:17.988691+00:00, run_after=2024-06-18T15:59:17.988691+00:00
[2024-06-18T15:59:17.997+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.481 seconds
[2024-06-18T15:59:48.820+0000] {processor.py:157} INFO - Started process (PID=25666) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:59:48.822+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T15:59:48.824+0000] {logging_mixin.py:154} INFO - [2024-06-18T15:59:48.823+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:59:49.050+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T15:59:49.102+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:59:49,102 - Sync 1 DAGs
[2024-06-18T15:59:49.269+0000] {logging_mixin.py:154} WARNING - 2024-06-18 15:59:49,269 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:54:49.268929+00:00, run_after=2024-06-18T15:59:49.268929+00:00
[2024-06-18T15:59:49.279+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.463 seconds
[2024-06-18T16:00:19.382+0000] {processor.py:157} INFO - Started process (PID=25910) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:00:19.385+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T16:00:19.387+0000] {logging_mixin.py:154} INFO - [2024-06-18T16:00:19.386+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:00:19.620+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:00:19.859+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:00:19,859 - Sync 1 DAGs
[2024-06-18T16:00:19.871+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:00:19,871 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:55:19.870992+00:00, run_after=2024-06-18T16:00:19.870992+00:00
[2024-06-18T16:00:19.881+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.504 seconds
[2024-06-18T16:00:50.897+0000] {processor.py:157} INFO - Started process (PID=26184) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:00:50.900+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T16:00:50.901+0000] {logging_mixin.py:154} INFO - [2024-06-18T16:00:50.900+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:00:51.105+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:00:51.111+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:00:51,111 - Sync 1 DAGs
[2024-06-18T16:00:51.123+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:00:51,123 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:55:51.123575+00:00, run_after=2024-06-18T16:00:51.123575+00:00
[2024-06-18T16:00:51.131+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.239 seconds
[2024-06-18T16:01:21.862+0000] {processor.py:157} INFO - Started process (PID=26428) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:01:21.864+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T16:01:21.864+0000] {logging_mixin.py:154} INFO - [2024-06-18T16:01:21.864+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:01:22.110+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:01:22.344+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:01:22,344 - Sync 1 DAGs
[2024-06-18T16:01:22.354+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:01:22,354 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:56:22.354715+00:00, run_after=2024-06-18T16:01:22.354715+00:00
[2024-06-18T16:01:22.366+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.509 seconds
[2024-06-18T16:01:53.221+0000] {processor.py:157} INFO - Started process (PID=26692) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:01:53.223+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T16:01:53.225+0000] {logging_mixin.py:154} INFO - [2024-06-18T16:01:53.224+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:01:53.431+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:01:53.669+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:01:53,669 - Sync 1 DAGs
[2024-06-18T16:01:53.680+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:01:53,680 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:56:53.680128+00:00, run_after=2024-06-18T16:01:53.680128+00:00
[2024-06-18T16:01:53.689+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.473 seconds
[2024-06-18T16:02:24.507+0000] {processor.py:157} INFO - Started process (PID=26956) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:02:24.509+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T16:02:24.510+0000] {logging_mixin.py:154} INFO - [2024-06-18T16:02:24.510+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:02:24.722+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:02:24.914+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:02:24,914 - Sync 1 DAGs
[2024-06-18T16:02:24.924+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:02:24,924 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:57:24.924181+00:00, run_after=2024-06-18T16:02:24.924181+00:00
[2024-06-18T16:02:24.931+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.429 seconds
[2024-06-18T16:02:55.716+0000] {processor.py:157} INFO - Started process (PID=27216) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:02:55.719+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T16:02:55.720+0000] {logging_mixin.py:154} INFO - [2024-06-18T16:02:55.720+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:02:55.925+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:02:56.118+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:02:56,118 - Sync 1 DAGs
[2024-06-18T16:02:56.128+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:02:56,127 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:57:56.127894+00:00, run_after=2024-06-18T16:02:56.127894+00:00
[2024-06-18T16:02:56.136+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.424 seconds
[2024-06-18T16:03:26.198+0000] {processor.py:157} INFO - Started process (PID=27473) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:03:26.199+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T16:03:26.199+0000] {logging_mixin.py:154} INFO - [2024-06-18T16:03:26.199+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:03:26.359+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:03:26.365+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:03:26,365 - Sync 1 DAGs
[2024-06-18T16:03:26.375+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:03:26,375 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:58:26.375480+00:00, run_after=2024-06-18T16:03:26.375480+00:00
[2024-06-18T16:03:26.383+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.188 seconds
[2024-06-18T16:03:57.019+0000] {processor.py:157} INFO - Started process (PID=27704) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:03:57.021+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T16:03:57.022+0000] {logging_mixin.py:154} INFO - [2024-06-18T16:03:57.022+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:03:57.226+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:03:57.233+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:03:57,232 - Sync 1 DAGs
[2024-06-18T16:03:57.245+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:03:57,244 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:58:57.244848+00:00, run_after=2024-06-18T16:03:57.244848+00:00
[2024-06-18T16:03:57.253+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.237 seconds
[2024-06-18T16:04:28.171+0000] {processor.py:157} INFO - Started process (PID=27948) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:04:28.172+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T16:04:28.173+0000] {logging_mixin.py:154} INFO - [2024-06-18T16:04:28.173+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:04:28.377+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:04:28.384+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:04:28,384 - Sync 1 DAGs
[2024-06-18T16:04:28.545+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:04:28,545 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:59:28.545751+00:00, run_after=2024-06-18T16:04:28.545751+00:00
[2024-06-18T16:04:28.552+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.387 seconds
[2024-06-18T16:04:59.370+0000] {processor.py:157} INFO - Started process (PID=28192) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:04:59.372+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T16:04:59.373+0000] {logging_mixin.py:154} INFO - [2024-06-18T16:04:59.373+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:04:59.568+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:04:59.768+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:04:59,768 - Sync 1 DAGs
[2024-06-18T16:04:59.779+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:04:59,779 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T15:59:59.779270+00:00, run_after=2024-06-18T16:04:59.779270+00:00
[2024-06-18T16:04:59.787+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.423 seconds
[2024-06-18T16:05:29.939+0000] {processor.py:157} INFO - Started process (PID=28450) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:05:29.940+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T16:05:29.941+0000] {logging_mixin.py:154} INFO - [2024-06-18T16:05:29.941+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:05:30.145+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:05:30.579+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:05:30,579 - Sync 1 DAGs
[2024-06-18T16:05:30.633+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:05:30,633 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T16:00:30.632789+00:00, run_after=2024-06-18T16:05:30.632789+00:00
[2024-06-18T16:05:30.693+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.756 seconds
[2024-06-18T16:06:01.583+0000] {processor.py:157} INFO - Started process (PID=28696) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:06:01.585+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T16:06:01.586+0000] {logging_mixin.py:154} INFO - [2024-06-18T16:06:01.585+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:06:01.887+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:06:01.929+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:06:01,929 - Sync 1 DAGs
[2024-06-18T16:06:01.940+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:06:01,939 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T16:01:01.939845+00:00, run_after=2024-06-18T16:06:01.939845+00:00
[2024-06-18T16:06:01.947+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.369 seconds
[2024-06-18T16:06:32.071+0000] {processor.py:157} INFO - Started process (PID=28940) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:06:32.072+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T16:06:32.073+0000] {logging_mixin.py:154} INFO - [2024-06-18T16:06:32.073+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:06:32.435+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:06:32.484+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:06:32,483 - Sync 1 DAGs
[2024-06-18T16:06:32.495+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:06:32,495 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T16:01:32.495647+00:00, run_after=2024-06-18T16:06:32.495647+00:00
[2024-06-18T16:06:32.505+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.438 seconds
[2024-06-18T16:07:02.570+0000] {processor.py:157} INFO - Started process (PID=29184) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:07:02.571+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T16:07:02.573+0000] {logging_mixin.py:154} INFO - [2024-06-18T16:07:02.572+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:07:02.747+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:07:03.010+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:07:03,009 - Sync 1 DAGs
[2024-06-18T16:07:03.019+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:07:03,019 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T16:02:03.019492+00:00, run_after=2024-06-18T16:07:03.019492+00:00
[2024-06-18T16:07:03.028+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.463 seconds
[2024-06-18T16:07:33.505+0000] {processor.py:157} INFO - Started process (PID=29428) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:07:33.507+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T16:07:33.508+0000] {logging_mixin.py:154} INFO - [2024-06-18T16:07:33.508+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:07:33.714+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:07:33.934+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:07:33,934 - Sync 1 DAGs
[2024-06-18T16:07:33.944+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:07:33,943 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T16:02:33.943831+00:00, run_after=2024-06-18T16:07:33.943831+00:00
[2024-06-18T16:07:33.952+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.451 seconds
[2024-06-18T16:08:04.754+0000] {processor.py:157} INFO - Started process (PID=29672) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:08:04.756+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T16:08:04.757+0000] {logging_mixin.py:154} INFO - [2024-06-18T16:08:04.757+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:08:05.058+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:08:05.100+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:08:05,100 - Sync 1 DAGs
[2024-06-18T16:08:05.109+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:08:05,109 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T16:03:05.109711+00:00, run_after=2024-06-18T16:08:05.109711+00:00
[2024-06-18T16:08:05.117+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.367 seconds
[2024-06-18T16:08:35.954+0000] {processor.py:157} INFO - Started process (PID=29916) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:08:35.955+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T16:08:35.957+0000] {logging_mixin.py:154} INFO - [2024-06-18T16:08:35.956+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:08:36.114+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:08:36.296+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:08:36,296 - Sync 1 DAGs
[2024-06-18T16:08:36.304+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:08:36,304 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T16:03:36.304458+00:00, run_after=2024-06-18T16:08:36.304458+00:00
[2024-06-18T16:08:36.312+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.363 seconds
[2024-06-18T16:09:06.725+0000] {processor.py:157} INFO - Started process (PID=30160) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:09:06.727+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T16:09:06.728+0000] {logging_mixin.py:154} INFO - [2024-06-18T16:09:06.728+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:09:06.942+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:09:07.180+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:09:07,180 - Sync 1 DAGs
[2024-06-18T16:09:07.188+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:09:07,188 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T16:04:07.188758+00:00, run_after=2024-06-18T16:09:07.188758+00:00
[2024-06-18T16:09:07.196+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.486 seconds
[2024-06-18T16:46:36.653+0000] {processor.py:157} INFO - Started process (PID=30166) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:46:36.655+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T16:46:36.656+0000] {logging_mixin.py:154} INFO - [2024-06-18T16:46:36.656+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:46:37.003+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T16:46:37.025+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:46:37,024 - Sync 1 DAGs
[2024-06-18T16:46:37.559+0000] {logging_mixin.py:154} WARNING - 2024-06-18 16:46:37,558 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T16:41:37.558517+00:00, run_after=2024-06-18T16:46:37.558517+00:00
[2024-06-18T16:46:37.592+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.943 seconds
[2024-06-18T17:23:55.479+0000] {processor.py:157} INFO - Started process (PID=30411) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:23:55.484+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T17:23:55.485+0000] {logging_mixin.py:154} INFO - [2024-06-18T17:23:55.485+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:23:56.361+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:23:56.455+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:23:56,455 - Sync 1 DAGs
[2024-06-18T17:23:56.469+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:23:56,469 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T17:18:56.469372+00:00, run_after=2024-06-18T17:23:56.469372+00:00
[2024-06-18T17:23:56.489+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 1.032 seconds
[2024-06-18T17:24:26.739+0000] {processor.py:157} INFO - Started process (PID=30662) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:24:26.740+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T17:24:26.741+0000] {logging_mixin.py:154} INFO - [2024-06-18T17:24:26.741+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:24:27.240+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:24:27.301+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:24:27,301 - Sync 1 DAGs
[2024-06-18T17:24:27.315+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:24:27,315 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T17:19:27.315030+00:00, run_after=2024-06-18T17:24:27.315030+00:00
[2024-06-18T17:24:27.330+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.594 seconds
[2024-06-18T17:24:57.410+0000] {processor.py:157} INFO - Started process (PID=30929) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:24:57.411+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T17:24:57.412+0000] {logging_mixin.py:154} INFO - [2024-06-18T17:24:57.411+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:24:57.826+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:24:57.881+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:24:57,880 - Sync 1 DAGs
[2024-06-18T17:24:57.891+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:24:57,891 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T17:19:57.891363+00:00, run_after=2024-06-18T17:24:57.891363+00:00
[2024-06-18T17:24:57.902+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.494 seconds
[2024-06-18T17:25:28.021+0000] {processor.py:157} INFO - Started process (PID=31150) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:25:28.023+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T17:25:28.024+0000] {logging_mixin.py:154} INFO - [2024-06-18T17:25:28.024+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:25:28.545+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:25:28.607+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:25:28,607 - Sync 1 DAGs
[2024-06-18T17:25:28.619+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:25:28,619 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T17:20:28.619176+00:00, run_after=2024-06-18T17:25:28.619176+00:00
[2024-06-18T17:25:28.632+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.615 seconds
[2024-06-18T17:25:59.607+0000] {processor.py:157} INFO - Started process (PID=31424) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:25:59.610+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T17:25:59.611+0000] {logging_mixin.py:154} INFO - [2024-06-18T17:25:59.611+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:26:00.114+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:26:00.122+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:26:00,122 - Sync 1 DAGs
[2024-06-18T17:26:00.137+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:26:00,136 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T17:21:00.136745+00:00, run_after=2024-06-18T17:26:00.136745+00:00
[2024-06-18T17:26:00.149+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.544 seconds
[2024-06-18T17:26:30.560+0000] {processor.py:157} INFO - Started process (PID=31668) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:26:30.562+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T17:26:30.563+0000] {logging_mixin.py:154} INFO - [2024-06-18T17:26:30.563+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:26:31.157+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:26:31.164+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:26:31,164 - Sync 1 DAGs
[2024-06-18T17:26:31.176+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:26:31,176 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T17:21:31.176300+00:00, run_after=2024-06-18T17:26:31.176300+00:00
[2024-06-18T17:26:31.186+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.629 seconds
[2024-06-18T17:27:01.503+0000] {processor.py:157} INFO - Started process (PID=31930) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:27:01.506+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T17:27:01.507+0000] {logging_mixin.py:154} INFO - [2024-06-18T17:27:01.507+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:27:01.954+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:27:01.964+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:27:01,964 - Sync 1 DAGs
[2024-06-18T17:27:01.981+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:27:01,980 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T17:22:01.980836+00:00, run_after=2024-06-18T17:27:01.980836+00:00
[2024-06-18T17:27:01.993+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.493 seconds
[2024-06-18T17:27:32.036+0000] {processor.py:157} INFO - Started process (PID=32156) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:27:32.037+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T17:27:32.038+0000] {logging_mixin.py:154} INFO - [2024-06-18T17:27:32.037+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:27:32.495+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:27:32.551+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:27:32,550 - Sync 1 DAGs
[2024-06-18T17:27:32.565+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:27:32,565 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T17:22:32.565629+00:00, run_after=2024-06-18T17:27:32.565629+00:00
[2024-06-18T17:27:32.581+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.548 seconds
[2024-06-18T17:28:02.718+0000] {processor.py:157} INFO - Started process (PID=32400) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:28:02.719+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T17:28:02.720+0000] {logging_mixin.py:154} INFO - [2024-06-18T17:28:02.720+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:28:03.184+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:28:03.198+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:28:03,197 - Sync 1 DAGs
[2024-06-18T17:28:03.212+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:28:03,212 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T17:23:03.212088+00:00, run_after=2024-06-18T17:28:03.212088+00:00
[2024-06-18T17:28:03.223+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.507 seconds
[2024-06-18T17:28:33.336+0000] {processor.py:157} INFO - Started process (PID=32664) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:28:33.337+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T17:28:33.337+0000] {logging_mixin.py:154} INFO - [2024-06-18T17:28:33.337+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:28:33.726+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:28:33.780+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:28:33,780 - Sync 1 DAGs
[2024-06-18T17:28:33.792+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:28:33,792 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T17:23:33.792002+00:00, run_after=2024-06-18T17:28:33.792002+00:00
[2024-06-18T17:28:33.803+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.470 seconds
[2024-06-18T17:29:03.892+0000] {processor.py:157} INFO - Started process (PID=32918) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:29:03.893+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T17:29:03.895+0000] {logging_mixin.py:154} INFO - [2024-06-18T17:29:03.894+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:29:04.337+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:29:04.345+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:29:04,345 - Sync 1 DAGs
[2024-06-18T17:29:04.358+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:29:04,358 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T17:24:04.358166+00:00, run_after=2024-06-18T17:29:04.358166+00:00
[2024-06-18T17:29:04.370+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.480 seconds
[2024-06-18T17:29:34.933+0000] {processor.py:157} INFO - Started process (PID=33186) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:29:34.934+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T17:29:34.935+0000] {logging_mixin.py:154} INFO - [2024-06-18T17:29:34.934+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:29:35.358+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:29:35.410+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:29:35,409 - Sync 1 DAGs
[2024-06-18T17:29:35.419+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:29:35,419 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T17:24:35.419652+00:00, run_after=2024-06-18T17:29:35.419652+00:00
[2024-06-18T17:29:35.430+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.499 seconds
[2024-06-18T17:30:06.363+0000] {processor.py:157} INFO - Started process (PID=33430) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:30:06.364+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T17:30:06.365+0000] {logging_mixin.py:154} INFO - [2024-06-18T17:30:06.365+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:30:06.746+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:30:06.798+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:30:06,798 - Sync 1 DAGs
[2024-06-18T17:30:06.810+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:30:06,810 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T17:25:06.810762+00:00, run_after=2024-06-18T17:30:06.810762+00:00
[2024-06-18T17:30:06.824+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.463 seconds
[2024-06-18T17:30:36.867+0000] {processor.py:157} INFO - Started process (PID=33674) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:30:36.869+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T17:30:36.870+0000] {logging_mixin.py:154} INFO - [2024-06-18T17:30:36.870+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:30:37.253+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:30:37.297+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:30:37,296 - Sync 1 DAGs
[2024-06-18T17:30:37.306+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:30:37,306 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T17:25:37.306054+00:00, run_after=2024-06-18T17:30:37.306054+00:00
[2024-06-18T17:30:37.314+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.449 seconds
[2024-06-18T17:31:07.333+0000] {processor.py:157} INFO - Started process (PID=33943) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:31:07.334+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T17:31:07.335+0000] {logging_mixin.py:154} INFO - [2024-06-18T17:31:07.335+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:31:07.693+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:31:07.698+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:31:07,698 - Sync 1 DAGs
[2024-06-18T17:31:07.710+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:31:07,710 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T17:26:07.710350+00:00, run_after=2024-06-18T17:31:07.710350+00:00
[2024-06-18T17:31:07.718+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.387 seconds
[2024-06-18T17:31:37.748+0000] {processor.py:157} INFO - Started process (PID=34212) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:31:37.750+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T17:31:37.751+0000] {logging_mixin.py:154} INFO - [2024-06-18T17:31:37.750+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:31:38.166+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:31:38.220+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:31:38,220 - Sync 1 DAGs
[2024-06-18T17:31:38.232+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:31:38,232 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T17:26:38.232370+00:00, run_after=2024-06-18T17:31:38.232370+00:00
[2024-06-18T17:31:38.242+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.496 seconds
[2024-06-18T17:32:08.269+0000] {processor.py:157} INFO - Started process (PID=34406) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:32:08.271+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T17:32:08.272+0000] {logging_mixin.py:154} INFO - [2024-06-18T17:32:08.272+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:32:08.693+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:32:08.756+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:32:08,756 - Sync 1 DAGs
[2024-06-18T17:32:08.767+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:32:08,767 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T17:27:08.767749+00:00, run_after=2024-06-18T17:32:08.767749+00:00
[2024-06-18T17:32:08.779+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.512 seconds
[2024-06-18T17:32:38.964+0000] {processor.py:157} INFO - Started process (PID=34680) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:32:38.966+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T17:32:38.968+0000] {logging_mixin.py:154} INFO - [2024-06-18T17:32:38.967+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:32:39.393+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:32:39.463+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:32:39,463 - Sync 1 DAGs
[2024-06-18T17:32:39.477+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:32:39,477 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T17:27:39.477404+00:00, run_after=2024-06-18T17:32:39.477404+00:00
[2024-06-18T17:32:39.493+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.531 seconds
[2024-06-18T17:33:09.667+0000] {processor.py:157} INFO - Started process (PID=34946) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:33:09.668+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T17:33:09.668+0000] {logging_mixin.py:154} INFO - [2024-06-18T17:33:09.668+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:33:10.094+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:33:10.102+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:33:10,101 - Sync 1 DAGs
[2024-06-18T17:33:10.115+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:33:10,115 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T17:28:10.115732+00:00, run_after=2024-06-18T17:33:10.115732+00:00
[2024-06-18T17:33:10.130+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.466 seconds
[2024-06-18T17:33:40.169+0000] {processor.py:157} INFO - Started process (PID=35212) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:33:40.170+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T17:33:40.172+0000] {logging_mixin.py:154} INFO - [2024-06-18T17:33:40.171+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:33:40.605+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:33:40.655+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:33:40,654 - Sync 1 DAGs
[2024-06-18T17:33:40.666+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:33:40,666 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T17:28:40.666736+00:00, run_after=2024-06-18T17:33:40.666736+00:00
[2024-06-18T17:33:40.678+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.511 seconds
[2024-06-18T17:34:10.731+0000] {processor.py:157} INFO - Started process (PID=35459) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:34:10.732+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T17:34:10.732+0000] {logging_mixin.py:154} INFO - [2024-06-18T17:34:10.732+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:34:11.274+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:34:11.331+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:34:11,331 - Sync 1 DAGs
[2024-06-18T17:34:11.342+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:34:11,342 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T17:29:11.342102+00:00, run_after=2024-06-18T17:34:11.342102+00:00
[2024-06-18T17:34:11.355+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.625 seconds
[2024-06-18T17:34:41.374+0000] {processor.py:157} INFO - Started process (PID=35728) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:34:41.375+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T17:34:41.375+0000] {logging_mixin.py:154} INFO - [2024-06-18T17:34:41.375+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:34:42.024+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:34:42.127+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:34:42,125 - Sync 1 DAGs
[2024-06-18T17:34:42.140+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:34:42,140 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T17:29:42.140515+00:00, run_after=2024-06-18T17:34:42.140515+00:00
[2024-06-18T17:34:42.153+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.781 seconds
[2024-06-18T17:35:13.077+0000] {processor.py:157} INFO - Started process (PID=36012) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:35:13.079+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T17:35:13.080+0000] {logging_mixin.py:154} INFO - [2024-06-18T17:35:13.080+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:35:13.492+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:35:13.500+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:35:13,499 - Sync 1 DAGs
[2024-06-18T17:35:13.512+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:35:13,512 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T17:30:13.512613+00:00, run_after=2024-06-18T17:35:13.512613+00:00
[2024-06-18T17:35:13.524+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.449 seconds
[2024-06-18T17:35:44.532+0000] {processor.py:157} INFO - Started process (PID=36255) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:35:44.534+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T17:35:44.536+0000] {logging_mixin.py:154} INFO - [2024-06-18T17:35:44.535+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:35:45.029+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:35:45.085+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:35:45,085 - Sync 1 DAGs
[2024-06-18T17:35:45.102+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:35:45,101 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T17:30:45.101825+00:00, run_after=2024-06-18T17:35:45.101825+00:00
[2024-06-18T17:35:45.114+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.587 seconds
[2024-06-18T17:36:15.200+0000] {processor.py:157} INFO - Started process (PID=36499) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:36:15.202+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T17:36:15.204+0000] {logging_mixin.py:154} INFO - [2024-06-18T17:36:15.204+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:36:15.674+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:36:15.742+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:36:15,742 - Sync 1 DAGs
[2024-06-18T17:36:15.757+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:36:15,757 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T17:31:15.756896+00:00, run_after=2024-06-18T17:36:15.756896+00:00
[2024-06-18T17:36:15.771+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.574 seconds
[2024-06-18T17:36:45.997+0000] {processor.py:157} INFO - Started process (PID=36743) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:36:45.998+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T17:36:45.999+0000] {logging_mixin.py:154} INFO - [2024-06-18T17:36:45.999+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:36:46.450+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:36:46.457+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:36:46,457 - Sync 1 DAGs
[2024-06-18T17:36:46.472+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:36:46,471 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T17:31:46.471860+00:00, run_after=2024-06-18T17:36:46.471860+00:00
[2024-06-18T17:36:46.482+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.488 seconds
[2024-06-18T17:37:16.936+0000] {processor.py:157} INFO - Started process (PID=36987) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:37:16.938+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T17:37:16.939+0000] {logging_mixin.py:154} INFO - [2024-06-18T17:37:16.939+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:37:17.290+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:37:17.336+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:37:17,335 - Sync 1 DAGs
[2024-06-18T17:37:17.347+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:37:17,347 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T17:32:17.347240+00:00, run_after=2024-06-18T17:37:17.347240+00:00
[2024-06-18T17:37:17.356+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.423 seconds
[2024-06-18T17:37:48.258+0000] {processor.py:157} INFO - Started process (PID=37231) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:37:48.259+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T17:37:48.260+0000] {logging_mixin.py:154} INFO - [2024-06-18T17:37:48.259+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:37:48.721+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:37:48.788+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:37:48,787 - Sync 1 DAGs
[2024-06-18T17:37:48.797+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:37:48,797 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T17:32:48.797448+00:00, run_after=2024-06-18T17:37:48.797448+00:00
[2024-06-18T17:37:48.806+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.552 seconds
[2024-06-18T17:38:19.471+0000] {processor.py:157} INFO - Started process (PID=37475) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:38:19.474+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T17:38:19.476+0000] {logging_mixin.py:154} INFO - [2024-06-18T17:38:19.475+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:38:20.232+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:38:20.289+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:38:20,288 - Sync 1 DAGs
[2024-06-18T17:38:20.298+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:38:20,298 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T17:33:20.298565+00:00, run_after=2024-06-18T17:38:20.298565+00:00
[2024-06-18T17:38:20.306+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.842 seconds
[2024-06-18T17:38:50.497+0000] {processor.py:157} INFO - Started process (PID=37719) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:38:50.498+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T17:38:50.498+0000] {logging_mixin.py:154} INFO - [2024-06-18T17:38:50.498+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:38:50.873+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:38:50.920+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:38:50,919 - Sync 1 DAGs
[2024-06-18T17:38:50.930+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:38:50,930 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T17:33:50.929910+00:00, run_after=2024-06-18T17:38:50.929910+00:00
[2024-06-18T17:38:50.938+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.443 seconds
[2024-06-18T17:39:21.294+0000] {processor.py:157} INFO - Started process (PID=37963) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:39:21.296+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T17:39:21.298+0000] {logging_mixin.py:154} INFO - [2024-06-18T17:39:21.297+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:39:21.692+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T17:39:21.734+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:39:21,733 - Sync 1 DAGs
[2024-06-18T17:39:21.742+0000] {logging_mixin.py:154} WARNING - 2024-06-18 17:39:21,742 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T17:34:21.742585+00:00, run_after=2024-06-18T17:39:21.742585+00:00
[2024-06-18T17:39:21.750+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.463 seconds
[2024-06-18T18:11:08.052+0000] {processor.py:157} INFO - Started process (PID=37977) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T18:11:08.055+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T18:11:08.063+0000] {logging_mixin.py:154} INFO - [2024-06-18T18:11:08.063+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T18:11:08.957+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T18:11:09.036+0000] {logging_mixin.py:154} WARNING - 2024-06-18 18:11:09,036 - Sync 1 DAGs
[2024-06-18T18:11:09.051+0000] {logging_mixin.py:154} WARNING - 2024-06-18 18:11:09,051 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T18:06:09.051786+00:00, run_after=2024-06-18T18:11:09.051786+00:00
[2024-06-18T18:11:09.070+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 1.020 seconds
[2024-06-18T19:34:54.221+0000] {processor.py:157} INFO - Started process (PID=38221) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T19:34:54.223+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T19:34:54.226+0000] {logging_mixin.py:154} INFO - [2024-06-18T19:34:54.225+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T19:34:54.727+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T19:34:54.733+0000] {logging_mixin.py:154} WARNING - 2024-06-18 19:34:54,733 - Sync 1 DAGs
[2024-06-18T19:34:54.744+0000] {logging_mixin.py:154} WARNING - 2024-06-18 19:34:54,744 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T19:29:54.744475+00:00, run_after=2024-06-18T19:34:54.744475+00:00
[2024-06-18T19:34:54.755+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.539 seconds
[2024-06-18T20:57:00.558+0000] {processor.py:157} INFO - Started process (PID=38465) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T20:57:00.563+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T20:57:00.570+0000] {logging_mixin.py:154} INFO - [2024-06-18T20:57:00.568+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T20:57:01.521+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T20:57:01.528+0000] {logging_mixin.py:154} WARNING - 2024-06-18 20:57:01,528 - Sync 1 DAGs
[2024-06-18T20:57:01.541+0000] {logging_mixin.py:154} WARNING - 2024-06-18 20:57:01,540 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T20:52:01.540830+00:00, run_after=2024-06-18T20:57:01.540830+00:00
[2024-06-18T20:57:01.551+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 1.004 seconds
[2024-06-18T22:35:26.158+0000] {processor.py:157} INFO - Started process (PID=38709) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T22:35:26.161+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T22:35:26.163+0000] {logging_mixin.py:154} INFO - [2024-06-18T22:35:26.162+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T22:35:26.962+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T22:35:26.983+0000] {logging_mixin.py:154} WARNING - 2024-06-18 22:35:26,983 - Sync 1 DAGs
[2024-06-18T22:35:27.022+0000] {logging_mixin.py:154} WARNING - 2024-06-18 22:35:27,021 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T22:30:27.021130+00:00, run_after=2024-06-18T22:35:27.021130+00:00
[2024-06-18T22:35:27.059+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.909 seconds
[2024-06-18T23:55:33.543+0000] {processor.py:157} INFO - Started process (PID=38953) to work on /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T23:55:33.547+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/dag_kafka_spark.py for tasks to queue
[2024-06-18T23:55:33.552+0000] {logging_mixin.py:154} INFO - [2024-06-18T23:55:33.549+0000] {dagbag.py:536} INFO - Filling up the DagBag from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T23:55:34.241+0000] {processor.py:839} INFO - DAG(s) dict_keys(['kafka_spark_dag']) retrieved from /opt/airflow/dags/dag_kafka_spark.py
[2024-06-18T23:55:34.249+0000] {logging_mixin.py:154} WARNING - 2024-06-18 23:55:34,248 - Sync 1 DAGs
[2024-06-18T23:55:34.262+0000] {logging_mixin.py:154} WARNING - 2024-06-18 23:55:34,262 - Setting next_dagrun for kafka_spark_dag to 2024-06-18T23:50:34.262721+00:00, run_after=2024-06-18T23:55:34.262721+00:00
[2024-06-18T23:55:34.274+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/dag_kafka_spark.py took 0.745 seconds
